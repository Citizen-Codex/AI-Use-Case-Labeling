Department,Agency,Use Case ID (Added),Use Case Name,Short Description,Use Case Category,Use Case Type,AI Techniques,Date Published,
Department of Agriculture,ARS,AI1,4% Repair Dashboard,"The model reviews the descriptions of expenses tagged to repairs and maintenance and classifies expenses as ""repair"" or ""not repair"" based on keywords in context.",Operational,Expense Categorization,NLP,5/15/23,
Department of Agriculture,ARS,AI2,ARS Project Mapping,NLP of research project plans including term analysis and clustering enables national program leaders to work with an interactive dashboard to find synergies and patterns within and across the various ARS research program portfolios.,Operational,Research Project Plans,NLP,5/15/23,
Department of Agriculture,ARS,AI3,NAL Automated indexing,"Cogito (vendor) software, uses AI for automated subject indexing to annotate peer reviewed journal articles (~500,000 annually) using the National Ag Library Thesaurus concept space (NALT). Only NALT concepts are annotated as metadata to content in the Library's bibliographic citation database, AGRICOLA, PubAg, and Ag Data Commons.",Operational,Research Annotations,NLP,5/15/23,
Department of Agriculture,APHIS,AI4,Predictive modeling of invasive pest species and category at the port of entry using machine learning algorithms,Macine learning algorithms are used to develop with inspection data and improve prediction ability of detecting invasive/quarantine significant pests at the port of entry.,Mission,Animal Identification,ML,5/15/23,
Department of Agriculture,APHIS,AI5,Detection of pre-symptomatic HLB infected citrus,Identify pixels with HLB infection signature in multispectral and thermal imagery,Mission,Landscape Identification,Visual Analysis,5/15/23,
Department of Agriculture,APHIS,AI6,High throughput phenotyping in citrus orchards,"Locate, count, and categorize citrus trees in an orchard to monitor orchard health",Mission,Landscape Identification,ML,5/15/23,
Department of Agriculture,APHIS,AI7,Detection of aquatic weeds,Identify and locate aquatic weeds,Mission,Landscape Identification,ML,5/15/23,
Department of Agriculture,APHIS,AI8,Automated Detection & Mapping of Host Plants from Ground Level Imagery,Generate maps of target trees from ground-level (streetview) imagery,Mission,Landscape Identification,ML,5/15/23,
Department of Agriculture,ERS,AI9,Democratizing Data,"The purpose of this project is to use AI tools, machine learning and natural language processing to understand how publicly-funded data and evidence are used to serve science and society.",Mission,Research Impact,NLP,5/15/23,
Department of Agriculture,ERS,AI10,Westat,"A competition to find automated, yet effective, ways of linking USDA nutrition information to 750K food items in a proprietary data set of food purchases and acquisitions. Competing teams used a number of  AI methods including Natural Language Processing (NLP), random forest, and semantic matching.",Mission,Food Categorization (Nutrition Information),"NLP, ML",5/15/23,
Department of Agriculture,FPAC,AI11,Land Change Analysis Tool (LCAT),We employ a random forest machine learning classifier to produce high resolution land cover maps from aerial and/or satellite imagery.  Training data is generate from a custom-built web application.  We built and operate a 192-node docker cluster to parallize CPU-intensive processing tasks.  We are publishing results through a publicly available  Image service.  To date we have mapped over 600 million acres and have generated over 700 thousand traiing samples.,Mission,Landscape Identification,ML,5/15/23,
Department of Agriculture,Federal CDO Council,AI12,OCIO/CDO Council Comment Analysis Tool,"The Comment Analysis pilot has shown that a toolset leveraging recent advances in Natural Language Processing (NLP) can aid the regulatory comment analysis process. We developed tools that help comment reviewers identify the topics and themes of comments, as well as group comments that are semantically similar. Tools like these offer significant value by creating efficiencies through novel insights and streamlined processing of comments, reducing duplicative, upfront development efforts across government, and ultimately realizing cost savings for agencies and the USG. ",Operational,Public Comment Analysis,NLP,5/15/23,
Department of Agriculture,FNS,AI13,Retailer Receipt Analysis,"The Retailer Receipt Analysis is a Proof of Concept (POC) that uses Optical Character Recognition (OCR), an application of artificial intelligence on a sample (no more than 1000) of FNS receipt and invoice data. Consultants will use this data to demonstrate how the existing manual process can be automated, saving staff time, ensuring accurate review, and detecting difficult patterns. The goal of this POC will pave the way for a review system that (1) has an automated workflow and learns from analyst feedback (2) can incorporate know SNAP fraud patterns, look for new patterns, and visualize alerts on these patterns on retailer invoices and receipts.",Mission,SNAP Receipt Review,ML,5/15/23,
Department of Agriculture,Forest Service,AI14,Ecosystem Management Decision Support System (EMDS),"EMDS is a spatial decision support system for landscape analysis and planning that runs as a component of ArcGIS and QGIS. Users develop applications for their specific problem that may use any combination of four AI engines for 1) logic processing, 2) multi-criteria decision analysis, 3) Bayesian networks, and Prolog-based decision trees.",Mission,Landscape Identification,ML,5/15/23,
Department of Agriculture,Forest Service,AI15,Wildland Urban Interface - Mapping Wildfire Loss,"This is a proof-of-concept study to investigate the use of machine learning (deep learning / convolutional neural networks) and object-based image classification techniques to identify buildings, building loss, and defensible space around buildings before and after a wildfire event in wildland-urban interface settings.",Mission,Landscape Identification - Post Fire,ML,5/15/23,
Department of Agriculture,Forest Service,AI16,CLT Knowledge Database,"The CLT knowledge database catalogs cross-laminated timber information in an interface that helps users find relevant information. The information system uses data aggregator bots that search the internet for relevant information. These bots search for hundreds of keywords and use machine learning to determine if what is found is relevant. The search engine uses intelligent software to locate and update pertinent CLT references, as well as categorize information with respect to common application and interest areas. As of 2/24/2022, the CLT knowledge database has cataloged >3,600 publications on various aspects of CLT. This system fosters growth of mass timber markets by disseminating knowledge and facilitating collaboration among stakeholders, and by reducing the risk of duplication of efforts. Manufacturers, researchers, design professionals, code officials, government agencies, and other stakeholders directly benefit from the tool, thereby supporting the increasing use of mass timber, which benefits forest health by increasing the economic value of forests.",Operational,Knowledge Catalog,ML,5/15/23,
Department of Agriculture,Forest Service,AI17,RMRS Raster Utility,"RMRS Raster Utility is a .NET object oriented library that simplifies data acquisition, raster sampling, and statistical and spatial modeling while reducing the processing time and storage space associated with raster analysis. It includes machine learning techniques.",Mission,Research Analysis,ML,5/15/23,
Department of Agriculture,Forest Service,AI18,TreeMap 2016,"TreeMap 2016 provides a tree-level model of the forests of the conterminous United States. It matches forest plot data from Forest Inventory and Analysis (FIA) to a 30x30 meter (m) grid. TreeMap 2016 is being used in both the private and public sectors for projects including fuel treatment planning, snag hazard mapping, and estimation of terrestrial carbon resources. A random forests machine-learning algorithm was used to impute the forest plot data to a set of target rasters provided by Landscape Fire and Resource Management Planning Tools (LANDFIRE: https://landfire.gov). Predictor variables consisted of percent forest cover, height, and vegetation type, as well as topography (slope, elevation, and aspect), location (latitude and longitude), biophysical variables (photosynthetically active radiation, precipitation, maximum temperature, minimum temperature, relative humidity, and vapour pressure deficit), and disturbance history (time since disturbance and disturbance type) for the landscape circa 2016.",Mission,Landscape - Planning,ML,5/15/23,
Department of Agriculture,Forest Service,AI19,Landscape Change Monitoring System (LCMS),"The Landscape Change Monitoring System (LCMS) is a National landsat/sentinal remote sensing-based data produced by the USDA Forest Service for mapping and monitoring changes related to vegetation canopy cover, as well as land cover and land use. The process utilizes temporal change classifications together with training data in a supervised classification process for vegetation gain, and loss as well as land cover and use.",Mission,Landscape Identification,"ML, Visual Analysis",5/15/23,
Department of Agriculture,Forest Service,AI20,Geospatial and Remote Sensing Training Courses,"Several courses are offered which teach the use of software and scripting which allow for machine learning.  The courses change, but current topics include Intro and Advanced Change Detection, eCognition (software package), Geospatial Scripting for Google Earth Engine.  Some of the courses show how to use Collect Earth Online.",Mission,Research Analysis,ML,5/15/23,
Department of Agriculture,Forest Service,AI21,Forest Health Detection Monitoring,"Machine learning models are used to (1) upscale training data, using Sentinel-2, Landsat,  MODIS, and lidar imagery, that was collected from both the field and high-resolution imagery to map and monitor stages of forest mortality and defoliation across the United States, and (2) to post-process raster outputs to vector polygons.",Mission,Landscape Identification,ML,5/15/23,
Department of Agriculture,NASS,AI22,Cropland Data Layer,"A machine learning algorithm is used to interpret readings from satellite-based sensors and CLASSIFY the type of crop or activity that falls in each 30 square meter pixel (a box of fixed size) on the ground.  The algorithms are trained on USDA&%2339;s Farm Services Agency data and other sources of data as sources of &quot;ground truth&quot;.  It allows us to not only produce a classification, but to assess the accuracy of the classification as well.  For commodities, like corn and soybeans, the CDL is highly accurate.  The CDL has been produced for national coverage since 2008.  Some summary and background about the CDL is available in a number of peer reviewed research papers and presentations",Mission,Landscape Identification,ML,5/15/23,
Department of Agriculture,NASS,AI23,List Frame Deadwood Identification,"The deadwood model leverages boosted regression trees with inputs such as  administrative linkage data, frame data, and historical response information as inputs, to produce a propensity score representing a relative likelihood of a farm operation being out of business.  Common tree splits were identified using the model and combined with expert knowledge to develop a recurring process for deadwood clean up.",Mission,Landscape Identification,ML,5/15/23,
Department of Agriculture,NIFA,AI24,Climate Change Classification NLP,"The model classifies NIFA funded projects as climate change related or not climate related through natural language processing techniques. The model input features include text fields containing the project's title, non-technical summary, objectives and keywords. The target is a dummy variable classification of projects as climate change related or not climate change related.",Operational,Project Classification,NLP,5/15/23,
Department of Agriculture,OSSP,AI25,Video Surveillance System,"The Video Surveillance System: the VSS system design will include a video management system, NVRs, DVRs, encoders, fixed cameras, Pan and Tilt cameras, network switches, routers, IP cables, equipment racks and mounting hardware. The Video Surveillance System (VSS)- shall control multiple sources of video surveillance subsystems to collect, manage, and present video clearly and concisely. VMS shall integrate the capabilities of each subsystem across single or multiple sites, allowing video management of any compatible analog or digital video device through a unified configuration platform and viewer. Disparate video systems are normalized and funneled through a shared video experience. Drag and drop cameras from the Security Management System hardware tree into VMS views and leverage Security Management System alarm integration and advanced features that help the operator track a target through a set of sequential cameras with a simplified method to select a new central camera and surrounding camera views.",Operational,Security,Visual Analysis,5/15/23,
Department of Agriculture,OCIO,AI26,Acquisition Approval Request Compliance Tool,"A natural language processing (NLP) model was developed to utilize the text in procurement header and line descriptions within USDA's Integrated Acquisition System (IAS) to determine the likelihood that an award is IT-related, and therefore might require an AAR. The model uses the text characteristics for awards that have an AAR number entered into IAS and then calculates the probability of being IT-related for those procurements that did not have an AAR Number entered in IAS.",Operational,Procurement,NLP,5/15/23,
Department of Agriculture,NRCS,AI27,Operational water supply forecasting for western US rivers,"Western US water management is underpinned by forecasts of spring-summer river flow volumes made using operational hydrologic models. The USDA Natural Resources Conservation Service (NRCS) National Water and Climate Center operates the largest such forecast system regionally, carrying on a nearly century-old tradition. The NWCC recently developed a next-generation prototype for generating such operational water supply forecasts (WSFs), the multi-model machine-learning metasystem (M4), which integrates a variety of AI and other data-science technologies carefully chosen or developed to satisfy specific user needs. Required inputs are data around snow and precipitation from the NRCS Snow Survey and Water Supply Forecast program SNOTEL environmental monitoring network, but are flexible.  In hindcasting test-cases spanning diverse environments across the western US and Alaska, out-of-sample accuracy improved markedly over current benchmarks. Various technical design elements, including multi-model ensemble modeling, autonomous machine learning (AutoML), hyperparameter pre-calibration, and theory-guided data science, collectively permitted automated training and operation.  Live operational testing at a subset of sites additionally demonstrated logistical feasibility of workflows, as well as geophysical explainability of results in terms of known hydroclimatic processes, belying the black-box reputation of machine learning and enabling relatable forecast storylines for NRCS customers.",Mission,River flow estimation,ML,5/15/23,
Department of Agriculture,APHIS,AI28,Standardization of cut flower business names for message set data,"Natural language processing technique. Data are cleaned (e.g., remove punctuation) to facilitate matching. Cosine similarity is calculated, similar terms are matched, and the results are output.",Operational,Data Cleansing,NLP,5/15/23,
Department of Agriculture,OCIO,AI29,Intelligent Ticket Routing,"Routes BMC Remedy tickets to proper work group automatically utilizing python, jupyterhub, scikit learn, gitlab, flask, gunicorn, nginx, erms.",Operational,IT Service Desk Routing,ML,5/15/23,
Department of Agriculture,OCIO,AI30,Predictive Maintenance Impacts,"Predict impacts of DISC maintenance on infrastructure items.  Utilizes: einblick, mysql, python, linux, tableau",Operational,IT Maintence Prediction,ML,5/15/23,
Department of Agriculture,OASCR,AI31,Artificial Intelligence SPAM Mitigation Project,"The AI Solution invoves Robotic Process Automation + AI/ML model solution to automatically classify and remove spam and marketing emails that appear in civil rights complaints email channels. A significant portion of incoming OASCR emails are spam, marketing and phishing emails. ",Operational,SPAM Filter,MLL,5/15/23,
Department of Agriculture,APHIS,AI32,"Approximate string or fuzzy matching, used to automate matching similar, but not identical, text in administrative documents","The algorithm computes a string similarity metric which can be used to classify similar strings into a single category, reducing information duplication and onerous, manual error-checking",Operational,Data Cleansing,Fuzzy Matching,5/15/23,
Department of Agriculture,APHIS,AI33,Training machine learning models to automatically read file attachments and save information into a more convenient Excel format. ,"Artificial intelligence used to automate document processing and information extraction. Program managers often need information from specific form fields that are sent as PDF email attachments. Many emailed documents are received each day, making manually opening each attachment and copying the needed information too time-consuming. ",Operational,Document Processing,ML,5/15/23,
Department of Agriculture,APHIS,AI34,Artificial Intelligence for correlative statistical analysis,"AI-type statistical techniques are used to model predictive relationships between variables. We routinely use modeling approaches such as random forest, artificial neural networks, k-nearest neighbor clustering, and support vector machines, for statistical prediction. ",Mission,Research Analysis,Neural Networks,5/15/23,
Department of Agriculture,NASS,AI35,Census of Agricuilture Response Propensity Scores,"The response propensity scores to the COA are derived from random forest models that use historical data, control data, and other survey data. These scores are used to help target more effective data collection.",Mission,Research Analysis,ML,5/15/23,
Department of Agriculture,NRCS,AI36,Ecological Site Descriptions (machine learning),"Analysis of over 20 million records of soils data and 20,000 text documents of ecological state and transition information.",Mission,Research Analysis,ML,5/15/23,
Department of Agriculture,NRCS,AI37,Conservation Effects Assessment Project,"The goal is to predict conservation benefits at the field level. The model uses farmer survey data, APEX modeling results and environmental data.",Mission,Benefit Predictions (Consumer),ML,5/15/23,
Department of Agriculture,NRCS,AI38,Digital Imagery (no-change) for NRI program,Using neural networks and other AI technologies to detect no-changes in digital imagery for the NRI (national resources inventory) program ,Mission,Landscape Identification,Neural Networks,5/15/23,
Department of Agriculture,FNS,AI39,Nutrition Education & Local Access Dashboard,"The goal of the this Dashboard is to provide a county-level visualization of FNS nutrition support, specifically nutrition education and local food access, alongside other metrics related to hunger and nutritional health. As part of this dashboard, the team developed a K-means clustering script to group States by 7 different clustering options:  Farm to School Intensity & Size, Program Activity Intensity, Ethnicity & Race, Fresh Food Access, School Size, and Program Participation. This allows users to find like-minded, or similar, States based on any of these characteristics, opening up avenues for partnerships with States that they otherwise may not have considered.",Mission,User Group Mapping (Consumer),ML,5/15/23,
Department of Commerce,International Trade Administration (ITA),AI40,B2B Matchmaking,The system's algorithms and AI technology qualifies data and makes B2B matches with event participants according to their specific needs and available opportunities. The systems inputs are data related to event participants and the outputs are suggested B2B matches between participants and a match strength scorecard.,Mission,Event Matching (Consumer),,9/22/22,
Department of Commerce,International Trade Administration (ITA),AI41,Chatbot Pilot,"Chatbot embedded into trade.gov to assist ITA clients with FAQs, locating information and content, suggesting events and services. ITA clients would enter input into the chatbot in the form of questions or responses to prompts. The chatbot would scan ITA content libraries and input from ITA staff and return answers and suggestions based on client persona (exporter, foreign buyer, investor).",Mission,Website ChatBot,,9/22/22,
Department of Commerce,International Trade Administration (ITA),AI42,Consolidated Screening List,"The Consolidated Screening List (CSL) is a list of parties for which the United States Government maintains restrictions on certain exports, reexports, or transfers of items. It consists of the consolidation of 13 export screening lists of the Departments of Commerce, State, and Treasury. The CSL search engine has “Fuzzy Name Search” capabilities, allowing a search without knowing the exact spelling of an entity’s name. In Fuzzy Name mode, the CSL returns a “score” for results that exactly or nearly match the searched name. This is particularly helpful when searching on CSL for names that have been translated into English from non-Latin alphabet languages.",Mission,Website Search function,Fuzzy Matching,9/22/22,
Department of Commerce,International Trade Administration (ITA),AI43,AD/CVD Self Initiation,"The ADCVD program investigates allegations of dumping and/or countervailing of duties. Investigations are initiated when a harmed US entity files a petition identifying the alleged offence and the specific harm inflicted. Self-Initiation will allow ITA to monitor trade patterns for this activity and preemptively initiate investigations by identifying harmed US entities, often before these entities are aware of the harm.",Mission,Dumping Identification - Initiates Compliant,,9/22/22,
Department of Commerce,International Trade Administration (ITA),AI44,Market Diversification Toolkit,The Market Diversification Tool identifies potential new export markets using current trade patterns. A user enters what products they make and the markets they currently export to. The Market Diversification Tool applies a ML algorithm to identify and compare markets that should be considered. The tool brings together product-specific trade and tariff data and economy-level macroeconomic and governance data to provide a picture of which markets make sense for further market research. Users can limit the markets in the results to only the ones they want to consider and modify how each of the eleven indicators in the tool contributes to a country’s overall score. Users can export all the data to a spreadsheet for further analysis.,Mission,Research Analysis - For Users,ML,9/22/22,
Department of Commerce,National Oceanic and Atmospheric Administration (NOAA),AI45,Fisheries Electronic Monitoring Image Library,The Fisheries Electronic Monitoring Library (FEML) will be the central repository for electronic monitoring (EM) data related to marine life.,Mission,Wildlife Imagery,,9/22/22,
Department of Commerce,National Oceanic and Atmospheric Administration (NOAA),AI46,"Passive acoustic analysis using ML in Cook Inlet, AK","Passive acoustic data is analyzed for detection of beluga whales and classification of the different signals emitted by these species. Detection and classification are done with an ensemble of 4 CNN models and weighted scoring developed in collaboration with Microsoft. Results are being used to inform seasonal distribution, habitat use, and impact from anthropogenic disturbance within Cook Inlet beluga critical habitat. The project is aimed to expand to other cetacean species as well as anthropogenic noise.",Mission,Wildlife Identification,,9/22/22,
Department of Commerce,National Oceanic and Atmospheric Administration (NOAA),AI47,AI-based automation of acoustic detection of marine mammals,"Timely processing of these data is critical for adapting mitigation measures as climate change continues to impact Arctic marine mammals. Infrastructure for Noise and Soundscape Tolerant Investigation of Nonspecific Call Types (INSTINCT) is command line software which was developed in-house for model training, evaluation, and deployment of machine learning models for the purpose of marine mammal detection in passive acoustic data. It also includes annotation workflows for labeling and validation. INSTINCT has been successfully deployed in several analyses, and further development of detectors within INSTINCT is desired for future novel studies and automation. Continued integration of AI methods into existing processes of the CAEP acoustics group requires a skilled operator familiar with INSTINCT, machine learning, and acoustic repertoire of Alaska region marine mammals.",Mission,,,9/22/22,
Department of Commerce,National Oceanic and Atmospheric Administration (NOAA),AI48,Developing automation to determine species and count using optical survey data in the Gulf of Mexico,"VIAME - This project focuses on optical survey collected in the Gulf of Mexico: 1) develop an image library of landed catch, 2) develop of automated image processing (ML/DL) to identify and enumerate species from underwater imagery and 3) develop automated algorithms to process imagery in near real time and download information to central database.",,,,9/22/22,
Department of Commerce,National Oceanic and Atmospheric Administration (NOAA),AI49,Fast tracking the use of VIAME for automated identification of reef fish,We've been compiling image libraries for use in creating automated detection and classification models for use in automating the annotation process for the SEAMAP Reef Fish Video survey of the Gulf of Mexico. This work is being conducted in VIAME but we're looking at several other paths forward in the project to identify best performing models. Current status is that models are performing well enough that we will incorporate automated analysis in video reads this spring as part of a supervised annotation-qa/qc process.,,,,9/22/22,
Department of Commerce,National Oceanic and Atmospheric Administration (NOAA),AI50,A Hybrid Statistical-Dynamical System for the Seamless Prediction of Daily Extremes and Subseasonal to Seasonal Climate Variability,"Demonstrate the skill and suitability for operations of a statistical- dynamical prediction system that yields seamless probabilistic forecasts of daily extremes and sub seasonal-toseasonal temperature and precipitation. We recently demonstrated a Bayesian statistical method for post-processing seasonal forecasts of mean temperature and precipitation from the North American Multi-Model Ensemble (NMME). We now seek to test the utility of an updated hybrid statistical-dynamical prediction system that facilitates seamless sub seasonal and seasonal forecasting. Importantly, this method allows for the representation of daily extremes consistent with climate conditions. This project explores the use of machine learning.",,,,9/22/22,
Department of Commerce,National Oceanic and Atmospheric Administration (NOAA),AI51,FathomNet,"FathomNet provides much-needed training data (e.g., annotated, and localized imagery) for developing machine learning algorithms that will enable fast, sophisticated analysis of visual data. We've utilized interns and college class curriculums to localize annotations on NOAA video data for inclusion in FathomNet and to begin training our own algorithms.",,,,9/22/22,
Department of Commerce,National Oceanic and Atmospheric Administration (NOAA),AI52,ANN to improve CFS T and P outlooks,"Fan Y., Krasnopolsky, V., van den Dool H., Wu, C. , and Gottschalck J. (2021). Using Artificial Neural Networks to Improve CFS Week 3-4 Precipitation and Temperature Forecasts.",,,,9/22/22,
Department of Commerce,National Oceanic and Atmospheric Administration (NOAA),AI53,"Drought outlooks by using ML
techniques",Drought outlooks by using ML techniques with NCEP models. Simple NN and Deep Learning techniques used for GEFSv12 to predict Week 1-5 Prcp & T2m over CONUS,,,,9/22/22,
Department of Commerce,National Oceanic and Atmospheric Administration (NOAA),AI54,EcoCast: A dynamic ocean management tool to reduce bycatch and support sustainable fisheries,Operational tool that uses boosted regression trees to model the distribution of swordfish and bycatch species in the California Current,,,,9/22/22,
Department of Commerce,National Oceanic and Atmospheric Administration (NOAA),AI55,Coastal Change Analysis Program (C-CAP),"Beginning in 2015, C-CAP embarked on operational high resolution land cover development effort that utilized geographic object-based image analysis and ML algorithms such as Random Forest to classify coastal land cover from 1m multispectral imagery. More recently, C-CAP has been relying on a CNN approach for the deriving the impervious surface component of their land cover products. The majority of the work is accomplished through external contracts. Prior to the high-res effort, C-CAP focused on developing Landsat based moderate resolution multi-date land cover for the coastal U.S. In 2002, C-CAP adopted a methodology that employed Classification and Regression Trees for land cover data development.",,,,9/22/22,
Department of Commerce,National Oceanic and Atmospheric Administration (NOAA),AI56,Deep learning algorithms to automate right whale photo id,"AI for right whale photo id began with a Kaggle competition and has since expanded to include several algorithms to match right whales from different viewpoints (aerial, lateral) and body part (head, fluke, peduncle). The system is now live and operational on the Flukebook platform for both North Atlantic and southern right whales. We have a paper in review at Mammalian Biology.",,,,9/22/22,
Department of Commerce,National Oceanic and Atmospheric Administration (NOAA),AI57,NN Radiation,"Developing fast and accurate NN LW- and SW radiations for GFS and GEFS. NN LW- and SW radiations have been successfully developed for previous version of GFS, see: doi: 10.1175/2009MWR3149.1 and the stability and robustness of the approach used was demonstrated, see: https://arxiv.org/ftp/arxiv/papers/2103/2103.07024.pdf NN LW- and SW radiations will be developed for the current versions of for GFS and GEFS.",,,,9/22/22,
Department of Commerce,National Oceanic and Atmospheric Administration (NOAA),AI58,NN training software for the new generation of NCEP models,Optimize NCEP EMC Training and Validation System for efficient handling of high spatial resolution model data produced by the new generation of NCEP's operational models,,,,9/22/22,
Department of Commerce,National Oceanic and Atmospheric Administration (NOAA),AI59,Coral Reef Watch,"For more than 20 years, NOAA Coral Reef Watch (CRW) has been using remote sensing, modeled, and in situ data to operate a Decision Support System (DSS) to help resource managers (our target audience), researchers, decision makers, and other stakeholders around the world prepare for and respond to coral reef ecosystem stressors, predominantly resulting from climate change and warming of the Earth's oceans. Offering the world's only global early-warning system of coral reef ecosystem physical environmental changes, CRW remotely monitors conditions that can cause coral bleaching, disease, and death; delivers information and early warnings in near real-time to our user community; and uses operational climate forecasts to provide outlooks of stressful environmental conditions at targeted reef locations worldwide. CRW products are primarily sea surface temperature (SST)-based but also incorporate light and ocean color, among other variables",,,,9/22/22,
Department of Commerce,National Oceanic and Atmospheric Administration (NOAA),AI60,Robotic microscopes and machine learning algorithms remotely and autonomously track lower trophic levels for improved ecosystem monitoring and assessment,"Phytoplankton are the foundation of marine food webs supporting fisheries and coastal communities. They respond rapidly to physical and chemical oceanography, and changes in phytoplankton communities can impact the structure and functioning of food webs. We use a robotic microscope called an Imaging Flow Cytobot (IFCB) to continuously collect images of phytoplankton from seawater. Automated taxonomic identification of imaged phytoplankton uses a supervised machine learning approach (random forest algorithm). We deploy the IFCB on fixed (docks) and roving (aboard survey ships) platforms to autonomously monitor phytoplankton communities in aquaculture areas in Puget Sound and in the California Current System. We map the distribution and abundance of phytoplankton functional groups and their relative food value to support fisheries and aquaculture and describe their changes in relation to ocean and climate variability and change.",,,,9/22/22,
Department of Commerce,National Oceanic and Atmospheric Administration (NOAA),AI61,Edge AI survey payload development,"Continued support of multispectral aerial imaging payload running detection model pipelines in real-time. This is a nine camera (color, infrared, ultraviolet) payload controlled by dedicated on-board computers with GPUs. YOLO detection models run at a rate faster than image collection, allowing real-time processing of imagery as it comes off the cameras. Goals of effort are to reduce overall data burden (by TBs) and reduce the data processing timeline, expediting analysis and population assessment for arctic mammals.",,,,9/22/22,
Department of Commerce,National Oceanic and Atmospheric Administration (NOAA),AI62,Ice seal detection and species classification in multispectral aerial imagery,"Refine and improve detection and classification pipelines with the goal of reducing false positive rates (to < 50%) while maintaining > 90% accuracy and significantly reducing or eliminating the labor intensive, post survey review process.",,,,9/22/22,
Department of Commerce,National Oceanic and Atmospheric Administration (NOAA),AI63,First Guess Excessive Rainfall Outlook,Machine Learning Product that is a first guess for the WPC Excessive Rainfall Outlook - It is learned from the ERO with atmospheric variables. It is for the Day 4-7 products,,,,9/22/22,
Department of Commerce,National Oceanic and Atmospheric Administration (NOAA),AI64,"CoralNet: Ongoing operational use, improvement, and development, of machine vision point classification","CoralNet is our operational point annotation software for benthic photo quadrat annotation. Our development of our classifiers has allowed us to significantly reduce our human annotation, and we continue to co-develop (and co-fund) new developments in CoralNet,",,,,9/22/22,
Department of Commerce,National Oceanic and Atmospheric Administration (NOAA),AI65,Automated detection of hazardous low clouds in support of safe and efficient transportation,"This is a maintenance and sustainment project for the operational GOES-R fog/low stratus (FLS) products. The FLS products are derived from the combination of GOES-R satellite imagery and NWP data using machine learning. The FLS products, which are available in AWIPS, are routinely used by the NWS Aviation Weather Center and Weather Forecast Offices.",,,,9/22/22,
Department of Commerce,National Oceanic and Atmospheric Administration (NOAA),AI66,The Development of ProbSevere v3 - An improved nowcasting model in support of severe weather warning operations,"ProbSevere is a ML model that utilizes NWP, satellite, radar, and lightning data to nowcast severe wind, severe hail, and tornadoes. ProbSevere, which was transitioned to NWS operations in October 2020, is a proven tool that enhances operational severe weather warnings. This project aims to develop the next version of ProbSevere, ProbSevere v3. ProbSevere v3 utilizes additional data sets and improved machine learning techniques to improve upon the operational version of ProbSevere. ProbSevere v3 was successfully demonstrated in the 2021 Hazardous Weather Testbed and a JTTI proposal was recently submitted to facilitate an operational update. The development is funded by GOES-R.",,,,9/22/22,
Department of Commerce,National Oceanic and Atmospheric Administration (NOAA),AI67,"The VOLcanic Cloud Analysis Toolkit (VOLCAT): An application system for detecting, tracking, characterizing, and forecasting hazardous volcanic events","Volcanic ash is a major aviation hazard. The VOLcanic Cloud Analysis Toolkit (VOLCAT) consists of several AI powered satellite applications including: eruption detection, alerting, and volcanic cloud tracking. These applications are routinely utilized by Volcanic Ash Advisory Centers to issue volcanic ash advisories. Under this project, the VOLCAT products will be further developed, and subsequently transitioned to the NESDIS Common Cloud Framework, to help ensure adherence to new International Civil Aviation Organization requirements.",,,,9/22/22,
Department of Commerce,National Oceanic and Atmospheric Administration (NOAA),AI68,SUVI Thematic Maps,"The GOES-16 Solar Ultraviolet Imager (SUVI) is NOAA's operational solar extremeultraviolet imager. The SUVI Level 2 Thematic Map files in these directories are produced by NOAA's National Centers for Environmental Information in Boulder, Colorado. These data have been processed from Level 2 High Dynamic Range (HDR) composite SUVI images. The FITS file headers are populated with metadata to facilitate interpretation by users of these observations. Please note that these files are considered to be experimental and thus will be improved in future releases. Users requiring assistance with these files can contact the NCEI SUVI team by emailing goesr.suvi@noaa.gov. The SUVI Thematic Maps product is a Level 2 data product that (presently) uses a machine learning classifier to generate a pixel-by-pixel map of important solar features digested from all six SUVI spectral channels.",,,,9/22/22,
Department of Commerce,National Oceanic and Atmospheric Administration (NOAA),AI69,"BANTER, a machine learning acoustic event classifier",A supervised machine learning acoustic event classifier using hierarchical random forests,,,,9/22/22,
Department of Commerce,National Oceanic and Atmospheric Administration (NOAA),AI70,ProbSR (probability of subfreezing roads,A machine-learned algorithm that provides a 0-100% probability roads are subfreezing,,,,9/22/22,
Department of Commerce,National Oceanic and Atmospheric Administration (NOAA),AI71,VIAME: Video and Image Analysis for the Marine Environment Software Toolkit,"The Video and Image Analysis for the Marine Environment Software Toolkit, commonly known as VIAME, is an open-source, modular software toolkit that allows users to employ high-level, deep-learning algorithms for automated annotation of imagery using a low code/no code graphical user interface. VIAME is available free of charge to all NOAA users. The NOAA Fisheries Office of Science and Technology supports an annual maintenance contract covering technical and customer support by the developer, routine software updates, bug fixes, and development efforts that support broad, cross center application needs.",,,,9/22/22,
Department of Commerce,National Oceanic and Atmospheric Administration (NOAA),AI72,ENSO Outlooks using observed/analyzed fields,LSTM model that uses ocean and atmospheric predictors throughout the tropical Pacific to forecast ONI values up to 1 year in advance. An extension of this was submitted to the cloud portfolio with the intent of adding a CNN layer that that uses reforecast data to improve the ONI forecasts.,,,,9/22/22,
Department of Commerce,National Oceanic and Atmospheric Administration (NOAA),AI73,Using community-sourced underwater photography and image recognition software to study green sea turtle distribution and ecology in southern California,"The goal of this project is to study green turtles in and around La Jolla Cove in the San Diego Region-a highly populated site with ecotourism-by engaging with local photographers to collect green turtle underwater images. The project uses publicly available facial recognition software (HotSpotter) to identify individual turtles, from which we determine population size, residency patterns, and foraging ecology",,,,9/22/22,
Department of Commerce,National Oceanic and Atmospheric Administration (NOAA),AI74,An Interactive Machine Learning Toolkit for Classifying Species Identity of Cetacean Echolocation Signals in Passive Acoustic Recordings,"Develop robust automated machine learning detection and classification tools for acoustic species identification of toothed whale and dolphin echolocation clicks for up to 20
species found in the Gulf of Mexico. Tool development project funded from June 2018 to  May 2021. Tool will be used for automated analyses of long-term recordings from Gulfwide passive acoustic moored instruments deployed from 2010-2025 to look at environmental processes driving trends in marine mammal density and distribution.",,,,9/22/22,
Department of Commerce,National Oceanic and Atmospheric Administration (NOAA),AI75,Steller sea lion automated count program,"NOAA Fisheries Alaska Fisheries Science Center's Marine Mammal Laboratory (MML) is mandated to monitor the endangered western Steller sea lion population in Alaska. MML conducts annual aerial surveys of known Steller sea lion sites across the southern Alaska coastline to capture visual imagery. It requires two full-time, independent counters to process overlapping imagery manually (to avoid double counting sea lions in multiple frames), and count and classify individuals by age and sex class. These counts are vital for population and ecosystem-based modeling to better understand the species and ecosystem, to inform sustainable fishery management decisions, and are eagerly anticipated by stakeholders like the NOAA Alaska Regional Office, industry, and environmental groups. MML worked with Kitware to develop detection and image registration pipelines with VIAME (updates to the DIVE program to support updated interface needs). MML is now working to assess the algorithms efficacy and develop a workflow to augment the traditional counting method (to RL 9).",,,,9/22/22,
Department of Commerce,National Oceanic and Atmospheric Administration (NOAA),AI76,Steller sea lion brand sighting,"Detection and identification of branded steller sea lions from remote camera images in the western Aleutian Islands, AK. The goal is to help streamline photo processing to reduce the effort required to review images.",,,,9/22/22,
Department of Commerce,National Oceanic and Atmospheric Administration (NOAA),AI77,Replacing unstructured WW3 in the Great Lakes with a Recurrent neural network and a boosted ensemble decision tree,"Investigated replacing unstructured WW3 in the Great Lakes with (i) a Recurrent Neural Network (RNN, especially an LSTM) developed by EMC and (ii) a boosted ensemble decision tree (XGBoost) developed by GLERL. These two AI models were trained on two decades of wave observations in Lake Erie and compared to the operational Great Lakes unstructured WW3.",,,,9/22/22,
Department of Commerce,National Oceanic and Atmospheric Administration (NOAA),AI78,Using k-means clustering to identify spatially and temporally consistent wave systems,"Postprocessing that uses k-means clustering to identify spatially and temporally consistent wave systems from the output of NWPS v1.3. Has been successfully evaluated in the field by NWS marine forecasters nationwide and has been implemented into operations on February 3, 2021.",,,,9/22/22,
Department of Commerce,National Oceanic and Atmospheric Administration (NOAA),AI79,Picky,Using CNN to pick out objects of a particular size from sides scan imagery. Presents users with a probability that allows for automation of contact picking in the field. Side scan imagery is simple one channel intensity image which lends itself well to basic CNN techniques.,,,,9/22/22,
Department of Commerce,National Telecommunications and Information Administration (NTIA),AI80,Data Science: Clutter,"NTIA’s Institute for Telecommunication Sciences (ITS) is investigating the use of AI to automatically identify and classify clutter obstructed radio frequency propagation paths. Clutter is vegetation, buildings, and other structures that cause radio signal loss through dispersion, reflection, and diffraction. It does not include terrain effects. The classifier is a convolutional neural network (CNN) trained using lidar data coinciding with radio frequency propagation measurements made by ITS. This trained CNN can be fed new radio path lidar data and a clutter classification label is predicted.",,,,9/22/22,
Department of Commerce,National Telecommunications and Information Administration (NTIA),AI81,WAWENETS,The algorithm produces estimates of telecommunications speech quality and speech intelligibility. The input is a recording of speech from a telecommunications system in digital file format. The output is a single number that indicates speech quality (typically on a 1 to 5 scale) or speech intelligibility (typically on a 0 to 1 scale).,,,,9/22/22,
Department of Commerce,Minority Business Development Administration (MBDA),AI82,Azure Chatbot,Azure Chatbot is being leveraged to automate and streamline the user response to potential questions for MBDA users while interacting with the external facing MBDA website. The solution leverages AI based chatbot response coupled with Machine Learning and Natural Language Processing capabilities,,,,9/22/22,
Department of Commerce,United States Patent and Trade Office (USPTO),AI83,AI retrieval for patent search,"Augmentation for next generation patent search tool to assist examiners identify relevant documents and additional areas to search. System takes input from published or unpublished applications and provides recommendations on further prior art areas to search, giving the user the ability to sort by similarity to concepts of their choosing.",,,,9/22/22,
Department of Commerce,United States Patent and Trade Office (USPTO),AI84,AI use for CPC classification,System that classifies incoming patent application based on the cooperative patent classification scheme for operational assignment of work and symbol recommendation for aI search. Backoffice processing system that uses incoming patent applications as input and outputs the resulting classification symbols.,,,,9/22/22,
Department of Commerce,United States Patent and Trade Office (USPTO),AI85,"AI retrieval for TM design coding
and Image search","Clarivate COTS solution to assist examiner identification of similar trademark images, to suggest the correct assignment of mark image design codes, and to determine the potential acceptability of the identifications of goods and services. System is anticipated to use both incoming trademark images and registered trademark images and output design codes and/or other related images.",,,,9/22/22,
Department of Commerce,United States Patent and Trade Office (USPTO),AI86,Enriched Citation,"Data dissemination system that identifies which references, or prior art, were cited in specific patent application office actions, including: bibliographic information of the reference, the claims that the prior art was cited against, and the relevant sections that the examiner relied upon. System extracts information from unstructured office actions and provides the information through a structured public facing API.",,,,9/22/22,
Department of Commerce,United States Patent and Trade Office (USPTO),AI87,Inventor Search Assistant (iSAT),"Service to help inventors ""get started"" identifying relevant documents, figures, and classification codes used to conduct a novelty search. System takes a user entered short description of invention and provides a user selectable set of recommended documents, figures, and classification areas.",,,,9/22/22,
Department of Education,Federal Student Aid,AI88,Aidan Chatbot,"FSA's virtual assistant uses natural language processing to answer common financial aid questions and help customers get information about their federal aid on StudentAid.gov. In just over two years, Aidan has interacted with over 2.6 million unique customers, resulting in more than 11 million user messages.",,,,2/3/23,
Department of Energy,Idaho National Laboratory,AI89,"Advances in Nuclear Fuel Cycle Nonproliferation, Safeguards, and Security Using an Integrated Data Science Approach","This research will develop a digital twin of a centrifugal contactor system that receives data from traditional and real time sensors, constructs a digital representation or simulation of the chemical separations component within the nuclear fuel cycle, and performs data analysis through machine learning to determine anomalies, failures, and trends. The research will include the identification and implementation of advanced artificial intelligence, machine learning, and data analysis techniques advised by a team of nuclear safeguards experts.",,,,7/1/22,
Department of Energy,Idaho National Laboratory,AI90,Development of a multi-sensor data science system used for signature development on solvent extraction processes conducted within Beartooth facility,"This project will develop a system that utilizes non-traditional measurement sources such as vibration, acoustics, current, and light, and traditional sources such as flow, and temperature in conjunction with data-based, machine learning techniques that will allow for signal discovery. The goal is to characterize stages within a solvent extraction process can increase target metals recovery, indicate process faults, account for special nuclear material, and inform near real-time decision making.",,,,7/1/22,
Department of Energy,Idaho National Laboratory,AI91,Scalable Framework of Hybrid Modeling with Anticipatory Control Strategy for Autonomous Operation of Modular and Microreactors,"The goal this research is to develop and validate novel and scalable models to achieve faster-than-real-time prediction and decision-making capabilities. To achieve the project goal of autonomous operation of microreactors, a novel hybrid modeling approach combining both physics-based and artificial intelligence techniques will be developed at the component or sub-system level, integrated with anticipatory control techniques, and scaled. A novel distributed anticipatory control strategy will be developed as part of the scalability analysis to understand the risk of cascading failures when emerging reactors are deployed as part of a full feeder microgrid.",,,,7/1/22,
Department of Energy,Idaho National Laboratory,AI92,Accelerating and Improving the Reliability of Low Failure Probability Computations to Support the Efficient Safety Evaluation and Deployment of Advanced Reactor Technologies,"This project will research artificial intelligence enabled Monte Carlo algorithms to significantly reduce the computational burden by reducing the number of finite element evaluations when estimating low failure probabilities. These will be implemented in the Multiphysics Object-Oriented Simulation Environment, which will help the nuclear engineering community to efficiently conduct probabilistic failure analyses and uncertainty quantification studies for the design and optimization of advanced reactor technologies.",,,,7/1/22,
Department of Energy,Idaho National Laboratory,AI93,Accelerating deployment of nuclear fuels through reduced-order thermo- physical property models and machine learning,"This project will develop a novel physics-based tool that combines 1) reduced-order models, 2) machine learning algorithms, 3) fuel performance methods, and 4) state-of- the-art thermal property characterization equipment and irradiated nuclear fuel data sets to accelerate nuclear fuel discovery, development, and deployment. The models will describe thermal conductivity, specific heat, thermal expansion, and self-diffusion coefficients as a function of temperature and irradiation.",,,,7/1/22,
Department of Energy,Idaho National Laboratory,AI94,Promoting Optimal Sparse Sensing and Sparse Learning for Nuclear Digital Twins,"This project will address the efficient use of limited experimental data available for nuclear digital twin (NDT) training and demonstration. This involves developing sparse data reconstruction methods and using NDT models to define sensor requirements (location, number, accuracy) for the design of demonstration experiments. NDTs  should leverage 1) sparse sensing for identifying optimal locations and the minimal set of required sensors and 2) sparse learning and recovery of full maps of responses of interest for stronger prediction, diagnostics, and prognostics capabilities.",,,,7/1/22,
Department of Energy,Idaho National Laboratory,AI95,Artificial Intelligence Enhanced Advanced Post Irradiation Examination,"This project uses post irradiation examination of uranium-10wt.% zirconium (UZr) metallic fuel as a case study to show how artificial intelligence (AI)-based technology can facilitate and accelerate nuclear fuel development. The approach will 1) revisit the microstructural image and local thermal conductivity data collected from UZr, 2) build a benchmark dataset for the microstructural patterns of irradiated UZr, and 3) train the machine learning and deep learning models to uncover the relationships between micro/nanoscale structure, zirconium phase redistribution, local thermal conductivity, and engineering scale fuel properties.",,,,7/1/22,
Department of Energy,Idaho National Laboratory,AI96,Secure Millimeter Wave Spectrum Sharing with Autonomous Beam Scheduling,This approach exploits the millimeter wave beam directionality and utilizes the beam sensing capabilities at end devices to prove that an autonomous radio frequency  beam scheduler can support secure 5G spectrum sharing and guarantee optimality for base stations. Measurements and predictive analytics are used to develop the autonomous beam scheduling algorithms. These improvements will benefit mission critical communications and emergency response operations as well as enable   secure communication for critical infrastructure without expensive and competitive licensed bands.,,,,7/1/22,
Department of Energy,Brookhaven National Laboratory,AI97,Objective-Driven Data Reduction for Scientific Workflows,"This project aims to develop theories and algorithms for objective-driven reduction of scientific data in workflows that are composed of various models, including data- driven AI models",,,,7/1/22,
Department of Energy,Office of Electricity,AI98,The Grid Resilience and Intelligence Platform (GRIP),"AI within GRIP is used to develop metrics that quantify the impact of the anticipated weather related extreme events. The platform uses utility data combined with physical models, distribution power solver  to infer the potential  grid impacts given a major storm.",,,,7/1/22,
Department of Energy,Office of Electricity,AI99,Open-Source High-Fidelity Aggregate Composite Load Models of Emerging Load Behaviors for Large-Scale Analysis (GMLC 0064),"1. Machine learning methods such as cross-correlation, random forest, regression tree and transfer learning are used to estimate the load composition data and motor protection profiles for different climante regions in the Western US
2. Deep learning algorithm is appplied to calibrate the parameters of WECC composite load model to match the responses with detailed feeder model",,,,7/1/22,
Department of Energy,Office of Electricity,AI100,Big Data Synchrophasor Monitoring and Analytics for Resiliency Tracking (BDSMART),"Explore the use of big data, artificial intelligence (AI), and machine learning technology and tools on phasor measurement unit (PMU) data to identify and improve existing knowledge, and to discover new insights and tools for better grid operation and management.",,,,7/1/22,
Department of Energy,Office of Electricity,AI101,Combinatorial Evaluation of Physical Feature Engineering and Deep Temporal Modeling  for Synchrophasor Data at Scale,"Explore the use of big data, artificial intelligence (AI), and machine learning technology and tools on phasor measurement unit (PMU) data to identify and improve existing knowledge, and to discover new insights and tools for better grid operation and management.",,,,7/1/22,
Department of Energy,Office of Electricity,AI102,MindSynchro,"Explore the use of big data, artificial intelligence (AI), and machine learning technology and tools on phasor measurement unit (PMU) data to identify and improve existing knowledge, and to discover new insights and tools for better grid operation and management.",,,,7/1/22,
Department of Energy,Office of Electricity,AI103,PMU-Based Data Analytics Using Digital Twin Phasor Analytics Software,"Explore the use of big data, artificial intelligence (AI), and machine learning technology and tools on phasor measurement unit (PMU) data to identify and improve existing knowledge, and to discover new insights and tools for better grid operation and management.",,,,7/1/22,
Department of Energy,Office of Electricity,AI104,A Robust Event Diagnostic Platform: Integrating Tensor Analytics and Machine Learning Into Real-time Grid Monitoring,"Explore the use of big data, artificial intelligence (AI), and machine learning technology and tools on phasor measurement unit (PMU) data to identify and improve existing knowledge, and to discover new insights and tools for better grid operation and management.",,,,7/1/22,
Department of Energy,Office of Electricity,AI105,"Discovery of Signatures, Anomalies, and Precursors in Synchrophasor Data with Matrix Profile and Deep Recurrent Neural Networks","Explore the use of big data, artificial intelligence (AI), and machine learning technology and tools on phasor measurement unit (PMU) data to identify and improve existing knowledge, and to discover new insights and tools for better grid operation and management.",,,,7/1/22,
Department of Energy,Office of Electricity,AI106,Machine Learning Guided Operational Intelligence,"Explore the use of big data, artificial intelligence (AI), and machine learning technology and tools on phasor measurement unit (PMU) data to identify and improve existing knowledge, and to discover new insights and tools for better grid operation and management.",,,,7/1/22,
Department of Energy,Office of Electricity,AI107,Robust Learning of Dynamic Interactions for Enhancing Power System Resilience,"Explore the use of big data, artificial intelligence (AI), and machine learning technology and tools on phasor measurement unit (PMU) data to identify and improve existing knowledge, and to discover new insights and tools for better grid operation and management.",,,,7/1/22,
Department of Energy,Idaho National Laboratory,AI108,Artificial Intelligence Based Process Control and Optimization for Advanced Manufacturing,"This project will develop the capability to intelligently control and optimize advanced manufacturing processes instead of the existing trial and error approach. To achieve this goal, artificial intelligence (AI) based control algorithms will be developed by employing deep reinforcement learning. To reduce the computational expense with advanced manufacturing models, physics-informed reduced order models (ROMs) will be developed. The AI-based control algorithms will employ the ROMs’ predictions to adaptively inform processing decisions in a simulation environment.",,,,7/1/22,
Department of Energy,Idaho National Laboratory,AI109,Smart Contingency Analysis Neural Network for in-depth Power Grid Vulnerability Analyses,"Typical contingency analysis for a power utility is limited to n-1 due to computational complexity and cost. A machine learning framework and resilience-chaos plots are leveraged to reduce computational expense required to discover, with 90% accuracy, n-2 contingencies by 50%.",,,,7/1/22,
Department of Energy,Idaho National Laboratory,AI110,Resilient Attack Interceptor for Intelligent Devices,The Resilient Attack Interceptor for Intelligent Devices approach focuses on developing external monitoring methods to protect industrial internet of things devices by correlating observable physical aspects that are produced naturally and involuntarily during the operational lifecycle with anomalous functionality.,,,,7/1/22,
Department of Energy,Idaho National Laboratory,AI111,Infrastructure eXpression,"The project developed a framework and process to translate industrial control system features to a machine-readable format for use with automated cyber tools. This research also examined other current and evolving standards for usability with diverse grid architectures that represent a set of variable conditions to establish the   foundation for determining where future research should focus and to support improvements to industry standards and architecture designs for machine-learning cyber defense solutions. This project’s success can serve as the foundation for prioritizing the next research steps to realize automated threat response, improving   the timeliness and fidelity of cyber incident consequence models, and enriching  national capabilities to share actionable threat intelligence at machine speed.",,,,7/1/22,
Department of Energy,Idaho National Laboratory,AI112,Protocol Analytics to enable Forensics of Industrial Control Systems,"The goal of this research is to discover methods and technologies to bridge gaps between the various industrial control systems (ICS) communication protocols and standard Ethernet to enable existing cybersecurity tools defend ICS networks and empower cybersecurity analysts to detect compromise before threat actors can disrupt infrastructure, damage property, and inflict harm. Research focuses on electronic signal analysis of captured communication to determine the protocol, using use machine learning to identify unknown protocols. Findings will be incorporated into  a prototype device.",,,,7/1/22,
Department of Energy,Idaho National Laboratory,AI113,Automated Type and Data Structure Resolution,This research identified and labeled type and structure data in an automated and scalable way such that the information can be used in other tools and other Reverse Engineering at Scale research areas such as symbolic execution. This was done initially by utilizing heuristic methods and then scaled by adopting a machine learning approach.,,,,7/1/22,
Department of Energy,Idaho National Laboratory,AI114,Signal Decomposition for Intrusion Detection in Reliability Assessment in Cyber Resilience,"The objective of this project is to research, assess, and implement machine learning and artificial intelligence and physics-based algorithms for signal decomposition and provide a straightforward framework wherein an anomaly detection algorithm can be trained on existing expected data and then used for false data injection detection. An advanced library for signal decomposition and analysis will be developed that allows combining machine learning and artificial intelligence algorithms and high-fidelity model comparisons for greatly improved false data injection detection. This library will facilitate online and posteriori analysis of digital signals for the purpose of detecting potential malicious tampering in physical processes.",,,,7/1/22,
Department of Energy,Idaho National Laboratory,AI115,Advanced Machine Learning-based Fifth Generation Network Attack Detection System,"The project goal is to prove that enhancing attack detection via innovative machine learning and artificial intelligence techniques into the fifth generation (5G) cellular network can help to secure mission-critical applications, such as automated vehicles and drones, connected health, emergency response operations, and other mission- critical devices that either are or will be connected to the 5G cellular network.",,,,7/1/22,
Department of Energy,Idaho National Laboratory,AI116,Red Teaming Artificial Intelligence,"This research will advance the state of the art for red team security assessment of machine learning and artificial intelligence systems by providing methods for the reverse engineering, exploitation, risk assessment and vulnerability remediation. The insights gained from the explorations into vulnerability assessment research will proactively address critical gaps in the cybersecurity community’s understanding of these systems and can be used to create appropriate risk evaluation metrics and provide best practices for inclusion into consequence-driven cyber-informed engineering.",,,,7/1/22,
Department of Energy,Idaho National Laboratory,AI117,Unattended Operation through Digital Twin Innovations,The team hypothesizes that artificial intelligence can predict events using the integrated data from test bed sensors and physics-based models. A second hypothesis is that integrating software and artificial intelligence with sensor data from a test bed will lead to a framework for future digital twins. The team will train artificial intelligence models to determine what attributes are most important for enabling intelligent autonomous control and will determine best practices for digital twin cybersecurity.,,,,7/1/22,
Department of Energy,Idaho National Laboratory,AI118,Secure and Resilient Machine Learning System for Detecting Fifth Generation (5G) Attacks including Zero-Day Attacks,This project will implement an advanced machine learning based 5G attack detection system that can achieve high classification speed (10k packets per second) with high accuracy (90% or greater) as well as address a vulnerability to zero-day attacks  (90% accuracy against real zero-day attacks recorded by Amazon Web Services) using field programmable gate array based deep autoencoders.,,,,7/1/22,
Department of Energy,Idaho National Laboratory,AI119,Automated Malware Analysis Via Dynamic Sandboxes,"The goal of this project is to develop an analysis framework enabled by dynamic sandboxes that allows for automated analysis, provides non-existing core capabilities to analyze industrial control system malware, and outputs to a format that is machine readable and an industry standard in sharing threat information. This will enable further analysis efforts via machine learning and provide a foundational platform that would allow for timely, automated analysis of malware samples.",,,,7/1/22,
Department of Energy,Idaho National Laboratory,AI120,Interdependent Infrastructure Systems Resilience Analysis for Enhanced Microreactor Power Grid Penetration,"This project will develop machine learning enabled integrated resource planning methodologies to help quantify key resilience elements across integrated energy systems and their vulnerabilities to threats and hazards. This includes the ability to accurately analyze and visualize a region’s critical infrastructure systems ability to sustain impacts, maintain critical functionality, recover from disruptive events. This advanced decision support capability can improve our understanding of these complex relationships and help predict the potential impacts that microreactors and distributed energy resources have on the reliability and resiliency of our energy systems.",,,,7/1/22,
Department of Energy,Idaho National Laboratory,AI121,Adaptive Fingerprinting of Control System Devices through Generative Adversarial Networks,This project focuses on the reduction of manual labor and operational cost required  for training an electromagnetic (EM)-based anomaly detection system for legacy industrial control systems devices and Industrial Internet of Things. This research would enable EM-based intrusion detection systems to be deployed to protect legacy control systems.,,,,7/1/22,
Department of Energy,Idaho National Laboratory,AI122,"Support Vector Analysis for Computational Risk Assessment, Decision-Making, and Vulnerability Discovery in Complex Systems","This project addressed limitations in current probabilistic risk assessment (PRA) by combining a support vector machine and PRA software to auto-detect system design vulnerabilities and find previously unseen issues, reduce human error, and reduce human costs. This method does not require training data that would only be available in the event of system or subsystem failures.",,,,7/1/22,
Department of Energy,Idaho National Laboratory,AI123,Deep Reinforcement Learning and Decision Analytics for Integrated Energy Systems,"This project will develop a novel deep reinforcement learning approach that can manage distributed or tightly coupled multi-agent systems utilizing deep neural networks for automatic system representation, modeling, and end-to-end learning. This new control method will enable complex, nonlinear system optimization over timescales from milliseconds to months.",,,,7/1/22,
Department of Energy,Idaho National Laboratory,AI124,"Nuclear-Renewable-Storage Digital Twin: Enhancing Design, Dispatch, and Cyber Response of Integrated Energy Systems","This project will develop a learning-based and digital twin enabled modeling and simulation framework for economic and resilient real-time decision-making of physics- informed integrated energy systems (IES) operation. High-fidelity physics models will be linked with large-scale grid monitoring data to provide real-time updates of IES states, predictive control systems, and optimized power dispatch solutions. Learning- based algorithms will make real-time decisions upon detection of component contingencies caused by climate-induced or man-made extreme events, such as cyber-attacks or extreme weather, thereby mitigating their impacts through appropriate counter measures.",,,,7/1/22,
Department of Energy,Idaho National Laboratory,AI125,Automated Infrastructure & Dependency Detection via Satellite Imagery and Dependency Profiles,"Computer vision, a broad set of techniques for training statistical models and neural networks to process images, has advanced substantially in recent years. Applying these capabilities to satellite imagery can improve critical infrastructure analysis and interdependency data build-outs. Combining advanced computer vision techniques, a functional taxonomic approach to critical infrastructure, and the unique geo-spatial and dependency datasets the research team developed can produce innovative and state- of-the-art image processing results that advance abilities to secure and defend national critical infrastructure.",,,,7/1/22,
Department of Energy,Idaho National Laboratory,AI126,Accelerated Nuclear Materials and Fuel Qualification by Adopting a First to Failure Approach,"Physics-based multi-scale modeling was coupled with deep, recursive, and transfer learning approaches to accelerate nuclear materials research and qualification of high- entropy alloys. Applying AI to combinatorial-based materials research enables subsequent analysis to focus on a limited number of candidates predicted to have the necessary materials properties for the application.",,,,7/1/22,
Department of Energy,Idaho National Laboratory,AI127,Evaluating thermal properties of advanced materials,"The standard thermal diffusivity measurement technique laser flash is enhanced by modifying the traditional experimental set up and analyzing results with a machine learning based tool that includes a finite element model, a least-squares fitting   algorithm and experimental data treatment algorithms. This tool helps elucidate thermo- physical properties of a material from a single laser flash measurement.",,,,7/1/22,
Department of Energy,Idaho National Laboratory,AI128,Spectral Observation Convolutional Neural Network,"This project developed method to analyze collected radiation spectra using advanced, scalable deep learning by combining spectroscopic expertise with high performance computing. Sophisticated deep learning can overcome the weaknesses of existing spectroscopic techniques and enhance the value of difficult measurements. This method was trained, tested, and operated on the International Space Station’s Spaceborne Computer-2 supercomputer, returning zero errors over the course of 100 training hours. This demonstrated performance autonomously in far-edge, low-wattage computing situations and in hazardous radiological environments where interference can cause errors.",,,,7/1/22,
Department of Energy,Idaho National Laboratory,AI129,Passive Strain Measurements for Experiments in Radiation Environments,This project will develop passive instrumentation to determine permanent strains induced by irradiation and extract critical parameters using modeling and simulation as well as machine learning algorithms. An irradiation experiment will be conducted that  will benefit from engineered anisotropic materials and characterize the directional deformation in response to neutron radiation. The results of the experiment will be incorporated into the model so that the material response can be predicted for future uses as a probe material.,,,,7/1/22,
Department of Energy,Idaho National Laboratory,AI130,Machine Learning Interatomic Potentials for Radiation Damage and Physical Properties in Model Fluorite Systems,This project will use machine learning interatomic potentials to study the influence of radiation damage on physical properties of calcium fluoride and uranium dioxide. Electron irradiation experiments and thermal conductivity measurements will be performed to validate the effectiveness of the developed potentials. The high throughput capability of this method will become an important combinatorial materials science tool for developing and qualifying new nuclear fuels.,,,,7/1/22,
Department of Energy,Idaho National Laboratory,AI131,Data-driven failure diagnosis and prognosis of solid-state ceramic membrane reactor under harsh conditions using deep learning technology with internal voltage sensors,This research will investigate in situ the effects of different components on the degradation behavior in a solid-state ceramic membrane reactor by embedding sensors that will collect current and impedance data during operation. Artificial intelligence will be used to understand the large amounts of data and predict reactor failure under harsh operating conditions.,,,,7/1/22,
Department of Energy,Idaho National Laboratory,AI132,Tailoring the Properties of Multiphase Materials Through the Use of Correlative Microscopy and Machine Learning,"This research uses state-of-the-art machine learning (ML) techniques in a new and novel manner to identify and correlate the critical microstructural features in a multiphase alloy that exhibits high strength and fracture toughness. Experimental data will be used to train a convolutional neural network (CNN) in a semi-supervised environment to identify key microstructural features and correlate those features with the strength and toughness. The resulting machine learning tool can be trained for additional microstructural features, different alloys, and/or target mechanical properties.",,,,7/1/22,
Department of Energy,Idaho National Laboratory,AI133,Microstructurally-driven Framework for Optimization of In-core Materials,"This research will develop a methodology that relies on mechanism-informed machine learning models, rapid ion irradiation and creep testing techniques, and advanced characterization coupled with automated image analysis to enable reactor developers to quickly understand the complex linkage between alloy composition, thermomechanical processing, the resulting microstructure, and swelling and creep behavior. This project will (1) develop and demonstrate a high-potential methodology for rapid development of future in-core materials and (2) provide critically important information on alloy design for optimized swelling and creep behavior to the advanced reactor development community.",,,,7/1/22,
Department of Health & Human Services,Health Resources and Services Administration (HRSA),AI134,Health Resources and Services Administration (HRSA) Electronic Handbooks (EHBs) AI Chatbot,"AI Chatbot
• Successfully developed and deployed HRSA EHBs AI Chatbot using Artificial Solutions Teneo platform for external HRSA EHBs grantees
• Built to allow grantees to communicate with the EHBs Chatbot using regular natural conversational expressions
• Provides knowledge- and action-based responses through a self-service platform with 24/7 availability
• Integrated with existing EHBs application UI and Salesforce for automated ticket creation
• Chatbot has the ability to refine and increase the accuracy of its responses as more and more users invoke/use the Chatbot",,,,1/1/23,
Department of Health & Human Services,Health Resources and Services Administration (HRSA),AI135,Health Resources and Services Administration (HRSA) BHW Community Need Analysis Platform,The first use case being developed is for primary care with behavioral health integration which uses a machine learning based automated clustering engine.  The development of this tool allows for BHW to dynamically assess the healthcare need of a population given a specific use case and relevant datasets.  The output of the model will be used as part of the Notice of Funding Opportunity (NOFO) grant proposal evaluation process.,,,,1/1/23,
Department of Health & Human Services,Centers for Disease Control and Prevention (CDC) NCHS,AI136,ICD-10 Coding of Cause of Death reported on Death Certificates (MedCoder),MedCoder ICD-10 cause of death codes to the literal text cause of death description provided by the cause of death certifier on the death certificate.  This includes codes for the underlying and contributing causes of death.,,,,1/1/23,
Department of Health & Human Services,Centers for Disease Control and Prevention (CDC) NCHS,AI137,Item Nonresponse Detection in Open-text Response Data,"NCHS is developing an item nonresponse detection model, to identify cases of item nonresponse (e.g., gibberish, uncertain/don’t know, refusals, or high-risk) among open-text responses to help improve survey data and question and questionnaire design. The system is a Natural Language Processing (NLP) model pre-trained using Contrastive Learning and fine-tuned on a custom dataset from survey responses.",,,,1/1/23,
Department of Health & Human Services,Centers for Disease Control and Prevention (CDC) NCHS,AI138,Sequential Coverage Algorithm (SCA) in Record Linkage,"CDC’s National Center for Health Statistics (NCHS) Data Linkage Program has implemented a supervised machine learning algorithm, known as the Sequential Coverage Algorithm (SCA) in their linkage programs. The SCA was used to develop joining methods (or blocking groups) when working with very large datasets. The SCA method improved the efficiency of blocking.",,,,1/1/23,
Department of Health & Human Services,Centers for Medicare & Medicaid Services (CMS),AI139,Chatbot - Voice,"CMS/OSFLO: To assist the CMS Badging Help Desk, this Chatbot (voice) is an automated phone response for general badging questions allowing help desk personnel to assist employees and contractors with more detailed/larger issues.",,,,1/1/23,
Department of Health & Human Services,Centers for Medicare & Medicaid Services (CMS),AI140,Chatbot - Text,"CMS/OSFLO: To assist the Security team, this Chatbot (text) provides an automated email response for general physical security questions, allowing the help desk team to assist employees and contractors  with more in depth issues.",,,,1/1/23,
Department of Health & Human Services,Centers for Medicare & Medicaid Services (CMS),AI141,Feedback Analysis Solution (FAS),"The Feedback Analysis Solution is a system that uses CMS or other publicly available data (such as Regulations.Gov) to review public comments and/or analyze other information from internal and external stakeholders. The FAS uses Natural Language Processing (NLP) tools to aggregate, sort and identify duplicates to create efficiencies in the comment review process. FAS also uses machine learning (ML) tools to identify topics, themes and sentiment outputs for the targeted dataset.",,,,1/1/23,
Department of Health & Human Services,Centers for Medicare & Medicaid Services (CMS),AI142,Predictive Intelligence - Incident Assignment for Quality Service Center (QSC).,Predictive Intelligence (PI) is used for incident assignment within the Quality Service Center (QSC). The solution runs on incidents created from the ServiceNow Service Portal (https://cmsqualitysupport.servicenowservices.com/sp_ess). The solution analyzes the short description provided by the end user in order to find key words with previously submitted incidents and assigns the ticket to the appropriate assignment group. This solution is re-trained with the incident data in our production instance every 3-6 months based on need.,,,,1/1/23,
Department of Health & Human Services,Centers for Medicare & Medicaid Services (CMS),AI143,Reasonable Accommodation RPA Bot,"The Bot pulls HR data related to staffing changes, e.g. promotions, reassignments, change in supervisor, and generates information for action by Reasonable Accommodation staff to ensure disability reasonable accommodations follow the employee.",,,,1/1/23,
Department of Health & Human Services,Centers for Medicare & Medicaid Services (CMS),AI144,Rapid Authority to Operate (ATO),"The Rapid ATO System was built using a natural language processing model and pipeline to process system security plans, to identify unique and commonly used technology components used across Federal Information Security Management Act (FISMA)  systems. Natural language processing (NLP) is a form of machine learning that derives intent or subject out of blocks of text. In this particular case it was used to identify common blocks of language used in similar ways across system security plan (SSP) documents. In this way, CMS could identify similar approaches to solving certain technology or process-related control areas within the Acceptable Risk Safeguards (ARS). The output was used to create a list of components to develop control description language in a re-usable way, as part of the Blueprint/Rapid ATO effort to streamline SSP generation for new systems.",,,,1/1/23,
Department of Health & Human Services,Centers for Medicare & Medicaid Services (CMS),AI145,Data Lake/Load-Extract-Load-Transform (L-ETL),"CMS is using Security Data Lake to modernize the load-extract-load-transform (L-ETL) pipelines and data tooling. CMS will be enhancing Agency security to bring together more system, telemetry and program data in one place with a unifying governance model. Building on top of a modern data platform will provide opportunities to experiment with machine learning model development against this data--solving any number of problems that require decisions to be made about inferences over time series data. There is no actual ML/AI work being done here today, rather, we are beginning work on the scaffolding that will open up these opportunities in 1-2 years time.",,,,1/1/23,
Department of Health & Human Services,Centers for Medicare & Medicaid Services (CMS),AI146,Priority Score Model - ranks providers within the Fraud Prevention System using logistic regression based on program integrity guidelines.,"Inputs - Medicare Claims data, Targeted Probe and Educate (TPE) Data, Jurisdiction information
Output -  ranks providers within the FPS system using logistic regression based on program integrity guidelines.",,,,1/1/23,
Department of Health & Human Services,Centers for Medicare & Medicaid Services (CMS),AI147,"Priority Score Timeliness - forecast the time needed to work on an alert produced by Fraud Prevention System (Random Forest, Decision Tree, Gradient Boost, Generalized Linear Regression)","Inputs - Medicare Claims data, TPE Data, Jurisdiction information
Output -  forecast the time needed to work on an alert produced by FPS (Random Forest, Decision Tree, Gradient Boost, Generalized Linear Regression)",,,,1/1/23,
Department of Health & Human Services,Centers for Medicare & Medicaid Services (CMS),AI148,Provider Education 90 Day - reviews claims for provider before and after education for statistical change in their claim submission patterns,"Inputs - Medicare Claims data, TPE Data, Jurisdiction information
Output -  reviews claims for provider before and after education for statistical change in their claim submission patterns",,,,1/1/23,
Department of Health & Human Services,Food and Drug Administration (FDA),AI149,Advanced Semantic Search and Indexing of Text for Tobacco Applications (ASSIST4Tobacco),ASSIST4Tobacco is a novel tool that will use semantic indexing to search tobacco authorization applications. The system will be based on an AI (artificial intelligence)-based NLP (Natural Language Processing) model which provides deeper search capabilities using a language model developed to represent relationships between words and concepts within a body of text.,,,,1/1/23,
Department of Health & Human Services,Food and Drug Administration (FDA),AI150,Artificial Intelligence-based Deduplication Algoirthm for Classfication of Duplicate Reports in the FDA Adverse Event Reports (FAERS),The deduplication algorithm is applied to nonpublic data in the FDA Adverse Event Reporting System (FAERS) to identify duplicate reports.  Unstructured data in free text FAERS narratives is processed through a natural language processing system to extract relevant clinical features. Both structured and unstructured data are then used in a probabilistic record linkage approach to score pairs of reports by evaluating multiple data fields and applying relative weights per field. The output of potential duplicate reports is further placed in groups to facilitate identification of FAERS reports during case series evaluation for safety issues of concern.,,,,1/1/23,
Department of Health & Human Services,Food and Drug Administration (FDA),AI151,Opioid Data Warehouse Term Identification and Novel Synthetic Opioid Detection and Evaluation Analytics,"The Term Identification and Novel Synthetic Opioid Detection and Evaluation Analytics use publicly available social media and forensic chemistry data to identify novel referents to drug products in social media text. It uses the FastText library to create vector models of each known NSO-related term in a large social media corpus, and provides users with similarity scores and expected prevalence estimates for lists of terms that could be used to enhance future data gathering efforts.",,,,1/1/23,
Department of Health & Human Services,National Institutes of Health (NIH) NIGMS,AI152,"National Institute of General Medical Sciences (NIGMS) AI Supported Searches, Information Systems and Tools System Acronym: NIGMS ASSIST)","NIGMS program staff often need information that is available through IMPAC or QVR to perform their daily tasks. In order to provide such information, DIMA and IRMB have collaborated to develop functions that utilize artificial intelligence and natural language processing methods to produce data relevant to the program staff’s mission. These tools are collected into a single system to make them available to the NIGMS community for use on a day-to-day basis. ASSIST provides a secure interface supported by Oracle, SQL server and Python analytics. The individual components of ASSIST provide the following functions:
- FLIP module (Development), provides the ability to identify investigators by PPID from Federal RePORTER based on user input of investigator PPIDs.
- TPAL module (Production), provides the ability to lookup potential matching program officers, including their corresponding predicted Program Area Codes, and ICs based on the input of unstructured scientific data.",,,,1/1/23,
Department of Health & Human Services,National Institutes of Health (NIH) NIGMS,AI153,Leveraging AI for Business Process Automation,"NIGMS has developed a method to automate the initial referral of grant applications to the proper scientific expertise within the Institute using Natural Language Processing and Machine Learning. NIGMS IRMB and DIMA are currently using this NLP/ML algorithm developed in R statistical software to parse grant applications and to determine Project Officer candidates for grant assignment. This process was previously fully manual and required a substantial person hour effort. NIGMS has collaborated with the Electronic Records Administration group to incorporate this technique into the Internal Referral Module, and the tool is now available to be adapted for broader use across the NIH.",,,,1/1/23,
Department of Health & Human Services,National Institutes of Health (NIH) NIEHS,AI154,Grant Application Subject-Matter Classification Tool,Natural language processing of grant applications for the purpose of classification for review assignment,,,,1/1/23,
Department of Health & Human Services,National Institutes of Health (NIH) NIEHS,AI155,Splunk IT System Monitoring Software,Splunk utilizes machine learning to aggregate system logs from IT infrastructure systems and endpoints for auditing and monitoring purposes,,,,1/1/23,
Department of Health & Human Services,National Institutes of Health (NIH) NIEHS,AI156,COVID-19 Pandemic Vulnerability Index Dashboard,"The dashboard creates risk profiles, called PVI scorecards, for every county in the United States, continuously updated with the latest data that summarize and visualize overall disease risk.",,,,1/1/23,
Department of Health & Human Services,National Institutes of Health (NIH) NHLBI,AI157,Leveraging AI/ML for classification and categorization of scientific concepts,Topical characterization of the research portfolio.  Inputs are publications and grants abstracts. These are fed into a text classification model and concept extraction. The outputs are category labels and list of concepts.,,,,1/1/23,
Department of Health & Human Services,National Institutes of Health (NIH) OPA,AI158,Machine learning system to predict translational progress in biomedical research,"Fundamental scientific advances can take decades to translate into improvements in human health. Shortening this interval would increase the rate at which scientific discoveries lead to successful treatment of human disease. One way to accomplish this would be to identify which advances in knowledge are most likely to translate into clinical research.
Toward that end, the NIH Office of Portfolio Analysis built a machine learning system that detects whether a paper is likely to be cited by a future clinical trial or guideline. Despite the noisiness of citation dynamics, as little as 2 years of postpublication data yield accurate predictions about a paper’s eventual citation by a clinical article (accuracy = 84%, F1 score = 0.56; compared to 19% accuracy by chance). We found that distinct knowledge flow trajectories are linked to papers that either succeed or fail to influence clinical research. Translational progress in biomedicine can therefore be assessed and predicted in real time based on information conveyed by the scientific community’s early reaction to a paper. For more information see the publication describing this system: Hutchins et al 2019 (https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.3000416)",,,,1/1/23,
Department of Health & Human Services,National Institutes of Health (NIH) OPA,AI159,Semantic analysis of scientific documents (word2vecOPA),"The NIH Office of Portfolio Analysis has developed a neural network approach to analysis of scientific content using dimensionality reduction (word2vecOPA). This method computationally converts words in scientific texts to numbers and summarizes documents by their semantic content by learning relationships between words from their context. This method is adaptable to specific corpora, including grants and scientific articles. For more information see the publication describing our word2vec approach: Hoppe et al 2019 (https://www.science.org/doi/10.1126/sciadv.aaw7238)",,,,1/1/23,
Department of Health & Human Services,National Institutes of Health (NIH) OPA,AI160,Person-level disambiguation for PubMed authors and NIH grant applicants,"High-quality disambiguation is required to correctly link researchers to their grants and outputs including articles, patents, and clinical trials. The NIH Office of Portfolio Analysis developed a disambiguation solution that used article level metadata to assign 24.5M unique papers from the PubMed database to 16.0M unique author names, then used a novel neural network model trained on ORCID identifiers to determine whether author-publication pairs refer to variant representations of the same person. For example, our model can determine whether hypothetical records listing Jane Smith and Jane M. Smith were the same person, or two different people, based on variables that include institutional affiliation, co-authorship, and article-affiliated Medical Subject Heading (MeSH) terms. For more information see the publication describing this method: Yu et al 2021 (https://www.biorxiv.org/content/10.1101/2021.02.02.429450v1.full.pdf)",,,,1/1/23,
Department of Health & Human Services,National Institutes of Health (NIH) OER,AI161,Program Class Code (Area of Science) Referral for NIAID,"The REFERRAL GROUP of Referral, Program Analysis Branch (RPAB) is responsible for program assignments for all research, training, career, and fellowship grant applications submitted to NIAID, from CSR.  The NIAID Program Class Code classification AI project evaluates the projects that are in RAPB and auto assigns these grant applications to the Program Class Codes.  The inputs are comprised of approximately 6,000+ grant applications that are currently manually assigned  by RPAB Staff. The output would be grant applications that are categorized into their  respective PCC's.",,,,1/1/23,
Department of Health & Human Services,National Institutes of Health (NIH) OER,AI162,"Research, Condition, and Disease Categorization (RCDC)","RCDC  is an electronic budget reporting tool that  categorizes projects using AI/NLP.  The inputs are grant applications, R&D contracts, intramural projects, inter agency agreements.  The RCDC Fingerprinting process identifies concepts in the extracted text from the source project, person or publication. The text is normalized, concepts are extracted, concepts and synonyms are matched to the RCDC thesaurus. A rank is applied based on the frequency of occurrence of the concepts within the text. Project fingerprints are sourced from the application description text (Title, Abstract and Specific Aims). Titles and abstracts provide the source of scientific concepts for publications. The system then outputs the projects into their respective areas of science.",,,,1/1/23,
Department of Health & Human Services,National Institutes of Health (NIH) OER,AI163,Query View Report (QVR) LIKE,"The LIKE feature in QVR makes use of the NIH Research, Condition and Disease Categorization (RCDC) indexing results to compare scientific terms associated with a project, person or publication and find scientifically similar projects, persons or publications.",,,,1/1/23,
Department of Health & Human Services,National Institutes of Health (NIH) OER,AI164,Internal Referral Module (IRM) NLP,"The IRM NLP module automatically refers projects to Program Officers once the grant application is received.  The system inputs are grant applications - the title, abstract, specific aims and Public Health Relevance is analyzed to automatically refer the grant application to the Program Officer who matches  a similar background with the science contained in the applications.  This process, is operating at a high accuracy rate and has effectively eliminated the referral bottleneck.",,,,1/1/23,
Department of Health & Human Services,National Institutes of Health (NIH) OER,AI165,NIH Grants Virtual Assistant,Chat Bot to assist users in finding grant related information via OER resources,,,,1/1/23,
Department of Health & Human Services,National Institutes of Health (NIH) NLM,AI166,Pangolin lineage classifications to support accessing and analysis of SARS-CoV-2 sequence data.,"The Pango nomenclature, called Pango lineages, is being used by researchers and public health agencies worldwide to track the transmission and spread of SARS-CoV-2, including variants of concern. The requirements for running the tool include having conda on a MacOS or Linux system, and the FASTA-formated sequence data. There are 2 methods for lineage assignment with Pango; within NCBI Virus we use the process which includes PangoLEARN, where a classification tree is used to group similar sequences.",,,,1/1/23,
Department of Health & Human Services,National Institutes of Health (NIH) NLM,AI167,Providing  MeSH Check Tag of NLM’s Medical Text Indexer (MTI) ons using Support Vector Machines (SVM),"Titles and abstracts from MEDLINE Citations are provided through SVM machine learning algorithm provides confidence scores for a set of MeSH CheckTags to the NLM Medical Text Indexer (MTI) program.  These CheckTags are small set of MeSH Descriptors designed to indicate Species, Sex, and Age in MEDLINE articles.",,,,1/1/23,
Department of Health & Human Services,National Institutes of Health (NIH) NLM,AI168,Determining selection for indexing MEDLINE articles using Neural Network Architecture with a Convolutional Neural Network (CNN),"Using the article title, abstract, journal, publication year, and indexing year of indexed and non-indexed articles that were submitted to MEDLINE in 2018, methods to automate the selection of indexed articles was researched. A classifier was developed that combines the predictions of many traditional machine learning algorithms and a Convolutional Neural Network (CNN). The final classification layer uses a sigmoid activation function to generate a single output value between zero and one, which can be interpreted as the probability of an article being in-scope for MEDLINE.",,,,1/1/23,
Department of Health & Human Services,National Institutes of Health (NIH) NLM,AI169,MetaMap to identity potential terms for indexing MEDLINE articles,"MetaMap is a widely available program providing access from biomedical text to the concepts in the unified medical language system (UMLS) Metathesaurus. MetaMap uses NLP to provide a link between the text of biomedical literature and the knowledge, including synonymy relationships, embedded in the Metathesaurus. The flexible architecture in which to explore mapping strategies and their application are made available. MTI uses the MetaMap to generate potential indexing terms.",,,,1/1/23,
Department of Health & Human Services,National Institutes of Health (NIH) NLM,AI170,Best Match: New relevance search for PubMed,"PubMed is a free search engine for biomedical literature accessed by millions of users from around the world each day. With the rapid growth of biomedical literature, finding and retrieving the most relevant papers for a given query is increasingly challenging. We have developed Best Match, a new relevance search algorithm for PubMed that leverages the intelligence of our users and cutting-edge machine-learning technology as an alternative to the traditional date sort order. The Best Match algorithm is trained with past user searches with dozens of relevance-ranking signals (factors) and demonstrates state-of-the-art retrieval performance in benchmarking experiments as well as an improved user experience in real-world testing.",,,,1/1/23,
Department of Health & Human Services,National Institutes of Health (NIH) NLM,AI171,SingleCite: Improving single citation search in PubMed,"A search that is targeted at finding a specific document in databases is called a Single Citation search, which is particularly important for scholarly databases, such as PubMed, because it is a typical information need of the users. We have developed SingleCite, an automated algorithm that establishes a query-document mapping by building a regression function to predict the probability of a retrieved document being the target based on three variables: the score of the highest scoring retrieved document, the difference in score between the two top retrieved documents, and the fraction of a query matched by the candidate citation. SingleCite shows superior performance in benchmarking experiments and is applied to rescue queries that would fail otherwise.",,,,1/1/23,
Department of Health & Human Services,National Institutes of Health (NIH) NLM,AI172,Computed Author: author name disambiguation for PubMed,"PubMed users frequently use author names in queries for retrieving scientific literature. However, author name ambiguity (different authors share the same name) may lead to irrelevant retrieval results. Thus we have developed a machine-learning method to score the features for disambiguating a pair of papers with ambiguous names.
Subsequently, agglomerative clustering is employed to collect all papers belong to the same authors from those classified pairs. Disambiguation performance is evaluated with manual verification of random samples of pairs from clustering results, with a higher accuracy than other state-of-the-art methods. It has been integrated into PubMed to facilitate
author name searches.",,,,1/1/23,
Department of Health & Human Services,National Institutes of Health (NIH) NLM,AI173,National Library of Medicine NLM-Gene: towards automatic gene indexing in PubMed articles,"Gene indexing is part of the NLM’s MEDLINE citation indexing efforts for improving literature retrieval and information access. Currently, gene indexing is performed manually by expert indexers. To assist this time-consuming and resource- intensive process, we have developed NLM-Gene, an automatic tool for finding gene names in the biomedical literature using advanced natural language processing and deep learning methods. Its performance has been assessed on gold- standard evaluation datasets and is to be integrated into the production MEDLINE indexing pipeline.",,,,1/1/23,
Department of Health & Human Services,National Institutes of Health (NIH) NLM,AI174,National Library of Medicine NLM-Chem: towards automatic chemical indexing in PubMed articles,"Chemical indexing is part of the NLM’s MEDLINE citation indexing efforts for improving literature retrieval and information access. Currently, chemcial indexing is performed manually by expert indexers. To assist this time-consuming and resource-intensive process, we have developed NLM-Chem, an automatic tool for finding chemical names in the biomedical literature using advanced natural language processing and deep learning methods. Its performance has been assessed on gold-standard evaluation datasets and is to be integrated into the production MEDLINE indexing pipeline.",,,,1/1/23,
Department of Health & Human Services,Office of Inspector General (OIG),AI175,Grants Analytics Portal,"The Grants Analytics Portal uses AI to enhance HHS OIG staff’s ability to access grants related data quickly and easily by: quickly navigating directly to the text of relevant findings across thousands of audits, the ability to discover similar findings, analyze trends, compare data between OPDIVs, and the means to see preliminary assessments of potential anomalies between grantees.",,,,1/1/23,
Department of Health & Human Services,Office of Inspector General (OIG),AI176,Text Analytics Portal,"The text analytics portal allows personnel without an analytics background to quickly examine text documents through a related set of search, topic modeling and entity recognition technologies",,,,1/1/23,
Department of Health & Human Services,Agency for Healthcare Research and Quality,AI177,Relevancy Tailoring,Adjusting the ranking of search results so that most relevant results show up at the top of the list,,,,1/1/23,
Department of Health & Human Services,Agency for Healthcare Research and Quality,AI178,Auto-generation Synonyms,"Behind the scenes, adding synonyms to search queries to improve search results",,,,1/1/23,
Department of Health & Human Services,Agency for Healthcare Research and Quality,AI179,Automated Suggestions,Auto-filling queries as  they are typed,,,,1/1/23,
Department of Health & Human Services,Agency for Healthcare Research and Quality,AI180,Suggested Related Content,"Show related searches that may provide the user with other related, valuable information",,,,1/1/23,
Department of Health & Human Services,Agency for Healthcare Research and Quality,AI181,Auto Tagging,Suggesting content tags automatically based on a machine-driven evaluation of how existing content is tagged,,,,1/1/23,
Department of Health & Human Services,Agency for Healthcare Research and Quality,AI182,Did you mean,Suggesting spelling corrections and reformatted search queries based on Google Analytics data,,,,1/1/23,
Department of Health & Human Services,Agency for Healthcare Research and Quality,AI183,Chatbot,An interactive interface that can respond to plain language queries in real time using natural language processing,,,,1/1/23,
Department of Homeland Security,DHS Headquarters,AI184,Sentiment Analysis and Topic Modeling (SenTop),"The initial purpose of the Sentiment Analysis and Topic Modeling (SenTop) project was to analyze survey responses for DHS’s Office of the Chief Procurement Officer related to contracting. However, it has evolved to be a general-purpose text analytics solution that can be applied to any domain/area. It also has been tested/used for human resources topics. SenTop is a DHS-developed Python package for performing descriptive text analytics. Specifically, sentiment analysis and topic modeling on free-form, unstructured text. SenTop uses several methods for analyzing text including combining sentiment analyses and topic modeling into a single capability, permitting identification of sentiments per topic and topics per sentiment. Other innovations include the use of polarity and emotion detection, fully automated topic modeling, and multi-model/multi-configuration analyses for automatic model/configuration selection. The code has been established, performs an analysis, and provides a report but it is only accessed and run by one person per customer request.",,,,8/17/22,
Department of Homeland Security,Cybersecurity and Infrastructure Security Agency (CISA),AI185,AIS Scoring & Feedback (AS&F),"Automated Indicator Sharing (AIS), a CISA capability, enables the real-time exchange of machine-readable cyber threat indicators and defensive measures to help protect against and ultimately reduce the prevalence of cyber incidents. AIS is offered as part of CISA’s broad authority to share information relating to cybersecurity risks, including authority to receive, analyze, and disseminate information, and fulfills CISA’s obligation under the Cybersecurity Information Sharing Act of 2015 to establish and operate the federal government’s capability and process for receiving cyber threat indicators and defensive measures, and to further share this information with certain other agencies, in some cases in a real-time manner. For more information, please visit: https://www.cisa.gov/ais.

AIS Automated Scoring & Feedback (AS&F), built on the AIS Scoring Framework, defines an algorithm by which organizations can enrich Structured Threat Information Expression Indicator objects, shared via AIS, with (1) an opinion value that provides an assessment of whether or not the information can be corroborated with other sources available to the entity submitting the opinion and (2) a confidence score that states the submitter’s confidence in the correctness of information they submit into AIS. When leveraged by CISA, AS&F uses artificial intelligence / machine learning to perform descriptive analytics from organizational-centric intelligence to support confidence and opinion classification of indicators of compromise. Together, these enrichments can help those receiving information from AIS prioritize actioning and investigating Indicator objects.",,,,8/17/22,
Department of Homeland Security,Cybersecurity and Infrastructure Security Agency (CISA),AI186,Automated PII Detection,"CISA's Automated Personally Identifiable Information (PII) Detection and Human Review Process incorporates descriptive, predictive, and prescriptive analytics. Automated PII Detection leverages natural language processing tasks including named entity recognition coupled with Privacy guidance thresholds to automatically detect potential PII from within Automated Indicator Sharing submissions. If submissions are flagged for possible PII, the submission will be queued for human review where the analysts will be provided with the submission and artificial intelligence-assisted guidance to the specific PII concerns. Within human review, analysts can confirm/deny proper identification of PII and redact the information (if needed). Privacy experts are also able to review the actions of the system and analysts to ensure proper performance of the entire process along with providing feedback to the system and analysts for process improvements (if needed). The system learns from feedback from the analysts and Privacy experts.

Through the incorporation of the automated PII detection, CISA complies with Privacy, Civil Rights and Civil Liberties requirements of CISA 2015 and scaled analyst review of submissions by removing false positives and providing guidance to submission to be reviewed. Through continual audits CISA will maintain integrity and trust in system and human processes. For more information, please visit: https://www.cisa.gov/ais.",,,,8/17/22,
Department of Homeland Security,Transporation Security Agency (TSA),AI187,CDC Airport Hotspot Throughput (PageRank),"TSA launched the “Stay Healthy. Stay Secure.” campaign, which details proactive and protective measures have been implemented at security checkpoints to make the screening process safer for passengers and our workforce by reducing the potential of exposure to the coronavirus. The campaign includes guidance and resources to help passengers prepare for the security screening process in the COVID environment. A big part of that campaign was the development of the Centers for Disease Control and Prevention's Airport Hotspot Throughput. This capability determines the domestic airports that have the highest rank of connecting flights during the holiday travel season to help mitigate the spread of COVID-19. This capability is a DHS-developed artificial intelligence model written in Spark/Scala that takes historical non-PII travel data and computes the highest-ranking airports based on the PageRank algorithm. TSA does not make decisions about flight cancellations or airport closures. These decisions are made locally, on a case-by-case basis, by individual airlines, airports, and public health officials. TSA will continuously evaluate and adapt procedures and policies to keep the public and our workforce safe as we learn more about this devastating disease and how it spreads.",,,,8/17/22,
Department of Homeland Security,U.S. Citizenship and Immigration Services (USCIS),AI188,Asylum Text Analytics (ATA),"USCIS oversees lawful immigration to the United States. As set forth in Section 451(b) of the Homeland Security Act of 2002, Public Law 107-296, Congress charged USCIS with administering the asylum program. USCIS, through its Asylum Division within the Refugee, Asylum & International Operations Directorate (RAIO), administers the affirmative asylum program to provide protection to qualified individuals in the United States who have suffered past persecution or have a well-founded fear of future persecution in their country of origin, as outlined under Section 208 of the Immigration and Nationality Act (INA), 8 U.S.C. § 1158 and Title 8 of the Code of Federal Regulations (C.F.R.), Part 208. Generally, an individual not in removal proceedings may apply for asylum through the affirmative asylum process regardless of how the individual arrived in the United States or his or her current immigration status by filing Form I-589, Application for Asylum and for Withholding of Removal. The ATA capability employs machine learning and data graphing techniques to identify plagiarism-based fraud in applications for asylum status and for the withholding of removal by scanning the digitized narrative sections of the associated forms and looking for common language patterns.",,,,8/17/22,
Department of Homeland Security,U.S. Citizenship and Immigration Services (USCIS),AI189,BET/FBI Fingerprint Success Maximization,"USCIS's Customer Profile Management Service (CPMS) serves as a person-centric repository of biometric and biographic information provided by applicants and petitioners (hereafter collectively referred to as “benefit requestors”) that have been issued a USCIS card evidencing the granting of an immigration related benefit (i.e., permanent residency, work authorization, or travel documents). The Biometrics Encounter Tool (BET) / Federal Bureau of Investigation (FBI) Fingerprint Success Maximization Service center technicians can receive immediate feedback when a set of prints is likely to be rejected by the FBI by incorporating machine learning models into the BET application. The FBI will not disclose their quality grading criteria for fingerprints, leaving CPMS with the responsibility of determining quality to prevent unnecessary secondary encounters with applicants. Using even the simplest of models would catch 98% of rejected submissions, which could have potentially saved USCIS from scheduling 42,763 additional appointments in 2020. This would come at the cost of forcing recapture during 11% of encounters. This effort aims to maximize the number of successful FBI submissions while minimizing the number of fingerprint recaptures necessary. For more information, please visit: https://www.dhs.gov/publication/dhsuscispia-060-customer-profile-management-service-cpms",,,,8/17/22,
Department of Homeland Security,U.S. Citizenship and Immigration Services (USCIS),AI190,Biometrics Enrollment Tool (BET) Fingerprint Quality Score,"USCIS's Customer Profile Management Service (CPMS) serves as a person-centric repository of biometric and biographic information provided by applicants and petitioners (hereafter collectively referred to as “benefit requestors”) that have been issued a USCIS card evidencing the granting of an immigration related benefit (i.e., permanent residency, work authorization, or travel documents). The Biometrics Enrollment Tool (BET) team has been working on enhancing their quality checks, with one of the new improvements being incorporation of the National Institute of Standards and Technology (NIST) Fingerprint Image Quality 2 (NIFQ2) algorithm (a trained machine learning algorithm) for scoring of fingerprints (https://www.nist.gov/services-resources/software/nfiq-2) into the BET application. This algorithm takes a fingerprint image and assigns a score between 0 - 100, with 100 indicating that this is the best quality fingerprint image that could be obtained. The higher the score, the more likely that the fingerprint will match when captured again. This algorithm has been in place for several Program Increments. BET had been providing Biometric Capture Technicians with a poor-quality indicator and encountered objections from technicians for the larger than expected number of recaptures required, based on contractual complications. The BET team continues to capture this data in the background, but this does not require recapture currently. For more information, please visit: https://www.dhs.gov/publication/dhsuscispia-060-customer-profile-management-service-cpms",,,,8/17/22,
Department of Homeland Security,U.S. Citizenship and Immigration Services (USCIS),AI191,Evidence Classifier,"USCIS is the component within DHS that oversees lawful immigration to the United States. USCIS receives immigration requests from individuals seeking immigration and non-immigration benefits. Once a benefit request form is submitted to USCIS, a series of processing and adjudication actions occur. One of the case management systems used to track and adjudicate certain immigration request forms is the Electronic Information System (ELIS). USCIS ELIS is an internal case management system composed of microservices to assist with performing complex adjudicative and processing tasks; one of those microservices is the Evidence Classifier. Until the introduction of the Evidence Classifier machine learning (ML) solution, those who are working cases and who are responsible for reviewing evidence documents would often have to sift through dozens, if not hundreds, of unlabeled pages to find one specific artifact — be that a green card, a birth certificate, or so on. To reduce the amount of adjudicative time spent on these repetitive tasks, a ML solution was built to systematically tag individual pages with some of the highest-volume, highest-impact evidence types. Calculated from September 28, 2021 , to May 20, 2022, the ML enhancements have saved around 24 million page scrolls, which amounts to approximately 13,348 hours saved, assuming it takes 2 seconds to review 1 page of evidence. This has nearly doubled cases with a 30-day adjudication rate from about 30% to 58%. For more information, please visit: https://www.dhs.gov/publication/dhsuscispia-056-uscis-electronic-immigration-system-uscis-elis",,,,8/17/22,
Department of Homeland Security,U.S. Citizenship and Immigration Services (USCIS),AI192,FDNS-DS NexGen,"USCIS created the Fraud Detection and National Security (FDNS) Directorate to strengthen the integrity of the nation’s immigration system and to ensure that immigration benefits are not granted to individuals that may pose a threat to national security and/or public safety. In addition, the FDNS Directorate is responsible for detecting, deterring, and combating immigration benefit fraud, hence the creation of the FDNS Data System (FDNS-DS). FDNS-DS NexGen is a case management system, that is planning to use resolved artificial intelligence (AI) / machine learning (ML) entities from other applications to aid in investigative work, enhance investigative case prioritization, and detect duplicate case work. In the future, there are plans to integrate AI/ML into the predictive modeling for future system enhancements, working side-by-side with the business stakeholders to develop best practices. Fraud occurs in numerous ways; being able to discover and detect persons with multiple identities allows for more comprehensive investigations, reduces investigative cycle time, and improves performance . Those basic implementations will speed up processing by several magnitudes. For more information, please visit: https://www.dhs.gov/publication/dhsuscispia-013-01-fraud-detection-and-national-security-directorate",,,,8/17/22,
Department of Homeland Security,U.S. Citizenship and Immigration Services (USCIS),AI193,Sentiment Analysis,"The USCIS Service Center Operations Directorate (SCOPS) provides services for persons seeking immigration benefits while ensuring the integrity and security of our immigration system. As part of that mission, we issued a two-part survey asking users both quantitative and qualitative questions. USCIS performed a statistical analysis of the quantitative results and then used Natural Language Processing modeling software to assign ""sentiments"" to categories ranging from strongly positive to strongly negative. This model was eventually enhanced using a machine learning model to have better reusability and performance. This capability has been deployed to production for more than one year.",,,,8/17/22,
Department of Homeland Security,U.S. Citizenship and Immigration Services (USCIS),AI194,Testing Performance of ML Model using H2O,"USCIS is the component within DHS that oversees lawful immigration to the United States. That means USCIS receives, processes, and maintains all applications for admission for Lawful permanent residents (LPRs), or adjustments to LPR status. Also known as “green card” holders, LPRs are non-citizens who are lawfully authorized to live permanently within the United States and are required to fill out Form I-90, Application to Replace Permanent Resident Card (Green Card). Since there has been a considerable influx of green card applications, USCIS used a combination of exploratory data analysis to determine the most used categories for applicants submitting I-90's, and machine learning to create predictions of workloads. USCIS used the H20 machine learning model to allow USCIS analysts to build and run several machine learning models on big data in an enterprise environment and identify the model that performs the best. It has already been successful in identifying the most accurate model for the I-90 Form Timeseries Analysis and Forecasting use case. This capability has been in production for more than one year.",,,,8/17/22,
Department of Homeland Security,U.S. Citizenship and Immigration Services (USCIS),AI195,Timeseries Analysis and Forecasting,"USCIS is the component within DHS that oversees lawful immigration to the United States. That means USCIS receives, processes, and maintains all applications for admission for Lawful permanent residents (LPRs), or adjustments to LPR status. Also known as “green card” holders, LPRs are non-citizens who are lawfully authorized to live permanently within the United States and are required to fill out Form I-90, Application to Replace Permanent Resident Card (Green Card). Since there has been a considerable influx of green card applications, USCIS used a combination of exploratory data analysis to determine the most used categories for applicants submitting I-90's and machine learning to create predictions of workloads. As a follow-on, USCIS used Autoregressive Integrated Moving Average (ARIMA) models on the I-90 form, which allowed the prediction of the total number of forms for a 2-year period. ARIMA is one of the easiest and effective machine learning algorithms to perform time series forecasting. This capability has been deployed in production for more than a year. This model was eventually enhanced using ML model to have better reusability and performance.",,,,8/17/22,
Department of Homeland Security,U.S. Coast Guard,AI196,Silicon Valley Innovation Program (SVIP) Language Translator,"USCG operators must be able to communicate with vessel occupants - many who may be non-English speakers - while performing a variety of rescue and investigative missions. The accurate and swift translation of information is critical to the safety and security of Coast Guard boarding teams and vessel occupants. The DHS’s Science and Technology Silicon Valley Innovation Program (SVIP) Language Translator solicitation sought new capabilities to support the Coast Guard in facilitating real-time communications with non-English speakers and those who are unable to communicate verbally. The solicitation also included requirements for language translation technology to be capable of operating both online and offline because many Coast Guard interactions take place in extreme environmental conditions, and in locations without cell service or an internet connection. There are two performers, myLanguage and Kynamics, who have designed an online/offline voice-to-text speech recognition and text-to-text translation system that employs deep learning and artificial intelligence. myLanguage completed Phase I and Kynamics completed Phase 2, adapting their voice translation technologies for use into a rugged, hand-held mobile device that can withstand extreme temperatures and, customized model designs and training language models to fit Coast Guard use cases.",,,,8/17/22,
Department of Homeland Security,U.S. Customs and Border Protection (CBP),AI197,Agent Portable Surveillance,"The agent portable surveillance system is a backpack mobile unit meant for single agent deployments. The system identifies border activities of interest by using artificial intelligence / machine learning to analyze data from Electro-Optical/Infra-Red cameras and radar. When an activity is detected, the system sends the information to agents through the Team Awareness Kit (TAK). Detections are shared with CBP TAK users to enhance efficiency and agent/officer safety.",,,,8/17/22,
Department of Homeland Security,U.S. Customs and Border Protection (CBP),AI198,Autonomous Surveillance Towers,"The system permits autonomous detection, identification, and tracking of items of interest. The tower scans constantly and autonomously; radar detects and recognizes movement; and the camera slews autonomously to the items of interest and the system software identifies the object. The system utilizes artificial intelligence / machine learning to analyze the camera and radar data which alerts the user and autonomously tracks the item of interest. End users can monitor the system and see near real-time photos by logging into the User Interface on any USCBP device.",,,,8/17/22,
Department of Homeland Security,U.S. Customs and Border Protection (CBP),AI199,I4 Viewer Matroid Image Analysis,"Matroid is a software that enables CBP end users to create and share vision detectors. Matroid detectors are trained computer vision models that recognize objects, people, and events in any image and in video streams. Once a detector is trained, it can monitor streaming video in real time, or efficiently search through pre-recorded video data or images to identify objects, people, and events of interest. Users can view detection information via a variety of reports and alert notifications to process and identify important events and trends. Detection data is also available through Matroid’s powerful developer Application Programming Interface and language-specific clients, so CBP applications can be integrated with the power of computer vision.",,,,8/17/22,
Department of Homeland Security,U.S. Customs and Border Protection (CBP),AI200,Open-source News Aggregation,"The platform enables users to make better decisions faster by identifying and forecasting emerging events on a global scale to mitigate risk, recognize threats, greatly enhance indications and warnings, and provide predictive intelligence capabilities. The artificial intelligence / machine learning models enable rapid access to automated intelligence assessments by fusing, processing, exploiting and analyzing open sources of data (including news, social media, economic indicators, governance indicators, travel warnings, weather and other sources). This system is an immediate and substantial force multiplier that shifts the traditional approach of monitoring and assessing the operational environment to focus on the forecast of the future geopolitical, socio, and economic environment.",,,,8/17/22,
Department of Homeland Security,U.S. Customs and Border Protection (CBP),AI201,Data Tagging and Classification,"The Homeland Security Investigations (HSI) Innovation Lab is developing an analytical platform called the Repository for Analytics in a Virtualized Environment (RAVEn). RAVEn facilitates large, complex analytical projects to support ICE’s mission to enforce and investigate violations of U.S. criminal, civil, and administrative laws. RAVEn also enables users to develop new tools to analyze trends and isolate criminal patterns as HSI mission needs arise. For more information, please read the DHS/ICE/PIA-055 - Privacy Impact Assessment 055 for the Repository for Analytics in a Virtualized Environment (RAVEn).

RAVEn leverages data tagging and classification to do the following:

The Email Analytics Tool streamlines how special agents and criminal analysts search, filter, translate, and report on electronic communications evidence and will help investigators more effectively determine the structure and organization of criminal enterprises.

The RAVEn - Lead Tracker is a centralized system where agents can send and receive leads and enter outcomes such as arrests and seizures. The goal is for all leads in the agency to be found in one place, rather than in various email inboxes.

The overarching goal of Mobile Device Analytics is to improve the efficiency of agents and analysts in identifying pertinent evidence, relationships, and criminal networks from data extracted from mobile devices.",,,,8/17/22,
Department of Homeland Security,U.S. Customs and Border Protection (CBP),AI202,Language Translator,"The Homeland Security Investigations (HSI) Innovation Lab is developing an analytical platform called the Repository for Analytics in a Virtualized Environment (RAVEn). RAVEn facilitates large, complex analytical projects to support ICE’s mission to enforce and investigate violations of U.S. criminal, civil, and administrative laws. RAVEn also enables users to develop new tools to analyze trends and isolate criminal patterns as HSI mission needs arise. For more information, please read the DHS/ICE/PIA-055 - Privacy Impact Assessment 055 for the Repository for Analytics in a Virtualized Environment (RAVEn).

RAVEn has incorporated a machine translation system for commercial vendors, Systran, which provides machine learning translation for over 100 different language combinations. Currently, the Innovation Lab has licenses for translating Chinese, Spanish, Arabic, Farsi, Russian, German, Ukrainian, and Filipino to English. Systran can translate plain text, word documents, and PDFs. Both a web-based user interface (UI) and Application Programming Interface (API) endpoints are available. Machine learning is used to increase the efficiency, accuracy, and quality of searching, analyzing, and translating speech.",,,,8/17/22,
Department of Homeland Security,U.S. Customs and Border Protection (CBP),AI203,RAVEn Compliance Automation Tool (CAT),"The Homeland Security Investigations (HSI) Innovation Lab is developing an analytical platform called the Repository for Analytics in a Virtualized Environment (RAVEn). RAVEn facilitates large, complex analytical projects to support ICE’s mission to enforce and investigate violations of U.S. criminal, civil, and administrative laws. RAVEn also enables users to develop new tools to analyze trends and isolate criminal patterns as HSI mission needs arise. For more information, please read the DHS/ICE/PIA-055 - Privacy Impact Assessment 055 for the Repository for Analytics in a Virtualized Environment (RAVEn).

RAVEn CAT is being developed as part of an effort to modernize HSI’s Form I-9 Inspection Process. The goal is to use machine learning and automation to increase the speed and efficiency of ingesting and processing Forms I-9 data. Easy to use front-end interface workflow that increases work productivity and reduces manual entry. RAVEn CAT currently employs an Optical Recognition Service (OCR) model and software (Tesseract OCR) to identify pixel coordinates of handwritten and read/extract computer typed characters from ingested forms for processing. Additional research into opensource Machine Learning Object Detection models is being made to help further augment accuracy of text identification and extraction of ingested forms into the pipeline.",,,,8/17/22,
Department of Homeland Security,U.S. Customs and Border Protection (CBP),AI204,RAVEn Normalization Services,"The Homeland Security Investigations (HSI) Innovation Lab is developing an analytical platform called the Repository for Analytics in a Virtualized Environment (RAVEn). RAVEn facilitates large, complex analytical projects to support ICE’s mission to enforce and investigate violations of U.S. criminal, civil, and administrative laws. RAVEn also enables users to develop new tools to analyze trends and isolate criminal patterns as HSI mission needs arise. For more information, please read the DHS/ICE/PIA-055 - Privacy Impact Assessment 055 for the Repository for Analytics in a Virtualized Environment (RAVEn).

Within RAVEn, HSI has utilized artificial intelligence / machine learning to verify, validate, correct, and normalize the accuracy and quality of addresses, phone numbers, and names. The normalization services let agents analyze both well-defined addresses (such as those in CONUS and Europe) and less well-defined addresses (such as addresses using mile markers); standardize phone numbers to their identified country and to the E164 ITU standard; and streamline the process of correcting data entry errors and/or pointing out purposeful misidentification, connecting information about a person across HSI datasets, and cutting down the number of resource hours needed for investigations.",,,,8/17/22,
Department of Interior,USGS,AI205,"Wildlife Underpass Camera Trap Image Classification, San Diego CA","This software system takes wildlife camera trap images as inputs and outputs the probability of the image belonging to user-specified taxonomic classes based on wildlife species present in each image. (Wildlife camera traps are motion-triggered, time lapse, and other camera systems placed in the field to capture images of wildlife at the location and times where and when the cameras are placed.) The process of humans reviewing, labeling, and QA/QCing labels is labor intensive, time consuming, and costly. Developing AI systems that can perform these tasks within an acceptable level of accuracy can reduce the costs in extracting tabular data from camera-based datasets and increase the volume of data for analysis. The system supports training experiments where model hyperparameters and training dataset characteristics can be varied to find those that are more optimal for training. Training, validation, and testing datasets have human-assigned labels and are used to train and evaluate the models. Once trained, the models can be used to predict classes on unlabeled images. We use a convolutional neural network (CNN) approach based on TensorFlow and training is run on the USGS Tallgrass supercomputer designed for AI/ML workflows.",,,,3/18/22,
Department of Interior,USGS,AI206,Walrus Haulout Camera Trap Image Classification,"This project extends the application of codes developed for wildlife underpass camera trap image classification. Similarly, the system takes walrus haulout camera trap images as inputs and outputs the probability of the image containing walruses and various human disturbances (boats, aircraft, etc.). We will use and further develop the previous system's capability of supporting training experiments. Training, validation, and testing datasets have human-assigned labels and are used to train and evaluate the models. Once trained, the models can be used to predict classes on unlabeled images from ongoing camera monitoring efforts. We use a convolutional neural network (CNN) approach based on TensorFlow and training is run on the USGS Tallgrass supercomputer designed for AI/ML workflows.",,,,3/18/22,
Department of Interior,USGS,AI207,ARMI Amphibian Species ID from Acoustic Data,"The mission of the USGS Amphibian and Reptile Minitoring Initiative (ARMI) is to provide essential scientific information to managers to help arrest or reverse amphibian population declines. Acoustic monitoring of amphibian (anuran) vocalizations are a core technique used by ARMI researchers. Reviewing audio recordings and identifying species vocalizations captured therein is time consuming and labor intensive. For these reasons, many recordings remain unprocessed, preventing valuable data from being available for analysis. Our goal is to train convlutional neural networks (CNNs) that take audio clips that have been converted to sonograms (images) and classify the species generating the vocalizations in the recordings. Our initial prototype project will attempt to develop models that can identify audio clips containing bullfrog (*Lithobates catesbeianus*, which are native in some parts of the US and a destructive invasive species in others) vocalizations. The software will be build using the TensorFlow Python API and training will be performed on the USGS Tallgrass supercomputer.",,,,3/18/22,
Department of Interior,USGS,AI208,Individual Mountain Lion ID from Camera Data,"This system will to take pairs of mountain lion (*Puma concolor*) facial images and output the probability that the images come from the same individual mountain lion. This will allow researches to passively ""mark"" individuals and support population estimation analyses. We will use a ""Siamese"" convolutional neural network architecture that has been used in other facial recognition and motion tracking applications.",,,,3/18/22,
Department of Interior,USGS,AI209,Walrus Object Detection in Drone/Satelite Imagery,"This system, once developed, will input drone imagery and output bounding boxes for individual walruses. If successful, this will allow researchers with Alaska Science Center to count the numbers of walruses in drone imagery to support population reseaarch. The system will use TensorFlow-based convolutional neural networks for object detection trained on the USGS Tallgrass supercomputer.",,,,3/18/22,
Department of Interior,USGS,AI210,PRObability of Streamflow PERmanence,"The PROSPER modeling framework was developed to incorporate sparse streamflow observation data representing wet or dry stream conditions and gridded hydroclimatic explanatory data to predict the annual probability of streamflow permanence at 30-m (PROSPER Pacific Northwest) or 10-m (PROSPER Upper Missouri) resolution. The training data are point observations of wet or dry at locations in the Pacific Northwest or Upper Missouri River basin. The PROSPER models were primarily developed using the FCPGTools (Barnhart, Sando, et al., 2020), R, and USGS HPC resources (Yeti).",,,,3/18/22,
Department of Interior,USGS,AI211,Water Mission Area Drought Prediction Project,"The goal of this project is to develop a method for predicting daily hydrologic drought using machine learning models calibrated on streamflow data (response) and meteorological forcing data. Models will be built at individual gages across CONUS, then transferred to ungaged basins using a 'donor model' approach that identifies which gages are most similar to the ungaged basin and combines the models from those gages for the final prediction. Models will be developed and run on the USGS HPC systems. ",,,,3/18/22,
Department of Interior,USGS,AI212,Water Mission Area Regional Drought Early Warning System,"The goal of this project is to build and test multiple ML models for predicting and forecasting daily hydrologic drought in the Colorado River Basin (CRB). Similar to the project listed in line 9, we use gridded meteorologic forcing data and daily streamflow data in the CRB to build random forest and neural networks (long-short term memory) to determine the best approach to predicting and forecasting hydrologic drought. The project is being developed on AWS and in cooperation with CHS. We are also using the USGS HPC systems.",,,,3/18/22,
Department of Interior,USGS,AI213,AI system to recognize individual fish and disease,"This study focuses on the development of an AI system to recognize individual fish and their disease status from images. Success of this effort could complement or replace traditional mark-recapture methods used for estimating abundance, survival, and movement, and this could greatly reduce costs to fisheries managers. Likewise, disease detection from images could enable new approaches for assessing status and trends in fish health.",,,,3/18/22,
Department of Interior,USGS,AI214,River Image SEnsing,"The River Image Sensing (RISE) project is charged with the development of a reliable camera system for integration into the operational streamgage monitoring network of the USGS Water Mission Area. In addition to capturing images and videos, the RISE system will be capable of producing time-series of surface water levels derived from still camera images using AI/ML modeling techniques.",,,,3/18/22,
Department of Interior,USGS,AI215,Estimating stream flow from images in headwaters,"The goals of this project are to 1) measure how much water flows in small, ungaged stream networks using timelapse images captured by inexpensive and off-the-shelf cameras and 2) provide a web-based platform for making the images, associated climate and other related data as well as the model itself easy to access and explore. Data for training come from user-uploaded imagery and flow data (when available). Database is available for uploading and image viewing here: https://www.usgs.gov/apps/ecosheds/fpe/",,,,3/18/22,
Department of Interior,USGS,AI216,Economic valuation of fisheries in the Delaware River,"The goal is to link existing hydrological flow data (e.g., USGS stream gages) and models (e.g., USGS Process-Guided Deep Learning Models for flow and temparture) with trout population dynamic models, changes to fish catch, and the economic benefits of recreational fishing. These trout population dynamic models will be developed based on observational data, existing literature estimates, and existing models.",,,,3/18/22,
Department of Interior,USGS,AI217,Stream physical habitat characterization in the Chesapeake Bay Watershed,"The project objective is to take a large dataset of rapid habitat assessment data collected by multiple jurisdictions in the Chesapeake Bay Watershed, train a predictive model using those data, and use that model to predict stream habitat conditions for all unmeasured stream reaches in the region. The model is able to generate predictions for multiple aspects of physical habitat condition. The model directly connects to EPA's database containing the training data, enabling it to be rapidly updated when new data updates occur.",,,,3/18/22,
Department of Interior,USGS,AI218,"Deep Learning for Automated Detection and Classification of Waterfowl, Seabirds, and other Wildlife from Digital Aerial Imagery","Our project includes two stages of AI, the first is a binary detector to automate the detection of wildlife in aerial imagery and the second is a robust classification algorithm to automate the taxonomic classification of wildlife from the binary detector. The input of the first and second stage are manually annotated polygons around targets of interest and their taxonomic classification values from family to species, respectively. We use Tallgrass to develop and train our algorithms, BlackPearl/Caldera to store our large image datasets, a hosted instance of a customized version of the Computer Vision Annotation Tool to gather manually annotated data, and a separate PostgreSQL database to store annotations and image metadata.",,,,3/18/22,
Department of Interior,USGS,AI219,Prediction of Regolith Thickness in the Delaware River Basin,This project uses observations of the depth to bedrock reported by private well drillers in the Delaware River Basin to train a Random Forest model to map the thickness of the regolith layer. This data product will support groundwater and hydrologic modeling efforts in the basin.,,,,3/18/22,
Department of Interior,USGS,AI220,ML-Mondays course on applications of deep learning to image analysis,"A course in application of deep learning image segmentation, image classification, and object-in-image detection. Course includes software written in Python using Keras and Tensorflow ML libraries, software documentation, data, website, and slides. See course website https://dbuscombe-usgs.github.io/MLMONDAYS/ for more details",,,,3/18/22,
Department of Interior,USGS,AI221,Coast Train,"Coast Train is a multi-labeler ML-ready dataset of orthomosaic and satellite images of coastal, estuarine, and wetland environments and corresponding thematic label masks. The data consist of spatial and time-series, and contains 1.2 billion labelled pixels, representing over 3.6 million hectares.",,,,3/18/22,
Department of Interior,USGS,AI222,Seabird and Marine Mammal Surveys Near Potential Renewable Energy Sites Offshore Central and Southern California,"The Seabird Studies Team at the Western Ecological Research Center (WERC), with support from the Bureau of Ocean Energy Management (BOEM), completed aerial photographic surveys of the ocean off central and southern California between 2018-2021. Over 800,000 high resolution images of the ocean were collected, with the goal of extracting and counting marine birds and mammals contained within. To process this volume of images machine learning offered the best methodology, but publicly available training data did not exist for this specific purpose. Through a collaboration with Conservation Metrics, Inc. we created a labeled training dataset using Faster RCNN models via active learning and transfer learning. We then evaluated a set of candidate models trained on different label aggregation schemes, selected a final model utilizing YOLOv5 architecture, and ran the final model on the complete image dataset. Images output from the final model classified targets into seven categories: bird, dark bird, dark bird flying, light bird, fish, marine mammal, and other. We are currently reviewing the final model output for false positives and negatives to evaluate performance. Next, we will reclassify model labels to the lowest taxonomic group possible. This manual review is occurring in the USGS cloud environment (Amazon Web Services) utilizing the opensource Computer Vision Annotation Tool (CVAT). Once low taxonomic reclassification is complete, we will generate maps of species distribution and abundance to inform BOEM’s planning in advance of potential offshore wind energy development along the California coast.",,,,3/18/22,
Department of Interior,USGS,AI223,Fouling Identification Neural Network (FINN),"Our product is an end-to-end system that is used to predict and detect sensor (sonde) fouling at USGS stream gages. The system is trained using supervised learning on multiple features derived from archived stream gage data labelled by expert field technicians. The system operated in real-time on Amazon Web Services (AWS), providing predictions every 30 minutes based off of raw data collected from the USGS AQUARIUS database. The system produces values detecting the likelihood that fouling is currently present and likelihood of fouling predicted to occur in the next 24 hours. These values are displayed on a Tableau dashboard that is connected to AWS using Amazon Athena. This dashboard also displays other stream gage network monitoring information from AQUARIUS, like time since a sonde was last visited by a technician.",,,,3/18/22,
Department of Interior,USGS,AI224,Mapping river bathymetry from remotely sensed data ,"We are using high frequency satellite images from the Planetscope constellation to estimate water depth in river channels. The short time lags between images allows us to average multiple scenes collected on the same day or within a couple of days to improve accuracy. In addition to established depth retrieval methods, we developed a neural network regression approach for this purpose. The training data consist of field measurements of water depth collected as part of other USGS projects on five different rivers. The neural network regression method is implemented in MATLAB using the Deep  Learning Toolbox.",,,,3/18/22,
Department of Interior,USGS,AI225,Mapping benthic algae along the Buffalo National River from remotely sensed data,"This study involves using orthophotos acquired from a manned, fixed-wing aircraft and multispectral images from two different satellites to map bottom-attached (benthic) algae along the Buffalo National River in northern Arkansas. The training data for this effort consist of field observations of water depth and percent cover of benthic algae along 8-10 cross sections from two distinct reaches of the Buffalo River. These field data are used to train a bagged trees (aka random forest) classification algorithm to distinguish among four ordinal levels of algal density: none, low, medium, and high.",,,,3/18/22,
Department of Interior,USGS,AI226,Characterization of Sub-surface drainage (tile drains) from satellite imagery,"Without knowing how tile-drain extent (sub-surface agricultural drainage) has changed with time, it is difficult to differentiate how streamflow and water quality have changed as a result of spatial extent and characteristics of tile-drain networks.  Our method delineates tile drains in satellite imagery, providing a way to look at historical imagery and to use satellite data to maintain an up-to-date geospatial layer of tile drain extent in basins of interest.  We use panchromatic imagery that is processed using a UNet model that was trained on a library of panchromatic images on which visible tile-drain networks had been traced.  Our workflow uses a combination of python scripting that is encapsulated in a Jupyter notebook; the entire process is open source.  ",,,,3/18/22,
Department of Interior,USGS,AI227,Waterfowl Lifehistory and Behavior Classification,"The model we developed provides a highly accurate daily classification of waterfowl behavior into 8 life history states/movement patterns using hourly GPS relocations and, optionally, remotely sensed habitat data.  This will provide waterfowl researchers and managers a tool for real-time capable rapid assessments and notification of important life history events to improve research and management outcomes and reduce project operational costs.  ",,,,3/18/22,
Department of Interior,USGS,AI228,Spot Elevation OCR from historical topo maps,The goal of this project is to create a database of summit spot elevations from the HTMC labeled for summits in CONUS.,,,,3/18/22,
Department of Interior,USGS,AI229,TerrainFeatures detection and recognition,The objective of this project is to use DL tools to extract terrain features. ,,,,3/18/22,
Department of Interior,USGS,AI230,The National Landcover database,"NLCD uses AI/ML to develop Landcover across all 50 states. The system includes HPC processes, cloud services, and local resources to create thematic and continuous field classifications. These classifications serve as the base for users and federal agencies across the nation to provide wildlife habitat estimates, urban runoff estimates, population growth, etc. etc.",,,,3/18/22,
Department of Interior,USGS,AI231,Artificial Intelligence for Environment & Sustainability (ARIES),"ARIES is an international research project based at the Basque Centre for Climate Change (Bilbao, Spain), to which USGS has been a long-term collaborator. ARIES uses semantics and machine reasoning to enable AI-assisted multidisciplinary, integrated modeling of coupled human-natural systems. See https://aries.integratedmodelling.org/ and https://docs.integratedmodelling.org/technote/ for full details.
ARIES is a full-stack solution for integrated modelling, supporting the production, curation, linking and deployment of scientific artifacts such as datasets, data services, modular model components and distributed computational services. Its purpose is to ensure — by design rather than just intent — that the pool of such artifacts constitutes a seamless knowledge commons, readily actionable (by humans and machines) through a full realization of the linked data paradigm augmented with semantics and powered by artificial intelligence. This design enables automation of a wide range of modeling tasks that would normally require human experts to perform.
ARIES’ underlying software stack, called k.LAB, includes client and server components that support the creation, maintenance and use of a distributed semantic web platform where scientific information can be stored, published, curated and connected. The software is licensed through the Affero General Public License (AGPL) v.3.0.",,,,3/18/22,
Department of Interior,USGS,AI232,Global Inland Fisheries Risk Index,We applied coupled manual and machine learning methods to an expansive literature set for major global inland fisheries to explore opportunities for improving user efficiency for linking anthropogenic drivers of environmental change to direct impacts. This work informs the relative influence of threats in the development of a global inland fisheries assessment using boosted regression trees to derive a spatially-explicit risk index of stressors.,,,,3/18/22,
Department of Interior,USGS,AI233,Fish and Climate Change Database (FiCli),"The Fish and Climate Change Database (FiCli) is a comprehensive database of peer-reviewed literature compiled through an extensive, systematic primary literature review to identify English-language, peer-reviewed journal publications with projected and documented examples of climate change impacts on inland fishes globally. We are currently exploring options to automate certain portions of the review process to increase our efficiency in maintaining and updating the database.",,,,3/18/22,
Department of Interior,USGS,AI234,Evaluating fish movement in restored coastal wetlands using imaging sonar and machine learning models,"Wetland managers are restoring coastal wetland habitats in the Great Lakes, and often seek more information on when and how fish access restored habitats. Terabytes of hydroacoustic data on fish movement need to be analyzed more efficiently, so a collaboration between USGS, USFWS, and the University of Michigan is developing a machine learning model (MLM) that identifies, tracks, and quantifies fish movement. The completed model will read proprietary sonar image files, convert them to a universal file format (i.e., .mp4), place bounding boxes around individual fish detected by the model, and track them across consecutive image frames to determine bi-directional movement. The model uses training data and TensorFlow-based convolutional neural networks for object detection. Post-processing uses sonar geometry to estimate length of individual fish, and output files include labeled videos showing bounding boxes (.avi format), model metrics (.txt format), an enumeration of bi-directional fish movement (i.e., left or right) in .csv format, and individual fish length estimates (.csv format). Model output will allow USGS researchers to estimate fish habitat use and associated community metrics in restored wetland habitats. This information will support USFWS and other management agencies restoring coastal wetland habitats.",,,,3/18/22,
Department of Interior,USGS,AI235,Fluvial Fish Native Distributions for the Conterminous United States using the NHDPlusV2.1 and Boosted Regression Tree (BRT) Models,"Species distribution models are developed for 271 fluvial fish species in their native ranges of the conterminous United States.  Boosted Regression Tree (BRT) models were used to develop presence/absence predictions for each of the National Hydrography Dataset Plus Version 2.1 stream segments within a species' native range.  Landscape data that describe the natural variation (e.g., slope, precip) and anthropogenic impacts (e.g., stream fragmentation) were summarized to stream segments and used as predictor variables. Native species ranges were used to geographically constrain distribution modeling efforts. R Version 4.0.2 (or newer) with ‘dismo’ and ‘labdsv’ packages are used for modeling.",,,,3/18/22,
Department of Interior,USGS,AI236,Prediction of Inland Salinity in the Delaware River Basin,"Once developed, the system will input watershed characteristics (soils, land cover), land use (road salt application) and meteorological timeseries, and output predictions of specific conductance (SC) for inland stream reaches in the Delaware River Basin (DRB). The model will be trained using SC sample data from within the DRB. The resulting model will allow for predictions in ungaged locations and time periods, and allow for an evaluation of salinity exposure in these stream reaches. The model will be built using pyTorch on the USGS Tallgrass supercomputer.",,,,3/18/22,
Department of Interior,USGS,AI237,Prediction of Salt Front Location in the Delaware River Estuary,"We are developing a machine learning model to make predictions of the 250 mg/L isochlor (salt front location) within the Delaware River Estuary. The model will be driven by river discharge into the estuary, tidal forcings, and meterological data from several points throughout the estuary. Model predictions will be compared with a process-based, hydrodynamic model, COAWST. The machine learning model is currently in development, but it will consist of a recurrent neural network architecture built using tools from pyTorch.",,,,3/18/22,
Department of Interior,USGS,AI238,Prediction of Water Temperature in the Delaware River Basin,We published a machine learning model to make water temperature predictions at 456 reaches in the Delaware River Basin. The recurrent graph convolutional network (RGCN) was pre-trained with predictions from a coupled process-based model that predicts stream flow and temperature (the Precipitation Runoff Modeling System with the coupled Stream Network Temperature Model or PRMS-SNTemp).,,,,3/18/22,
Department of Interior,USGS,AI239,Forecasting Water Temperature in the Delaware River Basin ,We developed a process-guided deep learning and data assimilation approach to operationally produce 7-day forecasts of daily maximum stream water temperature downstream of drinking water reservoirs in support of water management decisions. Our process-guided deep learning model was pretrained on output from an integrated stream-reservoir process-based model and used an autoregressive technique and data assimilation to ingest real-time observations of stream temperature to improve near-term forecasts. Our modeling system produced forecasts of daily maximum water temperature with an average root mean squared error (RMSE) from 1.1 to 1.4°C for 1-day-ahead and 1.4 to 1.9°C for 7-day-ahead forecasts across all sites. ,,,,3/18/22,
Department of Interior,USGS,AI240,Prediction of Flood Flow Metrics for Minimally Altered Catchments,"Once developed, the system will input watershed characteristics (soils, land cover) and long-term meteorological data, and output predictions of flood flow metrics (magnitude, duration, frequency, volume) for stream reaches. Two models will be trained using gage data from regions surrounding the Delaware River Basin and the Colorado River Basin. The resulting models will allow for estimating flood flow metrics in ungaged reaches, which can be used to inform infrastructure designs along those reaches (e.g., bridges). The current deliverable is predictions for minimally altered catchments, and future years may expend to predictions in altered catchments (e.g., those with dam regulation). The models will be built using various R packages on the USGS Tallgrass supercomputer.",,,,3/18/22,
Department of Interior,USGS,AI241,Process-Guided Deep Learning Predictions of Lake Water Temperature,"This process-guided deep learning model predicts depth-specific lake temperatures while obeying physical laws using inputs of meteorological drivers. Training consists of two stages. In the first stage, the model is pre-trained using process-based modeling outputs. Then, lake temperature observations are used to finetune the model in a second training stage. The models are trained to simultaneously fit observations and honor conservation of energy. The models were developed using various R and Python packages on the USGS Tallgrass supercomputer. The General Lake Model (GLM version 2) software was used for process-based modeling.",,,,3/18/22,
Department of Interior,USGS,AI242,Prediction of Lake Water Temperature using Lake Attributes,"Once developed, the system will input lake characteristics (surface area, elevation, and others to be determined) and output predictions of depth-specific lake temperatures. Training data consist of lake temperature observations, meteorological data, and lake characteristics. The models will be developed using various Python packages including PyTorch on the USGS Tallgrass supercomputer.",,,,3/18/22,
Department of Interior,USGS,AI243,Process-Guided Deep Learning for Dissolved Oxygen Predictions on Stream Networks,"1) this model predicts daily minimum, mean, and maximum dissolved oxygen (DO) concentrations at several stream locations in the Delaware River Basin. The inputs used are meteorological inputs (e.g., precipitation, cloud cover) and static catchment attributes (e.g., basin area). 2) the training data are DO concentrations collected by the USGS and made available via the National Water Information System (NWIS). 3) this work is being done using Python and R. The deep learning models were written via TensorFlow, the data prepartion is in R, and the modeling workflow was scripted via Snakemake.",,,,3/18/22,
Department of Interior,USGS,AI244,Multi-task deep learning for daily streamflow and water temperature,"1) This model predicts two interdependent variables, daily average streamflow and daily average stream water temperature, together using multi-task deep learning. A multi-task scaling factor controls the relative contribution of the auxiliary variable’s error to the overall loss during training. Input data include meteorological variables such as rainfall and humidity. 2) The training data were streamflow and water temperature observations. The stream temperature data were collected by the USGS and made available via NWIS. The streamflow observations were also collected by the USGS but collated along with input drivers in the CAMELS dataset. 3) This work was done using Python. The deep learning models were written via TensorFlow and the modeling workflow was scripted via Snakemake.",,,,3/18/22,
Department of Interior,USGS,AI245,Predicting Water Temperature Dynamics of Unmonitored Lakes With Meta‐Transfer Learning,"The approach compares the transfer of different model types from well-observed to unobserved lake systems. Process-based models, neural networks, and process-guided neural networks are trained on well observed lakes (source lakes) and then is used to make predictions in unobserved lakes (target lakes). The performance of each of those transfers is used to train a meta-model that uses lake characteristics (e.g., depth, area) to predict which source lakes will be good candidates for transfer to target lakes. The process-guided deep learning models were able to transfer better than process-based and pure machine learning approaches. ",,,,3/18/22,
Department of Interior,USGS,AI246,Process-guided deep learning for predicting stream temperature in out-of-bound conditions,"1) This work uses meteorological drivers to predict network wide daily average stream temperature in the Delaware River Basin. 2) The training data are water temperature observations available through NWIS and collected by the UGSS. 3) The work compares the performance of two deep learning achictectures, both of which incorporate process guidance through pretraining on process-based modelling outputs.  For each architecture, we're testing the ability of the model to generalize outside the bounds of its training data in order to better understand the limitations of each modelling approach for accurately predicting stream temperature under changing climate and precipitation regimes.",,,,3/18/22,
Department of Interior,USGS,AI247,Process guidance for learning groundwater influence on stream temperature predictions,"1) This work uses meteorological drivers to predict network wide daily average stream temperature in the Delaware River Basin. 2) The training data are water temperature observations available through NWIS and collected by the UGSS. 3) The work focuses on developing a custom loss function that helps deep learning models learn to account for groundwater influence on stream temperature.  Specifically, it uses the phase lag and amplitude dampening effect of groundwater to identify reaches likely influenced by shallow and deep groundwater inputs. ",,,,3/18/22,
Department of Interior,USGS,AI248,Explainable AI and interpretable machine learning,"This work focuses on developing expertise and resources for Explainable AI (XAI) within WMA PUMP Projects.  The inputs are various models developed for predicting stream temperature, discharge, dissolved oxygen, and other characteristics.  The outputs are interpretable metrics to help understand why models are making the predictions they are and what physical processes are getting captured with the model architectures.",,,,3/18/22,
Department of Interior,USGS,AI249,AI applications to mapping surface water,"The research is investigating the use of hand annotated hydrography from one region to train an artificial neural net (ANN) to identify where surface water is likely to be in other areas. The input training data includes lidar, radar, and other remotely sensed data along with modeled surface flow to inform the model. The work is using open-source tools in a high-performance computing environment. ",,,,3/18/22,
Department of Interior,USGS,AI250,Where’s the Rock: Using Neural Networks to Improve Land Cover Classification,"While machine learning techniques have been increasingly applied to land cover classification problems, these techniques have not focused on separating exposed bare rock from soil covered areas. Therefore, we are using a neural network to differentiate exposed bare rock (rock) from soil cover (other). We started with a training dataset by mapping exposed rock at 20 test sites across the Sierra Nevada Mountains (California, USA) using USDA’s 0.6 m National Aerial Inventory Program (NAIP) orthoimagery. These initial sites were used to train and test the original CNN and now NASA's DELTA toolkit, which is being run on the USGS high-performance computing facilities. The goal is to generate a machine learning approach to classify bare rock in NAIP orthoimagery, starting with the Sierras, in order to provide a more accurate map of soil vs. rock-covered areas for use in landslide hazard mapping, quantifying soil carbon storage, calculating water fluxes, etc.",,,,3/18/22,
Department of Interior,USGS,AI251,Data–driven prospectivity modelling of sediment–hosted Zn–Pb mineral systems and their critical raw materials,"Regional data (magnetics and their derivatives, gravity and their derivatives, black shales, terrane boundaries, LAB depths, permissive geology, paleo-latitude etc.) is loaded into Uber's H3 cube. Clastic Dominated (CD) and  Mississippi Valley Type (MVT) deposits are used to train a Weights of Evidence model and two different Gradient-Boosting Machine models. After training occured the result was a prospecticvity map for CD and MVT deposits in the three countries. ",,,,3/18/22,
Department of Interior,USGS,AI252,"Updating Real-time Earthquake Shaking, Ground Failure, and Impact products with remote sensing and ground truth observations","A breakthrough for rapid post-earthquake ground failure (GF) and loss modeling and reporting has been achieved with initial Bayesian updating of our global loss and GF models with ground-truth observations. Empirical models suffer from limited performance due to the complex, event-specific causal effects underlying the cascading processes of earthquake-triggered hazards and impacts. In contrast, satellite imagery-based impact assessments (e.g., NASA’s Damage Proxy Maps, or DPMs), while spatially accurate, lack the specificity as to what physical process caused those image changes. We present the first rapid seismic multi-hazard and damage updating framework based on variational Bayesian causal inference and remotely sensed DPMs. This machine learning framework enables accurate and high-resolution multi-hazard and damage estimates by jointly inferring shaking and secondary hazards and resulting building damage and quantifying their causal dependencies from imagery and prior loss and GF models. The underlying physical causal dependencies are modeled using a multi-layer causal Bayesian network. Initial results are impressive, showing that our framework significantly improves the GF prediction abilities. It also reveals the event-specific causal dependencies among ground shaking, GF, building damage, and other environmental factors. We expect improved PAGER products to more rapidly evolve to accurate and thus more actionable images, maps, and products. ",,,,3/18/22,
Department of Interior,USGS,AI253,Using Artificial Neural Networks to Improve Earthquake Ground-Motion Models,"The ML model provides estimates of peak ground-motion from earthquakes given the location, magnitude, and local geological structure at a site of interest. The training data is a compilation of about 12,000 peak ground-motions recorded at seismic stations for moderate to large earthquakes. I constructed the ML model in Python using Keras with TensorFlow.",,,,3/18/22,
Department of Interior,USGS,AI254,Leveraging Deep Learning to Improve Earthquake Monitoring,"The USGS National Earthquake Information Center monitors global earthquakes 24/7, rapidly detecting, characterizing, and publically desimating earthquake information. In order to improve the perfomance of their event characterization system, the NEIC has trained AI models to characterize earthquake source information using small portions of waveform data. These models improve autotmatic phase picking, classify phase types, and estimate source-station distances. The outcome of these models is improved automatic earthquake detections. The training dataset used in these models leverages the long standing reveiwed earthquake catalog produced by the NEIC combined with archived continous waveform recordings, many of which are USGS opperated stations. These tools have been developed primarily leveraging Python, Keras, and Tensorflow. ",,,,3/18/22,
Department of Interior,USGS,AI255,Using Gradient Boosting Method and Feature Selection to Reduce Aleatory Uncertainty of Earthquake Ground-Motion Models,"We develop ground-motion models for peak ground acceleration and peak ground velocity using a gradient boosting method (GBM). In total 128 GBM-based ground-motion models are developed for estimating PGA and PGV, respectively, using varying subsets of explanatory variables. We select eight GBM-based ground-motion models that have the lowest root mean squared error (rmse) for the cross-validation datasets among models with the same number of explanatory variables. The secondary variables, in order of importance, that contribute to the model accuracy are: VS30, Ztor, Ry, Rx, Rake, Zhyp, and Dip. By considering the tradeoff between the model accuracy and model complexity (number of explanatory variables), we find an optimal model to predict PGA and PGV uses four explanatory variables: M, Rjb, VS30, and Ztor. The variability decomposition results suggest that the reduction of total variability is mostly due to the reduction of inter-event variability, likely because more source parameters than site or path parameters are included as explanatory variables. ",,,,3/18/22,
Department of Interior,USGS,AI256,Application of machine learning to  ground motion-based earthquake early warning,"We use initial observations of an earthquake on seismic stations close to an earthquake to predict what the peak ground shaking will be across a region. The initial test dataset are waveforms from the USGS-collected, large-n seismic array in an area of induced seismicity in Oklahoma. Future datasets will include seismic data from the California Seismic Integrated Network (primarily supported by USGS) and possible the Japanese Meterological Agency. Currently running Python-based codes on a desktop, plan to move to AWS or similar. ",,,,3/18/22,
Department of Interior,USGS,AI257,A machine learning approach to developing ground motion models from simulated ground motions,"We use a machine learning approach to build a ground motion model (GMM) from a synthetic database of ground motions extracted from the Southern California CyberShake study. An artificial neural network is used to find the optimal weights that best fit the target data (without overfitting), with input parameters chosen to match that of state-of-the-art GMMs. We validate our synthetic-based GMM with empirically based GMMs derived from the globally based Next Generation Attenuation West2 data set, finding near-zero median residuals and similar amplitude and trends (with period) of total variability. Additionally, we find that the artificial neural network GMM has similar bias and variability to empirical GMMs from records of the recent Mw7.1 Ridgecrest event, which neither GMM has included in its formulation. As simulations continue to better model broadband ground motions, machine learning provides a way to utilize the vast amount of synthetically generated data and guide future parameterization of GMMs. ",,,,3/18/22,
Department of Interior,USGS,AI258,Integrating machine learning phase pickers into the Southern California Seismic Network earthquake catalog,"We evaluate the readiness of machine-learning models for automatic earthquake detection and phase picking to enhance the Southern California Seismic Network earthquake catalog, with the end-goal of using these models in routine seismic network operations. We first test a model called Generalized Phase Detection (GPD), trained on millions of manually-picked P- and S- arrival times from Southern California earthquakes and examples of noisy time series data.  Inputs are continuous seismic data time series, with 3 components (north, east, vertical), at hundreds of seismic stations located in southern California.  Outputs are arrival times of P and S seismic waves, with associated probabilities between 0 and 1, with a threshold probability applied for detection; these arrival times are fed into existing software to estimate earthquake locations, origin times, and magnitudes.  Custom software is written in Python with the model implemented in the PyTorch library.  We are also developing a cloud-native software architecture that takes real-time seismic data as input (~15 seconds at a time) and applies the GPD model within Amazon Web Services.",,,,3/18/22,
Department of Interior,USGS,AI259,Understanding the 2020-2021 Puerto Rico Earthquake sequence with deep learning approaches,"We enhance the earthquake catalog for the 2020-2021 southwestern Puerto Rico earthquake sequence with a variety of deep learning approaches to understand its complex fault system, triggering mechanisms, and long-lived vigorous nature of the aftershock sequence.  We use an existing deep learning model for earthquake detection and phase picking called EQTransformer, which was trained on a global data set of earthquake waveforms called STEAD, using the TensorFlow library.  We also apply deep learning methods for earthquake location (EikoNet and HypoSVI), trained on a known velocity model with a physics informed neural network using the PyTorch library, which then allows grid-free rapid seismic wave travel time calculation between any 2 locations within a 3D volume .  These machine learning methods for automatic earthquake detection, phase-picking, and location, which are all available as open-source Python codes, help increase the number of small earthquake observations and improve earthquake depth estimates, thus offering more detailed information about active faults and physical processes in this earthquake sequence.",,,,3/18/22,
Department of Interior,BLM,AI260,Land Use Plan Document and Data Mining and Analysis R&D,"Exploring the potential to identify patterns, rule alignment or conflicts, discovery, and mapping of geo history and/or rules. Inputs included unstructured planning documents. Outputs identify conflicts in resource management planning rules with proposed action locations requiring exclusion, restrictions, or stipluations as defined in the planning documents. ",,,,3/18/22,
Department of Interior,BOR,AI261,Data Driven Sub-Seasonal Forecasting of Temperature and Precipitation ,"Reclamation has run 2, year-long prize competitions where particants developed and deployed data driven methods for sub-seasonal (2-6 weeks into future) prediction of temperature and precipitation across the western US. Particpants outperformed benchmark forecasts from NOAA. Reclamation is currently working with Scripps Institute of Oceanography to further refine, evaluate, and pilot implement the most promising methods from these two copmetitions. Improving sub-seasonal forecasts has significant potential to enhance water management outcomes.  ",,,,3/18/22,
Department of Interior,BOR,AI262,Data Driven Streamflow Forecasting,"Reclamation, along with partners from the CEATI hydropower industry group (e.g. TVA, DOE-PNNL, and others) ran a year-long  evaluation of existing 10-day streamflow foreasting technologies and a companion prize competition open to the public, also focused on 10-day streamflow forecasts. Forecasts were issued every day for a year and verified agains observed flows. Across locations and metrics, the top perfoming foreacst product was a private, AI/ML forecasting company - UpstreamTech. Several competitors from the prize competition also performed strongly; outperforming benchmark forecasts from NOAA. Reclamation is working to further evaluate the UpstreamTech forecast products and also the top performers from the prize competition.  ",,,,3/18/22,
Department of Interior,BOR,AI263,Seasonal/Temporary Wetland/Floodplain Delineation using Remote Sensing and Deep Learning,"Reclamation was interested in determining if recent advancements in machine learning, specifically convolutional neural network architecture in deep learning, can provide improved seasonal/temporary wetland/floodplain delineation (mapping) when high temporal and spatial resolution remote sensing data is available? If so, then these new mappings could inform the management of protected species and provide critical information to decision-makers during scenario analysis for operations and planning.",,,,3/18/22,
Department of Interior,BOR,AI264,Improving UAS-derived photogrammetric data and analysis accuracy and confidence for high-resolution data sets using artificial intelligence and machine learning,"UAS derived photogrammetric products contain a large amount of potential information that can be less accurate than required for analysis and time consuming to analyze manually. By formulating a standard reference protocol and applying machine learning/artificial intelligence, this information will be unlocked to provide detailed analysis of Reclamation's assets for better informed decision making.",,,,3/18/22,
Department of Interior,BOR,AI265,Photogrammetric Data Set Crack Mapping Technology Search ,"This project is exploring a specific application of photogrammetric products to process analysis of crack mapping on Reclamation facilites.  This analysis is time consuming and has typically required rope access or other means to photograph and locate areas that can now be reached with drones or other devices.  By formulating a standard reference protocol and applying machine learning/AI, this information will be used to provide detailed analysis of Reclamation assets for better decision making. ",,,,3/18/22,
Department of Interior,BOR,AI266,Improved Processing and Analysis of Test and Operating Data from Rotating Machines,"This project is exploring a better method to analyze DC ramp test data from rotating machines. Previous DC ramp test analysis requires engineering expertise to recognize characteristic curves from DC ramp test plots. DC ramp tests produce a plot of voltage vs current for a ramping voltage applied to a rotating machine. By using machine learning/AI tools, such as linear regression, the ramp test plots can be analyzed by computer software, rather than manual engineering analysis, to recognize characteristic curves. The anticipated result will be faster and more reliable analysis of field-performed DC ramp testing.",,,,3/18/22,
Department of Interior,BSEE,AI267,Sustained Casing Pressure Identification,"Well casing pressure requests are submitted to BSEE to determine whether a well platform is experiencing a sustained casing pressure (SCP) problem. SCP is usually caused by gas migration from a high-pressured subsurface formation through the leaking cement sheath in one of the well’s casing annuli, but SCP can also be caused by defects in tube connections, downhole accessories, or seals. Because SCP can lead to major safety issues, quickly identifying wells with SCP could greatly mitigate accidents on the well platforms",,,,3/18/22,
Department of Interior,BSEE,AI268,Level 1 Report Corrosion Level Classification,"Level 1 surveys obtained from BSEE report the condition of well platforms. The reports include images of well platform components, which can be used to estimate coating condition and structural condition, important factors in the overall condition of the facility. The reports are used to assess the well platforms for safety concerns. The reports are submitted to BSEE and are manually reviewed to determine whether a well platform needs additional audits. Because the manual review process is time-consuming, an automated screening system that can identify parts of the wells that exhibit excess corrosion may greatly reduce report processing time.",,,,3/18/22,
Department of Interior,BSEE,AI269,Well Activity Report Classification,Researching use of self-supervised deep neural networks to identify classification systems for significant well event using data from well Activity Reports,,,,3/18/22,
Department of Justice,Drug Enforcement Administration,AI270,Drug Signature Program Algorithms,DEA's Special Testing and Research Laboratory utilizes AI/ML techniques and has developed a robust statistical methodology including multi-variate statistical analysis tools to automatically classify the geographical region of origin of samples selected for DEA's Heroin and Cocaine signature programs. The system provides for detection of anomalies and low confidence results.,,,,,
Department of Justice,Federal Bureau of Investigation,AI271,Complaint Lead Value Probability,"Threat Intake Processing System (TIPS) database uses artificial intelligence (AI) algorithms to accurately identify, prioritize, and process actionable tips in a timely manner. The AI used in this case helps to triage immediate threats in order to help FBI field offices and law enforcement respond to the most serious threats first. Based on the algorithm score, highest priority tips are first in the queue for human review.",,,,,
Department of Justice,Justice Management Division,AI272,Intelligent Records Consolidation Tool,"The Office of Records Management Policy uses an AI and Natural Language Processing (NLP) tool to assess the similarity of records schedules across all Department records schedules. The tool provides clusters of similar items to significantly reduce the time that the Records Manager spends manually reviewing schedules for possible consolidation. An AI powered dashboard provides recommendations for schedule consolidation and review, while also providing the Records Manager with the ability to review by cluster or by individual record. The solution's technical approach has applicability with other domains that require text similarity analysis.",,,,,
Department of Justice,Tax Division,AI273,Privileged Material Identification,The application scans documents and looks for attorney/client privileged information. It does this based on keyword input by the system operator.,,,,,
Department of Labor,,AI274,Form Recognizer for Benefits Forms,Custom machine learning model to extract data from complex forms to tag data entries to field headers. The input is a document or scanned image of the form and the output is a JSON response with key/value pairs extracted by running the form against the custom trained model.,,,,,
Department of Labor,,AI275,Language Translation,Language translation of published documents and website using natural language processing models.,,,,,
Department of Labor,,AI276,Audio Transcription,Transcription of speech to text for records keeping using natural language processing models.,,,,,
Department of Labor,,AI277,Text to Speech Conversion,Text to speech (Neural) for more realistic human sounding applications using natural language processing models.,,,,,
Department of Labor,,AI278,Claims Document Processing,To identify if physician’s note contains causal language by training custom natural language processing models.,,,,,
Department of Labor,,AI279,Website Chatbot Assistant,"The chatbot helps the end user with basic information about the program, information on who to contact, or seeking petition case status.",,,,,
Department of Labor,,AI280,Data Ingestion of Payroll Forms,Custom machine learning model to extract data from complex forms to tag data entries to field headers. The input is a document or scanned image of the form and the output is a JSON response with key/value pairs extracted by running the form against the custom trained model.,,,,,
Department of Labor,,AI281,Hololens,AI used by Inspectors to visually inspect high and unsafe areas from a safe location.,,,,,
Department of Labor,,AI282,DOL Intranet Website Chatbot Assistant,"Conversational chatbot on DOL intranet websites to help answer common procurement questions, as well as specific contract questions.",,,,,
Department of Labor,,AI283,Official Document Validation,AI detection of mismatched addresses and garbled text in official letters sent to benefits recipients.,,,,,
Department of Labor,,AI284,Electronic Records Management,"Meeting NARA metadata standards for (permanent) federal documents by using AI to identify data within the document, and also using NLP to classify and summarize documents.",,,,,
Department of Labor,,AI285,Call Recording Analysis,Automatic analysis of recorded calls made to Benefits Advisors in the DOL Interactive Voice Repsonse (IVR) center.,,,,,
Department of Labor,,AI286,Automatic Document Processing,Automatic processing of continuation of benefits form to extract pre-defined selection boxes.,,,,,
Department of Labor,,AI287,Automatic Data Processing Workflow with Form Recognizer,Automatic processing of current complex worflow to extract required data.,,,,,
Department of Labor,,AI288,Case Recording summarization,Using an open source large language model to summarize publicly available case recording documents which are void of personal identifiable information (PII) or any other sensitive information. This is not hosted in the DOL technical environment and is reviewed by human note takers.,,,,,
Department of Labor,,AI289,OEWS Occupation Autocoder,The input is state submitted response files that include occupation title and sometimes job description of the surveyed units. The autocoder reads the job title and assigns up to two 6-digit Standard Occupational Classification (SOC) codes along with their probabilities as recommendations for human coders. Codes above a certain threshold are appended to the submitted response file and sent back to states to assist them with their SOC code assignment.,,,,,
Department of Labor,,AI290,Scanner Data Product Classification,"BLS receives bulk data from some corporations related to the cost of goods they sell and services they provide. Consumer Price Index (CPI) staff have hand-coded a segment of the items in these data into Entry Level Item (ELI) codes. To accept and make use of these bulk data transfers at scale, BLS has begun to use machine learning to label data with ELI codes. The machine learning model takes as input word frequency counts from item descriptions. Logistic regression is then used to estimate the probability of each item being classified in each ELI category based on the word frequency categorizations. The highest probability category is selected for inclusion in the data. Any selected classifications that do not meet a certain probability threshold are flagged for human review.",,,,,
Department of Labor,,AI291,Expenditure Classification Autocoder,Custom machine learning model to assign a reported expense description from Consumer Expenditure Diary Survey respondents to expense classification categories known as item codes.,,,,,
Department of State,Bureau of Global Public Affairs (GPA),AI292,Facebook Ad Test Optimization System,GPA’s production media collection and analysis system that pulls data from half a dozen different open and commercial media clips services to give an up-to-date global picture of media coverage around the world.,,,,,
Department of State,Bureau of Global Public Affairs (GPA),AI293,Global Audience Segmentation Framework,A prototype system that collects and analyzes the daily media clips reports from about 70 different Embassy Public Affairs Sections.,,,,,
Department of State,Bureau of Global Public Affairs (GPA),AI294,Machine-Learning Assisted Measurement and Evaluation of Public Outreach,"GPA’s production system for collecting, analyzing, and summarizing the global digital content footprint of the Department.",,,,,
Department of State,Bureau of Global Public Affairs (GPA),AI295,GPATools and GPAIX,GPA’s production system for testing potential messages at scale across segmented foreign sub-audiences to determine effective outreach to target audiences.,,,,,
Department of State,Office of U.S. Foreign Assistance Resources (F),AI296,NLP for Foreign Assistance Appropriations Analysis,Natural language processing application for F/RA to streamline the extraction of earmarks and directives from the annual appropriations bill. Before NLP this was an entirely manual process.,,,,,
Department of State,Office of the Under Secretary for Public Diplomacy and Public Affairs,AI297,Optical Character Recognition – text extraction,Extract text from images using standard python libraries; inputs have been websites to collect data,,,,,
Department of State,Office of the Under Secretary for Public Diplomacy and Public Affairs,AI298,Topic Modeling,Cluster text into themes based on frequency of used words in documents; has been applied to digital media articles as well as social media posts; performed using available Python libraries,,,,,
Department of State,Office of the Under Secretary for Public Diplomacy and Public Affairs,AI299,forecasting,"using statistical models, projecting expected outcome into the future; this has been applied to COVID cases as well as violent events in relation to tweets",,,,,
Department of State,Office of the Under Secretary for Public Diplomacy and Public Affairs,AI300,Deepfake Detector,"Deep learning model that takes in an image containing a person’s face and classifies the image as either being real (contains a real person’s face) or fake (synthetically generated face, a deepfake often created using Generative Adversarial Networks).",,,,,
Department of State,Office of the Under Secretary for Public Diplomacy and Public Affairs,AI301,SentiBERTIQ,"GEC A&R uses deep contextual AI of text to identify and extract subjective information within the source material. This sentiment model was trained by fine-tuning a multilingual, BERT model leveraging word embeddings across 2.2 million labeled tweets spanning English, Spanish, Arabic, and traditional and simplified Chinese. The tool will assign a sentiment to each text document and output a CSV containing the sentiment and confidence interval for user review.",,,,,
Department of State,Office of the Under Secretary for Public Diplomacy and Public Affairs,AI302,TOPIQ,"GEC A&R’s TOPIQ tool automatically classifies text into topics for analyst review and interpretation. The tool uses Latent Dirichlet Allocation (LDA), a natural language processing technique that uncovers a specified number of topics from a collection of documents, and then assigns the probability that each document belongs to a topic.",,,,,
Department of State,Office of the Under Secretary for Public Diplomacy and Public Affairs,AI303,Text Similarity,GEC A&R’s Text Similarity capability identified different texts that are identical or nearly identical by calculating cosine similarity between each text. Texts are then grouped if they share high cosine similarity and then available for analysts to review further.,,,,,
Department of State,Office of the Under Secretary for Public Diplomacy and Public Affairs,AI304,Image Clustering,"Uses a pretrained deep learning model to generate image embeddings, then uses hierarchical clustering to identify similar images.",,,,,
Department of State,Office of the Under Secretary for Public Diplomacy and Public Affairs,AI305,Louvain Community Detection,"Takes in a social network and clusters nodes together into “communities” (i.e., similar nodes are grouped together)",,,,,
Department of State,Bureau of Administration (DOS),AI306,Federal Procurement Data System (FPDS) Auto-Populate Bot,"A/LM collaborated with A/OPE to develop a bot to automate the data entry in the Federal Procurement Data System (FPDS), reducing the burden on post’s procurement staff and driving improved compliance on DATA Act reporting. This bot is now used to update ~300 FPDS awards per week.  A/LM also partnered with WHA to develop a bot to automate closeout reminders for federal assistance grants nearing the end of the period of performance and begin developing bots to automate receiving report validation and customer service inbox monitoring.",,,,,
Department of State,Bureau of Administration (DOS),AI307,Product Service Code Automation ML Model,"A/LM developed a machine learning model to scan unstructured, user entered procurement data such as Requisition Title and Line Descriptions to automatically detect the commodity and services types being purchased for enhanced procurement categorization.",,,,,
Department of State,Bureau of Administration (DOS),AI308,Tailored Integration Logistics Management System (ILMS) User Analytics,A/LM plans to use available ILMS transactional data and planned transactions to develop tailored user experiences and analytics to meet the specifics needs of the user at that moment. By mining real system actions and clicks we can extract more meaningful information about our users to simplify their interactions with the system and reduce time to complete their daily actions.,,,,,
Department of State,Bureau of Administration (DOS),AI309,Supply Chain Fraud and Risk Models,"A/LM plans to expand current risk analytics through development of AI/ML models for detecting anomalous activity within the Integrated Logistics Management System (ILMS) that could be potential fraud or malfeasance. The models will expand upon existing risk models and focus on key supply chain functions such as: Asset Management, Procure-to-Pay, and Fleet Management.",,,,,
Department of State,Bureau of Administration (DOS),AI310,Tailored Integration Logistics Management System (ILMS) Automated User Support Bot,ILMS developed and deployed an automated support desk assistant using ServiceNow Virtual Agent to simplify support desk interactions for ILMS customers and to deflect easily resolved issues from higher cost support desk agents.,,,,,
Department of State,Bureau of Political-Military Affairs (PM),AI311,NLP to pull key information from unstructured text,Use NLP to extract information such as country names and agreement dates from dozens of pages of unstructured pdf document,,,,,
Department of State,Bureau of Political-Military Affairs (PM),AI312,K-Means clustering into tiers,Cluster countries into tiers based off data collected from open source and bureau data using k-means clustering,,,,,
Department of State,CSO,AI313,Automated Burning Detection,The Village Monitoring System program uses AI and machine learning to conduct daily scans of moderate resolution commercial satellite imagery to identify anomalies using the near-infrared band.,,,,,
Department of State,CSO,AI314,Automated Damage Assessments,"The Conflict Observatory program uses AI and machine learning on moderate and high-resolution commerical satellite imagery to document a variety of war crimes and other abuses in Ukraine, including automated damage assessments of a variety of buildings, including critical infrastructure, hospitals, schools, crop storage facilities.",,,,,
Department of State,Bureau of Administration (DOS),AI315,Within Grade Increase Automation,"A Natural Language Processing (NLP) model is used in coordination with Intelligent Character Recognition (ICR) to identify and extract values from the JF-62 form for within grade increase payroll actions. Robotic Process Automation (RPA) is then used to validate the data against existing reports, then create a formatted file for approval and processing.",,,,,
Department of State,Bureau of Administration (DOS),AI316,Verified Imagery Pilot Project,"The Bureau of Conflict and Stabilization Operations ran a pilot project to test how the use of a technology service, Sealr, could verify the delivery of foreign assistance to conflict-affected areas where neither U.S. Department of State nor our implementing partner could go.  Sealr uses blockchain encryption to secure photographs taken on smartphones from digital tampering.  It also uses artificial intelligence to detect spoofs, like taking a picture of a picture of something.  Sealr also has some image recognition capabilities.  The pilot demonstrated technology like Sealr can be used as a way to strengthen remote monitoring of foreign assistance to dangerous or otherwise inaccessible areas.",,,,,
Department of State,Bureau of Administration (DOS),AI317,Conflict Forecasting,"CSO/AA is developing a suite of conflict and instability forecasting models that use open-source political, social, and economic datasets to predict conflict outcomes including interstate war, mass mobilization, and mass killings. The use of AI is confined to statistical models including machine learning techniques including tree-based methods, neural networks, and clustering approaches.",,,,,
Department of State,CGFS,AI318,Automatic Detection of Authentic Material,The Foreign Service Institute School of Language Studies is developing a tool for automated discovery of authentic native language texts classified for both topic and Interagency Language Roundtable (ILR) proficiency level to support foreign language curriculum and language testing kit development.,,,,,
Department of State,CSO,AI319,ServiceNow AI-Powered Virtual Agent (Chatbot),IRM’s BMP Systems is planning to incorporate ServiceNow’s Virtual Agent into our existing applications to connect users with support and data requests. The Artificial Intelligence (AI) is provided by ServiceNow as part of their Platform as a Service (PaaS).,,,,,
Department of State,CSO,AI320,Apptio,Working Capital Fund (IRM/WCF) uses Apptio to bill bureaus for consolidated services run from the WCF. Cost models are built in Apptio so bureaus can budget for the service costs in future FYs. Apptio has the capability to extrapolate future values using several available formulas.,,,,,
Department of State,Foreign Service Institute (DOS),AI321,eRecords M/L Metadata Enrichment,"The Department’s central eRecords archive leverages machine learning models to add additional metadata to assist with record discovery and review. This includes models for entity extraction, sentiment analysis, classification and identifying document types.",,,,,
Department of State,Bureau of Information Resource Management (DOS),AI322,AI Capabilities Embedded in SMART,"Models have been embedded in the backend of the SMART system on OpenNet to perform entity extraction of objects within cables, sentiment analysis of cables, keyword extraction of topics identified within cables, and historical data analysis to recommend addressees and passlines to users when composing cables.",,,,,
Department of State,Bureau of Information Resource Management (DOS),AI323,Crisis Campaign Cable Analytics,Use optical character recognition and natural language processing on Department cables in order to evaluate gaps and trends in crisis training and bolster post preparedness for crisis events.,,,,,
Department of State,Bureau of Information Resource Management (DOS),AI324,Fast Text Word Builder,Fast Text is an AI approach to identifying similar terms and phrases based off a root word. This support A&R’s capability to build robust search queries for data collection.,,,,,
Department of State,Bureau of Information Resource Management (DOS),AI325,Behavioral Analytics for Online Surveys Test (Makor Analytics),"GEC executes a Technology Testbed to rapidly test emerging technology applications against foreign disinformation and propaganda challenges. GEC works with interagency and foreign partners to understand operational threats they’re facing and identifies technological countermeasures to apply against the operational challenge via short-duration tests of promising technologies. Makor Analytics is an AI quantitative research company that helps clients understand their audience’s perceptions and feelings helping to mitigate some of the limitations in traditional survey research. Makor Analytics’ proprietary behavioral analytics technology was developed to uncover true convictions and subtle emotions adding additional insights into traditional online survey results. The desired outcome of the pilot is a report analyzing the survey responses using behavioral analytics to provide target audience sentiment insights and subsequent recommendations. By leveraging AI behavioral analytics, the pilot aims to provide additional information beyond self-reported data that reflects sentiment analysis in the country of interest.",,,,,
Department of State,Office of Management Strategy and Solutions (M/SS),AI326,Crisis Campaign Cable Analytics,Use optical character recognition and natural language processing on Department cables in order to evaluate gaps and trends in crisis training and bolster post preparedness for crisis events.,,,,,
Department of State,Office of the Under Secretary for Public Diplomacy and Public Affairs,AI327,Fast Text Word Builder,Fast Text is an AI approach to identifying similar terms and phrases based off a root word. This support A&R’s capability to build robust search queries for data collection.,,,,,
Department of State,Office of the Under Secretary for Public Diplomacy and Public Affairs,AI328,Behavioral Analytics for Online Surveys Test (Makor Analytics),"GEC executes a Technology Testbed to rapidly test emerging technology applications against foreign disinformation and propaganda challenges. GEC works with interagency and foreign partners to understand operational threats they’re facing and identifies technological countermeasures to apply against the operational challenge via short-duration tests of promising technologies. Makor Analytics is an AI quantitative research company that helps clients understand their audience’s perceptions and feelings helping to mitigate some of the limitations in traditional survey research. Makor Analytics’ proprietary behavioral analytics technology was developed to uncover true convictions and subtle emotions adding additional insights into traditional online survey results. The desired outcome of the pilot is a report analyzing the survey responses using behavioral analytics to provide target audience sentiment insights and subsequent recommendations. By leveraging AI behavioral analytics, the pilot aims to provide additional information beyond self-reported data that reflects sentiment analysis in the country of interest.",,,,,
Department of Transporation,Bureau of Administration (DOS),AI329,Tailored Integration Logistics Management System (ILMS) Automated User Support Bot,ILMS developed and deployed an automated support desk assistant using ServiceNow Virtual Agent to simplify support desk interactions for ILMS customers and to deflect easily resolved issues from higher cost support desk agents.,,,,5/11/23,
Department of Transporation,Bureau of Political-Military Affairs (PM),AI330,NLP to pull key information from unstructured text,Use NLP to extract information such as country names and agreement dates from dozens of pages of unstructured pdf document,,,,5/11/23,
Department of Transporation,Bureau of Political-Military Affairs (PM),AI331,K-Means clustering into tiers,Cluster countries into tiers based off data collected from open source and bureau data using k-means clustering,,,,5/11/23,
Department of Transporation,CSO,AI332,Automated Burning Detection,The Village Monitoring System program uses AI and machine learning to conduct daily scans of moderate resolution commercial satellite imagery to identify anomalies using the near-infrared band.,,,,5/11/23,
Department of Transporation,CSO,AI333,Automated Damage Assessments,"The Conflict Observatory program uses AI and machine learning on moderate and high-resolution commerical satellite imagery to document a variety of war crimes and other abuses in Ukraine, including automated damage assessments of a variety of buildings, including critical infrastructure, hospitals, schools, crop storage facilities.",,,,5/11/23,
Department of Transporation,Bureau of Administration (DOS),AI334,Within Grade Increase Automation,"A Natural Language Processing (NLP) model is used in coordination with Intelligent Character Recognition (ICR) to identify and extract values from the JF-62 form for within grade increase payroll actions. Robotic Process Automation (RPA) is then used to validate the data against existing reports, then create a formatted file for approval and processing.",,,,5/11/23,
Department of Transporation,Bureau of Administration (DOS),AI335,Verified Imagery Pilot Project,"The Bureau of Conflict and Stabilization Operations ran a pilot project to test how the use of a technology service, Sealr, could verify the delivery of foreign assistance to conflict-affected areas where neither U.S. Department of State nor our implementing partner could go.  Sealr uses blockchain encryption to secure photographs taken on smartphones from digital tampering.  It also uses artificial intelligence to detect spoofs, like taking a picture of a picture of something.  Sealr also has some image recognition capabilities.  The pilot demonstrated technology like Sealr can be used as a way to strengthen remote monitoring of foreign assistance to dangerous or otherwise inaccessible areas.",,,,5/11/23,
Department of Transporation,Bureau of Administration (DOS),AI336,Conflict Forecasting,"CSO/AA is developing a suite of conflict and instability forecasting models that use open-source political, social, and economic datasets to predict conflict outcomes including interstate war, mass mobilization, and mass killings. The use of AI is confined to statistical models including machine learning techniques including tree-based methods, neural networks, and clustering approaches.",,,,5/11/23,
Department of Transporation,CGFS,AI337,Automatic Detection of Authentic Material,The Foreign Service Institute School of Language Studies is developing a tool for automated discovery of authentic native language texts classified for both topic and Interagency Language Roundtable (ILR) proficiency level to support foreign language curriculum and language testing kit development.,,,,5/11/23,
Department of Transporation,CSO,AI338,ServiceNow AI-Powered Virtual Agent (Chatbot),IRM’s BMP Systems is planning to incorporate ServiceNow’s Virtual Agent into our existing applications to connect users with support and data requests. The Artificial Intelligence (AI) is provided by ServiceNow as part of their Platform as a Service (PaaS).,,,,5/11/23,
Department of Transporation,CSO,AI339,Apptio,Working Capital Fund (IRM/WCF) uses Apptio to bill bureaus for consolidated services run from the WCF. Cost models are built in Apptio so bureaus can budget for the service costs in future FYs. Apptio has the capability to extrapolate future values using several available formulas.,,,,5/11/23,
Department of Transporation,Foreign Service Institute (DOS),AI340,eRecords M/L Metadata Enrichment,"The Department’s central eRecords archive leverages machine learning models to add additional metadata to assist with record discovery and review. This includes models for entity extraction, sentiment analysis, classification and identifying document types.",,,,5/11/23,
Department of Transporation,Bureau of Information Resource Management (DOS),AI341,AI Capabilities Embedded in SMART,"Models have been embedded in the backend of the SMART system on OpenNet to perform entity extraction of objects within cables, sentiment analysis of cables, keyword extraction of topics identified within cables, and historical data analysis to recommend addressees and passlines to users when composing cables.",,,,5/11/23,
Department of Transporation,Bureau of Information Resource Management (DOS),AI342,Crisis Campaign Cable Analytics,Use optical character recognition and natural language processing on Department cables in order to evaluate gaps and trends in crisis training and bolster post preparedness for crisis events.,,,,5/11/23,
Department of Transporation,Bureau of Information Resource Management (DOS),AI343,Fast Text Word Builder,Fast Text is an AI approach to identifying similar terms and phrases based off a root word. This support A&R’s capability to build robust search queries for data collection.,,,,5/11/23,
Department of Treasury,Bureau of Information Resource Management (DOS),AI344,Behavioral Analytics for Online Surveys Test (Makor Analytics),"GEC executes a Technology Testbed to rapidly test emerging technology applications against foreign disinformation and propaganda challenges. GEC works with interagency and foreign partners to understand operational threats they’re facing and identifies technological countermeasures to apply against the operational challenge via short-duration tests of promising technologies. Makor Analytics is an AI quantitative research company that helps clients understand their audience’s perceptions and feelings helping to mitigate some of the limitations in traditional survey research. Makor Analytics’ proprietary behavioral analytics technology was developed to uncover true convictions and subtle emotions adding additional insights into traditional online survey results. The desired outcome of the pilot is a report analyzing the survey responses using behavioral analytics to provide target audience sentiment insights and subsequent recommendations. By leveraging AI behavioral analytics, the pilot aims to provide additional information beyond self-reported data that reflects sentiment analysis in the country of interest.",,,,8/1/23,
Department of Treasury,Office of Management Strategy and Solutions (M/SS),AI345,Crisis Campaign Cable Analytics,Use optical character recognition and natural language processing on Department cables in order to evaluate gaps and trends in crisis training and bolster post preparedness for crisis events.,,,,8/1/23,
Department of Treasury,Office of the Under Secretary for Public Diplomacy and Public Affairs,AI346,Fast Text Word Builder,Fast Text is an AI approach to identifying similar terms and phrases based off a root word. This support A&R’s capability to build robust search queries for data collection.,,,,8/1/23,
Department of Treasury,Office of the Under Secretary for Public Diplomacy and Public Affairs,AI347,Behavioral Analytics for Online Surveys Test (Makor Analytics),"GEC executes a Technology Testbed to rapidly test emerging technology applications against foreign disinformation and propaganda challenges. GEC works with interagency and foreign partners to understand operational threats they’re facing and identifies technological countermeasures to apply against the operational challenge via short-duration tests of promising technologies. Makor Analytics is an AI quantitative research company that helps clients understand their audience’s perceptions and feelings helping to mitigate some of the limitations in traditional survey research. Makor Analytics’ proprietary behavioral analytics technology was developed to uncover true convictions and subtle emotions adding additional insights into traditional online survey results. The desired outcome of the pilot is a report analyzing the survey responses using behavioral analytics to provide target audience sentiment insights and subsequent recommendations. By leveraging AI behavioral analytics, the pilot aims to provide additional information beyond self-reported data that reflects sentiment analysis in the country of interest.",,,,8/1/23,
Department of Treasury,,AI348,Evaluate Multilingual BERT for Software Translation Use Case Evaluations,"Project is evaluating the cost-effectiveness of training a multi-lingual BERT model on IRS corpora and using the model as means to evaluate software translation output of IRS content. The framework is leveraging COMET, ROGUE, and BLEU measures. Furthermore, the product will also be assessed for English-Only and Spanish-Only content content classification.",,,,8/1/23,
Department of Treasury,,AI349,Large Corporate Compliance,Large Corporate Compliance is a machine learning model for classifying corporate taxes.,,,,8/1/23,
Department of Treasury,,AI350,Large Partnership Compliance,Large Partnership Compliance is a machine learning model for stratifying Partnership data and score risk of potential non-compliance.,,,,8/1/23,
Department of Treasury,,AI351,"LB&I Text Analytics
(including Appeals Case
Management)","Trained text extraction and tax domain-specific BERT models (called TaxBERT) using about 190,000 documents including Internal Revenue Code, Internal Revenue Manual, and PDFs from irs.gov, Revenue Rulings, Private Letter Rulings, Revenue Procedures, Treasury Decisions, and other legal tax-related documents. The extracted text was decomposed into 21 million sentences with 1 million unique tokens. Further filtering refinement resulted in 11 million unique sentences and 31 thousand unique vocabulary tokens which are then used to train domain-specific NLP models which can be used for targeted analytics.",,,,8/1/23,
Department of Treasury,,AI352,Line Anomaly Recommender,This use case seeks to identify a workload selection model that uses two recommender system models to measure overall compliance risk and identify anomalous tax returns and line-item values. The delivered pipeline capabilities can supplement the core case selection model processes by providing additional insight to IRS LB&I reviewers through the use of advanced deep learning techniques for anomaly detection,,,,8/1/23,
Department of Treasury,,AI353,NRP Redesign,"Deploy innovative active learning methods to provide a lower opportunity cost method of estimating a compliance baseline to support tax gap estimation, improper papyments reporting, development and validation of workload identfication and selection models, and inform policy analysis. System inputs require existing NRP data which provide an acceptable level of precision and quality for an acceptable level of data quality output.",,,,8/1/23,
Department of Treasury,,AI354,Projected Contract Award Date Web App,"Projected contract award dates are generated with a machine learning model that statistically predicts when procurement requests will become signed contracts. Input data includes funding information, date / time of year, and individual Contract Specialist workload. The model outputs projected contract award timeframes for specific procurement requests. 'When will a contract be signed?' is a key question for the IRS and generally for the federal government. This tool gives insight about when each request is likely to turn into a contract. The tool provides a technique other federal agencies can implement, potentially affecting $600 billion in government contracts. Weblink: https://www.irs.gov/newsroom/irsannounces-use-of-projected-contract-award-date-web-app-that-predictswhen-contracts-will-be-signed",,,,8/1/23,
Department of Treasury,,AI355,SBSE Issue Recommender,"Developed an AI-based recommender for detecting potential noncompliance issues which makes training returns selection more efficient and scalable, which has been applied to the process for selecting training
returns and field work.",,,,8/1/23,
Department of Veterans Affairs,,AI356,Artificial Intelligence physical therapy app,This app is a physical therapy support tool.  It is a data source agnostic tool which takes input from a variety of wearable sensors and then analyzes the data to give feedback to the physical therapist in an explainable format. ,,,,,
Department of Veterans Affairs,,AI357,Artificial intelligence coach in cardiac surgery,"The artificial intelligence coach in cardiac surgery infers misalignment in team members’ mental models during complex healthcare task execution. Of interest are safety-critical domains (e.g., aviation, healthcare), where lack of shared mental models can lead to preventable errors and harm. Identifying model misalignment provides a building block for enabling computer-assisted interventions to improve teamwork and augment human cognition in the operating room.",,,,,
Department of Veterans Affairs,,AI358,AI Cure,AICURE is a phone app that monitors adherence to orally prescribed medications during clinical or pharmaceutical sponsor  drug studies.,,,,,
Department of Veterans Affairs,,AI359,Acute kidney injury (AKI),"This project, a collaboration with Google DeepMind, focuses on detecting acute kidney injury (AKI), ranging from minor loss of kidney function to complete kidney failure. The artificial intelligence can also detect AKI that may be the result of another illness.",,,,,
Department of Veterans Affairs,,AI360,Assessing lung function in health and disease,Health professionals can use this artificial intelligence to determine predictors of normal and abnormal lung function and sleep parameters.,,,,,
Department of Veterans Affairs,,AI361,Automated eye movement analysis and diagnostic prediction of neurological disease,"Artificial intelligence  recursively analyzes previously collected data to both improve the quality and accuracy of automated algorithms, as well as to screen for markers of neurological disease (e.g. traumatic brain injury, Parkinson's, stroke, etc).",,,,,
Department of Veterans Affairs,,AI362,Automatic speech transcription engines to aid scoring neuropsychological tests.,Automated speech transcription engines analyze the cognitive decline of older VA patients. Digitally recorded speech responses are transcribed using multiple artificial intelligence-based speech-to-text engines. The transcriptions are fused together to reduce or obviate the need for manual transcription of patient speech in order to score the neuropsychological tests.,,,,,
Department of Veterans Affairs,,AI363,CuraPatient,"CuraPatient is a remote tool that allows patients to better manage their conditions without having to see a provider.  Driven by artificial intelligence, it allows patients to create a profile to track their health, enroll in programs, manage insurance, and schedule appointments.",,,,,
Department of Veterans Affairs,,AI364,Digital command center,The Digital Command Center seeks to consolidate all data in a medical center and apply predictive prescriptive analytics to allow leaders to better optimize hospital performance.  ,,,,,
Department of Veterans Affairs,,AI365,Disentangling dementia patterns using artificial intelligence on brain imaging and electrophysiological data,This collaborative effort focuses on developing a deep learning framework to predict the various patterns of dementia seen on MRI and EEG and explore the use of these imaging modalities as biomarkers for various dementias and epilepsy disorders.  The VA is performing retrospective chart review to achieve this.,,,,,
Department of Veterans Affairs,,AI366,Machine learning (ML) for enhanced diagnostic error detection and ML classification of protein electrophoresis text,"Researchers are performing chart review to collect true/false positive annotations and construct a vector embedding of patient records, followed by similarity-based retrieval of unlabeled records ""near"" the labeled ones (semi-supervised approach). The aim is to use machine learning as a filter, after the rules-based retrieval, to improve specificity. Embedding inputs will be selected high-value structured data pertinent to stroke risk and possibly selected prior text notes.",,,,,
Department of Veterans Affairs,,AI367,Behavidence,Behavidence is a mental health tracking app. Veterans download the app onto their phone and it compares their phone usage to that of a digital phenotype that represents people with confirmed diagnosis of mental health conditions. ,,,,,
Department of Veterans Affairs,,AI368,Machine learning tools to predict outcomes of hospitalized VA patients,"This is an IRB-approved study which aims to examine machine learning approaches to predict health outcomes of VA patients.  It will focus on the prediction of Alzheimer's disease, rehospitalization, and Chlostridioides difficile infection.",,,,,
Department of Veterans Affairs,,AI369,Nediser reports QA,"Nediser is a continuously trained artificial intelligence “radiology resident” that assists radiologists in confirming the X-ray properties in their radiology reports.  Nediser can select normal templates, detect hardware, evaluate patella alignment and leg length and angle discrepancy, and measure Cobb angles.",,,,,
Department of Veterans Affairs,,AI370,Precision medicine PTSD and suicidality diagnostic and predictive tool,"This model interprets various real time inputs in a diagnostic and predictive capacity in order to forewarn episodes of PTSD and suicidality, support early and accurate diagnosis of the same, and gain a better understanding of the short and long term effects of stress, especially in extreme situations, as it relates to the onset of PTSD.",,,,,
Department of Veterans Affairs,,AI371,Prediction of Veterans' Suicidal Ideation following Transition from Military Service,Machine learning is used to identify predictors of veterans' suicidal ideation. The relevant data come from a web-based survey of veterans’ experiences within three months of separation and every six months after for the first three years after leaving military service.,,,,,
Department of Veterans Affairs,,AI372,PredictMod,PredictMod uses artificial intelligence to determine if predictions can be made about diabetes based on the gut microbiome.,,,,,
Department of Veterans Affairs,,AI373,Predictor profiles of OUD and overdose,Machine learning prediction models evaluate the interactions of known and novel risk factors for opioid use disorder (OUD) and overdose in Post-9/11 Veterans. Several machine learning classification-tree modeling approaches are used to develop predictor profiles of OUD and overdose. ,,,,,
Department of Veterans Affairs,,AI374,Provider directory data accuracy and system of record alignment,"AI is used to add value as a transactor for intelligent identity resolution and linking.  AI also has a domain cache function that can be used for both Clinical Decision Support and for intelligent state reconstruction over time and real-time discrepancy detection.  As a synchronizer, AI can perform intelligent propagation and semi-automated discrepancy resolution.  AI adapters can be used for inference via OWL and logic programming.  Lastly, AI has long term storage (“black box flight recorder”) for virtually limitless machine learning and BI applications.",,,,,
Department of Veterans Affairs,,AI375,Seizure detection from EEG and video,Machine learning algorithms use EEG and video data from a VHA epilepsy monitoring unit in order to automatically identify seizures without human intervention.,,,,,
Department of Veterans Affairs,,AI376,SoKat Suicidial Ideation Detection Engine,The SoKat Suicide Ideation Engine (SSIE) uses natural language processing (NLP) to improve identification of Veteran suicide ideation (SI) from survey data collected by the Office of Mental Health (OMH) Veteran Crisis Line (VCL) support team (VSignals).,,,,,
Department of Veterans Affairs,,AI377,Using machine learning to predict perfusionists’ critical decision-making during cardiac surgery,"A machine learning approach is used to build predictive models of perfusionists’ decision-making during critical situations that occur in the cardiopulmonary bypass phase of cardiac surgery. Results may inform future development of computerized clinical decision support tools to be embedded into the operating room, improving patient safety and surgical outcomes.",,,,,
Department of Veterans Affairs,,AI378,Gait signatures in patients with peripheral artery disease,Machine learning is used to improve treatment of functional problems in patients with peripheral artery disease (PAD). Previously collected biomechanics data is used to identify representative gait signatures of PAD to 1) determine the gait signatures of patients with PAD and 2) the ability of limb acceleration measurements to identify and model the meaningful biomechanics measures from PAD data.,,,,,
Department of Veterans Affairs,,AI379,Medication Safety (MedSafe) Clinical Decision Support (CDS),"Using VA electronic clinical data, the Medication Safety (MedSafe) Clinical Decision Support (CDS) system analyzes current clinical management for diabetes, hypertension, and chronic kidney disease, and makes patient-specific, evidence-based recommendations to primary care providers.  The system uses knowledge bases that encode clinical practice guideline recommendations and an automated execution engine to examine multiple comorbidities, laboratory test results, medications, and history of adverse drug events in evaluating patient clinical status and generating patient-specific recommendations",,,,,
Department of Veterans Affairs,,AI380,"Prediction of health outcomes, including suicide death, opioid overdose, and decompensated outcomes of chronic diseases.","Using electronic health records (EHR) (both structured and unstructured data) as  inputs, this tool outputs deep phenotypes and predictions of health outcomes including suicide death, opioid overdose, and decompensated outcomes of chronic diseases.",,,,,
Department of Veterans Affairs,,AI381,VA-DoE Suicide Exemplar Project,The VA-DoE Suicide Exemplar project is currently utilizing artificial intelligence to improve VA's ability to identify Veterans at risk for suicide through three closely related projects that all involve collaborations with the Department of Energy.,,,,,
Department of Veterans Affairs,,AI382,Machine learning models to predict disease progression among veterans with hepatitis C virus,A machine learning model is used to predict disease progression among veterans with hepatitis C virus.,,,,,
Department of Veterans Affairs,,AI383,Prediction of biologic response to thiopurines,"Using CPRS and CDW data, artificial intelligence is used to predict biologic response to thiopurines among Veterans with irritable bowel disease.",,,,,
Department of Veterans Affairs,,AI384,Predicting hospitalization and corticosteroid use as a surrogate for IBD flares ,"This work examines data from 20,368 Veterans Health Administration (VHA) patients with an irritable bowel disease (IBD) diagnosis between 2002 and 2009. Longitudinal labs and associated predictors were used in random forest models to predict hospitalizations and steroid usage as a surrogate for IBD Flares.",,,,,
Department of Veterans Affairs,,AI385,Predicting corticosteroid free endoscopic remission with Vedolizumab in ulcerative colitis,This work uses random forest modeling on a cohort of 594 patients with Vedolizumab to predict the outcome of corticosteroid-free biologic remission at week 52 on the testing cohort. Models were constructed using baseline data or data through week 6 of VDZ therapy.,,,,,
Department of Veterans Affairs,,AI386,Use of machine learning to predict surgery in Crohn’s disease,"Machine learning analyzes patient demographics, medication use, and longitudinal laboratory values collected between 2001 and 2015 from adult patients in the Veterans Integrated Service Networks (VISN) 10 cohort. The data was used for analysis in prediction of Crohn’s disease and to model future surgical outcomes within 1 year.",,,,,
Department of Veterans Affairs,,AI387,Reinforcement learning evaluation of treatment policies for patients with hepatitis C virus,A machine learning model is used to predict disease progression among veterans with hepatitis C virus.,,,,,
Department of Veterans Affairs,,AI388,Predicting hepatocellular carcinoma in patients with hepatitis C,This prognostic study used data on patients with hepatitis C virus (HCV)-related cirrhosis in the national Veterans Health Administration who had at least 3 years of follow-up after the diagnosis of cirrhosis. The data was used to examine whether deep learning recurrent neural network (RNN) models that use raw longitudinal data extracted directly from electronic health records outperform conventional regression models in predicting the risk of developing hepatocellular carcinoma (HCC).,,,,,
Department of Veterans Affairs,,AI389,Computer-aided detection and classification of colorectal polyps,This study is investigating the use of artificial intelligence models for improving clinical management of colorectal polyps. The models receive video frames from colonoscopy video streams and analyze them in real time in order to (1) detect whether a polyp is in the frame and (2) predict the polyp's malignant potential.,,,,,
Department of Veterans Affairs,,AI390,GI Genius (Medtronic),The Medtronic GI Genius aids in detection of colon polyps through artificial intelligence.,,,,,
Department of Veterans Affairs,,AI391,Extraction of family medical history from patient records,This pilot project uses TIU documentation on African American Veterans aged 45-50 to extract family medical history data and identify Veterans who are are at risk of prostate cancer but have not undergone prostate cancer screening.,,,,,
Department of Veterans Affairs,,AI392,VA /IRB approved research study for finding colon polyps,This IRB approved research study uses  a randomized trial for finding colon polyps with artifical intelligence.,,,,,
Department of Veterans Affairs,,AI393,Interpretation/triage of eye images,"Artificial intelligence supports triage of eye patients cared for through telehealth, interprets eye images, and assesses health risks based on retina photos. The goal is to improve diagnosis of a variety of conditions, including glaucoma, macular degeneration, and diabetic retinopathy.",,,,,
Department of Veterans Affairs,,AI394,Screening for esophageal adenocarcinoma,National VHA administrative data is used to adapt tools that use electronic health records to predict the risk for esophageal adenocarcinoma.,,,,,
Department of Veterans Affairs,,AI395,Social determinants of health extractor,"AI is used with clinical notes to identify social determinants of health (SDOH) information. The extracted SDOH variables can be used during associated health related analysis to determine, among other factors, whether SDOH can be a contributor to disease risks or healthcare inequality.",,,,,
Social Security Administration,"Office of Analytics, Review, and Oversight; Office of Hearing Operations, Office of Disability Information Systems",AI396,Insight,"Insight is decision support software used by hearings and appeals-level Disability Program adjudicators to help maximize the quality, speed, and consistency of their decisionmaking.  Insight analyzes the free text of disability decisions and other case data to offer adjudicators real-time alerts on potential quality issues and case-specific reference information within a web application.  It also offers adjudicators a series of interactive tools to help streamline their work.  Adjudicators can leverage these features to speed their work and fix issues before the case moves forward (e.g. to another reviewing employee or to the claimant).  Insight’s features are powered by several natural language processing and artificial intelligence packages and techniques.",,,,,
Social Security Administration,"Oifice of Disability Determinations, Office of Disability Information Systems",AI397,Intelligent Medical Langage Analysis Generation (IMAGEN),"IMAGEN analyzes clinical text from disability applicants health records and transforms it to data and other useful formats to enable disability adjudicators to more easily find and identify clinical content that is relevant to SSA’s disability determination process.  IMAGEN leverages various Artifcial Intelligence (AI) machine learning technologies including Natural Language Processing (NLP), data modeling and predictive analytics to provide new tools and services that improve the organization and visualization of specific medical encounters, medical reports and lab results that will improve the efficiency and consistency of disability determinations and decisions.  IMAGEN analytics platform will also support other high-priority agency initiatives such as Continuing Disability Reviews (CDR). ",,,,,
Social Security Administration,"Office of Disability Information Systems,  Office of Hearing Operations, Office of Appelate Operations",AI398,Duplicate Identification Process (DIP),"DIP's objective is to help the user to identify and flag duplicate pages and documents within the disability electronic folder more efficiently, reducing the amount of task time associated with preparing cases for SSA's ALJ Hearings. DIP uses artificial intelligence software in the form of image recognition technology to accurately identify document and page duplication that is consistent with SSA policy.",,,,,
Social Security Administration,Office of Operations / Wikes-Barre Direct Operation Center,AI399,Handwriting recognition from forms,Artificial Intelligence (AI) performs Optical Character Recognition (OCR) against handwritten entries on specific standard forms submitted by clients. This use case is in support of a Robotic Process Automation (RPA) effort as well as a standalone use.,,,,,
U.S. Agency for Intenational Development,"USAID/Bureau for Development, Democracy, and Innovation (DDI)",AI400,Media Early Warning System (MEWS),To detect narratives and trends in social media alterations of images and video in order to find and counteract malign narratives,,,,FY2023,
U.S. Agency for Intenational Development,"USAID/Bureau for Development, Democracy, and Innovation (DDI)",AI401,Gender differentiated credit scoring,"University of California, Berkeley, is building a machine learning model to conduct gender differentiated credit scoring for customers of Rappicard in Mexico. They will compare this ML model to Rappi's ""status quo"" model to determine whether a gender differentiated model leads to greater access to credit for women.",,,,FY2023,
U.S. Agency for Intenational Development,USAID/Bureau for Global Health (GH),AI402,Using ML for predicting treatment interruption among PLHIV in Nigeria,"Using data from USAID funded Strengthening Integrated Delivery of HIV/AIDS Services (SIDHAS) project in Nigeria we trained and tested an algorithm that can be used for predicting the probability that someone newly initiated on ART will interrupt treatment. The algorithm has been successfully integrated into the Lafiya Management Information System (LAMIS), the individual-level client level electronic medical record system. Each week the outputs, for each new patient is shared with staff at the health facilities and those at high risk are provided with more intensive follow up support to reduce the risk of treatment interruption. We also conducted a qualitative assessment among to health care workers at the facilities to determine their perception of ML and determine what additional support are required for institutionalizing ML into their routine work.   ",,,,FY2023,
U.S. Agency for Intenational Development,"USAID/Bureau for Development, Democracy, and Innovation (DDI)",AI403,Machine Learning for Peace,"Objective 1 under the Illuminating New Solutions and Programmatic Innovations for Resilient Spaces’ (INSPIRES). Includes program activities
and website - https://web.sas.upenn.edu/mlp-devlab/",,,,FY2023,
U.S. Agency for Intenational Development,"USAID/Bureau for Development, Democracy, and Innovation (DDI)",AI404,Long-term impacts of land-use/land-cover dynamics on surface water quality in Botswana’s reservoirs using satellite data and artificial intelligence methods: Case study of the Botswana’s Limpopo River Basin (1984-2019),"For water supply, semi-arid Botswana relies on the reservoirs within the Botswana’s LRB. Reservoirs are particularly susceptible to the negative impacts of land-use and land-cover (LULC) activities and runoff because of their complex dynamics, relatively longer water residence times, and their role as an integrating sink for pollutants from their drainage basins. Despite these interrelationships and significance in regional and global economic stability, land and water (L-W) are often treated in “silos”. To understand the complex L-W nexus within the LRB, this study will use data-driven artificial intelligence for quantitative determination of the relationships between LULC change, together with socioeconomic development indicators and climate change, and their impacts on water quality and availability within the basin, both for 1984-2019 and to predict future scenarios (2020-2050). To advance data acquisition for LULC analysis and climate change, the study utilizes optical Earth-observation and meteorological satellite data. To provide near real-time and cost-effective approach for continuous monitoring of reservoir water quality within the basin, the study will develop empirical models for water quality estimation and water quality index mapping using 35-years of in-situ water quality measurements and water spectral observations using drone-borne spectrometer and optical satellite imagery through regression modeling and geospatial methods.",,,,FY2023,
U.S. Agency for Intenational Development,"USAID/Bureau for Development, Democracy, and Innovation (DDI)",AI405,Morogoro youth empowerment through establishment of social innovation (YEESI) lab for problem-centered training in machine vision,"The project proposes to establish a social innovation lab for a machine vision program that will be used by youth in the Morogoro region of Tanzania. There are young people in the area who have studied information technologies and allied sciences, and while most of them can write computer programs, they cannot solve machine vision problems. This project aims to increase awareness among the youth of Morogoro and nearby regions to address machine vision problems in agriculture. Machine vision is a new and understudied practice in Tanzania; hence, this project will contribute to efforts in the creation of scientific societies that address the most pressing problems faced by more than 80% of Tanzania’s population who engage in farming. The main agricultural problems can be classified into five categories, as explained below: (1) Disease Detection and Classification: The project will develop experts who will solve problems in disease identification using machine vision for most of the diseases in crops and livestock, which are misdiagnosed by farmers. (2) Weed Classification: The project will develop algorithms that accurately identify weeds and contribute to the growing scientific database for automatic weed detection. (3) Pest Detection and Classification: Appropriate tools using machine vision for Integrated Pest Management (IPM) are needed in Tanzania, as IPM has been hindered due to a lack of extension officers to train farmers on mitigation and identification of pests in agriculture. (4) Crop Seedlings Stand Count and Yield Estimation: Use of machine vision and drones instead of scouting manually to estimate stand counts would provide appropriate mitigation strategies for replanting that would be beneficial to commercial farmers. Also of importance are algorithms to sort and estimate yield by counting the fruits and to estimate the amount of other agricultural products. (5) Crop Vigor Estimation: Most farmers apply inputs evenly across the farm because they cannot predetermine crop vigor. Accurate estimation of crop health would help farmers to mitigate the problems earlier and improve crop performance and avoid failure. Algorithms to determine crop vigor developed in this project will contribute to the improvement of the methods to estimate crop performance earlier.",,,,FY2023,
U.S. Agency for Intenational Development,USAID/Bureau for Global Health (GH),AI406,Breakthrough RESEARCH’s Social Media Listening,"Social media listening draws on machine learning to synthesize and organize the vast quantities of data shared over social media platforms. Breakthrough RESEARCH carried out social listening on 12,301 social media posts in Nigeria to explore how gender-related online conversations manifest themselves and whether they have changed in the last five years. Using Crimson Hexagon’s machine learning algorithm, “Brightview,” publicly available social media content originating in the countries of interest was scraped by the algorithm, for posts relevant to RH/FP and youth. The resulting social media posts were then classified by topic, using language detected in the content. This provided a dataset categorizing conversations into overarching topics, allowing analyses to uncover key trends in topic specific conversation volume, insights about misinformation, attitudes and social norms, and more. The machine learning algorithm was able to identify relevant social media content. The 12,301 social media posts were qualitatively assessed and categorized, allowing researchers to monitor and track social media conversations far more expansively than allowed by research methods more traditionally used in public health and SBC programs.",,,,FY2023,
U.S. Agency for Intenational Development,"USAID/Bureau for Development, Democracy, and Innovation (DDI)",AI407,Project Vikela,Use AI to detect illegal rhino horn in airplane luggage X-Ray scanners,,,,FY2023,
U.S. Agency for Intenational Development,USAID/Bureau for Latin America and the Caribbean,AI408,NASA SERVIR - Bias Correcting Historical GEOGloWS ECMWF Streamflow Service (GESS) data using Machine Learning (ML) Techniques,"GEOGloWS ECMWF Streamflow Service (GESS) helps to organize the international community engaged in the hydrologic sciences, observations, and their application to forecasting and provides a forum for government-to-government collaboration, and engagement with the academic and private sectors to achieve the delivery of actionable water information. Since the formal creation of the initiative in 2017, the most significant element of GEOGloWS has been the application of Earth Observations (EO) to create a system that forecasts flow on every river of the world while also providing a 40-year simulated historical flow.

This application uses Long Short Term Memory (LSTM) Model with the time series of discharge data to bias correct the globally available GESS discharge information locally.",,,,FY2023,
U.S. Agency for Intenational Development,USAID/Bureau for Latin America and the Caribbean,AI409,"NASA SERVIR - Using artificial intelligence to forecast harmful algae blooms in Lake Atitlán, Guatemala","This application uses machine learning with Earth observations and weather-modeled data to forecast daily algal blooms in Lake Atitlán, Guatemala. The forecasting system is being used by Lake Authorities, such as the  Authority for Sustainable Management of the Lake Atitlan Basin and its surroundings (AMSCLAE),  to inform their Harmful Algal Blooms Alert System. This work is also supported by National Geographic and Microsoft through their Artificial Intelligence (AI) for Innovation grants. ",,,,FY2023,
U.S. Agency for Intenational Development,USAID/Bureau for Latin America and the Caribbean,AI410,NASA SERVIR - Mapping urban vulnerability using AI techniques,"This activity will improve urban vulnerability assessment in key population centers, particularly by co-creating replicable methods to use satellite imagery to map informal settlements. ",,,,FY2023,
U.S. Agency for Intenational Development,USAID/Bureau for Global Health (GH),AI411,Serbia: AI predictions for the utilization of hospital beds ,"AI technology was used to predict bed occupancy at hospitals with MoH data from 2019, with an overall median error by department around 20%. This was a proof-of-concept model developed at the request of the Institute of Public Health (IPH) Batut to understand how AI can work and the value add. CHISU was asked to subsequently focus on a different use case (waiting list optimization for scheduled imaging diagnostics services, specifically CT and MRI), which is considered higher priority to demonstrate the implementation of the national AI strategy and the effect of AI in data use for decision making by the government, and will be addressed in the 2023-4.",,,,FY2023,
U.S. Agency for Intenational Development,USAID/Bureau for Global Health (GH),AI412,Mali: AI predictions for the optimization of the allocation of the distribution of COVID-19 vaccines  ,AI technology was used to develop a pandemic preparedness AI model to support allocation of COVID-19 vaccines based on a multi-tiered strategy for target populations: 1) hotspots for COVID-19 positive cases and 2) pregnant/breastfeeding women using DHIS2 data. This was a proof-of-concept model.,,,,FY2023,
U.S. Agency for Intenational Development,USAID/Bureau for Global Health (GH),AI413,Indonesia: AI predictions for improving forecasts for TB drugs,AI technology will be used to develop a forecasting AI model for TB sensitive drugs to inform more accurate annual quantification exercises for the MoH linked to their national data integration platform SatuSehat,,,,FY2023,
U.S. Environmental Protection Agency,,AI414,Use of random forest model to predict exposure pathways,"Prioritizing the potential risk posed to human health by chemicals requires tools that can estimate exposure from limited information. In this study, chemical structure and physicochemical properties were used to predict the probability that a chemical might be associated with any of four exposure pathways leading from sources-consumer (near-field), dietary, far-field industrial, and far-field pesticide-to the general population. The balanced accuracies of these source-based exposure pathway models range from 73 to 81%, with the error rate for identifying positive chemicals ranging from 17 to 36%. We then used exposure pathways to organize predictions from 13 different exposure models as well as other predictors of human intake rates. We created a consensus, meta-model using the Systematic Empirical Evaluation of Models framework in which the predictors of exposure were combined by pathway and weighted according to predictive ability for chemical intake rates inferred from human biomonitoring data for 114 chemicals. The consensus model yields an R2 of ∼0.8. We extrapolate to predict relevant pathway(s), median intake rate, and credible interval for 479 926 chemicals, mostly with minimal exposure information. This approach identifies 1880 chemicals for which the median population intake rates may exceed 0.1 mg/kg bodyweight/day, while there is 95% confidence that the median intake rate is below 1 μg/kg BW/day for 474572 compounds.",,,,12/8/22,
U.S. Environmental Protection Agency,,AI415,Records Categorization,The records management technology team is using machine learning to predict the retention schedule for records. The machine learning model will be incorporated into a records management application to help users apply retention schedules when they submit new records.,,,,12/8/22,
U.S. Environmental Protection Agency,,AI416,Enforcement Targeting,"EPA’s Office of Compliance, in partnership with the University of Chicago, built a proof-of-concept to improve enforcement of environmental regulations through facility inspections by the EPA and state partners. The resulting predictive analytics showed a 47% improvement of identifying violations of the Resource Conservation and Recovery Act.",,,,12/8/22,
U.S. General Services Administration,OGP,AI417,Solicitation Review Tool (SRT),"The SRT intakes SAM.gov data for all Information and Communications Technology (ICT) solicitations. The system then compiles the data into a database to be used by machine learning algorithms. The first of these is a Natural Language Processing model that determines if a solicitation contains compliance language. If a solicitation does not have compliance language, then it is marked as non-compliant. Each agency is asked to review their data and validate the SRT predictions.  GSA also conducts random manual reviews monthly.",,,,,
U.S. General Services Administration,FAS,AI418,Acquisition Analytics,Takes Detailed Data on transactions and classifies each transaction within the Government-wide Category Management Taxonomy,,,,,
U.S. General Services Administration,FAS,AI419,City Pairs Program Ticket Forecast and Scenario Analysis Tools,"Takes segment-level City Pair Program air travel purchase data and creates near-term forecasts for the current and upcoming fiscal year by month and at various levels of granularity including DOD vs Civilian, Agency, and Region.",,,,,
U.S. General Services Administration,FAS,AI420,Category Taxonomy Refinement Using NLP,Uses token extraction from product descriptions more accurately shape intended markets for Product Service Codes (PSCs).,,,,,
U.S. General Services Administration,FAS,AI421,Key KPI Forecasts for GWCM,"Takes monthly historical data for underlying components used to calculate KPIs and creates near-term forecasts for the upcoming fiscal year. Pilot effort focuses on total agency/category spend (the denominator in multiple KPIs). If the pilot program is successful, the same methodology can be extended to other KPIs.",,,,,
U.S. General Services Administration,GSA IT (IDT),AI422,Service Desk Generic Ticket Classification,"We are building a model to take generic Service Desk tickets and classify them so that they can be automatically re-routed to the correct team that handles these types of tickets. The process of re-routing generic tickets is currently done manually, so the model will allow us to automate it. The initial model will target the top 5 most common ticket types.",,,,,
U.S. General Services Administration,GSA IT (IDT),AI423,Service Desk Virtual Agent (Curie),"Virtual agent that uses ML to provide predictive results for chat entries. A natural language chatbot (virtual assistant), we named Curie, as part of a multi-model customer service experience for employee's IT service requests leveraging knowledge-based articles.",,,,,
U.S. General Services Administration,FAS (QP0A),AI424,Contract Acquisition Lifecycle Intelligence (CALI),"CALI tool is an automated machine learning evaluation tool built to streamline the evaluation of vendor proposals against the solicitation requirements to support the Source Selection process. Once the Contracting Officer (CO) has received vendor proposals for a solicitation and is ready to perform the evaluation process, the CO will initiate evaluation by sending solicitation documents along with all associated vendor proposal documents to the Source Selection module, which will pass all documents to CALI. CALI will process the documents, associated metadata and begin analyzing the proposals in four key areas: format compliance, forms validation, reps & certs compliance, and requirements compliance. The designated evaluation members can review the evaluation results in CALI and submit finalized evaluation results back to the Source Selection module.  CALI is currently being trained with sample data from the EULAs under the Multiple Award Schedule (MAS) program.",,,,,
U.S. General Services Administration,TTS,AI425,Classifying Qualitative Data,"USAGov and USAGov en Español collect large amounts of qualitative data from survey comments, web searches and call center chat transcripts. Comments are grouped together by topic to determine where we need to make product updates/enhancements",,,,,
U.S. General Services Administration,FAS / GSA IT (IC),AI426,Chatbot for Federal Acquisition Community,"The introduction of a chatbot will enable the GSA FAS NCSC (National Customer Support Center) to streamline the customer experience process, and automate providing answers to documented commonly asked questions through public facing knowledge articles.  The end goal is this will reduce staffing requirements for NCSC’s live chat programs and allow the NCSC resources to be dedicated to other proactive customer services initiatives.  Customers will still have the option to connect to a live agent if they choose by requesting an agent.",,,,,
U.S. General Services Administration,GSA IT (IC),AI427,Document Workflow / Intelligent Data Capture and Extraction,"GSA is driving towards a more accurate and scalable document workflow platform. GSA seeks to intelligently capture, classify, and transfer critical data from unstructured and structured documents, namely PDF files, to the right process, workflow, or decision engine.",,,,,
U.S. General Services Administration,TTS/IAE,AI428,IAE FSD CCAI Virtual Agent,The virtual agent uses manual learning to understand customer needs and provide a response appropriately. Our AI is named SAM and uses natural language.,,,,,